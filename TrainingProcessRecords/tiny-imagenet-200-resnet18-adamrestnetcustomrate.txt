Users\GeoFly\Github\MultiModalClassifier-1\TorchClassifier> python myTorchTrainer.py --data_name 'tiny-imagenet-200' --data_type 'trainonly' --data_path "C:\Users\GeoFly\Documents\rfan\MultiModalClassifier\Dataset" --model_name 'resnetmodel1' --learningratename 'StepLR' --lr 0.1 --momentum 0.9 --wd 1e-4 --optimizer 'adamresnetcustomrate' --epochs 20       
2.1.1+cu118
Torch Version:  2.1.1+cu118
Torchvision Version:  0.16.1+cpu
Output path: ./outputs/tiny-imagenet-200_resnetmodel1_0326
Num GPUs: 2
0
NVIDIA GeForce RTX 3060
True
Num training images:  100000
Number of classes:  200
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
ResNet (ResNet)                          [128, 3, 224, 224]   [128, 200]           --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 64, 112, 112]  9,408                True
├─BatchNorm2d (bn1)                      [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
├─ReLU (relu)                            [128, 64, 112, 112]  [128, 64, 112, 112]  --                   --
├─MaxPool2d (maxpool)                    [128, 64, 112, 112]  [128, 64, 56, 56]    --                   --
├─Sequential (layer1)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    └─BasicBlock (0)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn1)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn2)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    └─BasicBlock (1)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn1)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn2)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
├─Sequential (layer2)                    [128, 64, 56, 56]    [128, 128, 28, 28]   --                   True
│    └─BasicBlock (0)                    [128, 64, 56, 56]    [128, 128, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 128, 28, 28]   73,728               True
│    │    └─BatchNorm2d (bn1)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─Sequential (downsample)      [128, 64, 56, 56]    [128, 128, 28, 28]   8,448                True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    └─BasicBlock (1)                    [128, 128, 28, 28]   [128, 128, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn1)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
├─Sequential (layer3)                    [128, 128, 28, 28]   [128, 256, 14, 14]   --                   True
│    └─BasicBlock (0)                    [128, 128, 28, 28]   [128, 256, 14, 14]   --                   True
│    │    └─Conv2d (conv1)               [128, 128, 28, 28]   [128, 256, 14, 14]   294,912              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─Sequential (downsample)      [128, 128, 28, 28]   [128, 256, 14, 14]   33,280               True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    └─BasicBlock (1)                    [128, 256, 14, 14]   [128, 256, 14, 14]   --                   True
│    │    └─Conv2d (conv1)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
├─Sequential (layer4)                    [128, 256, 14, 14]   [128, 512, 7, 7]     --                   True
│    └─BasicBlock (0)                    [128, 256, 14, 14]   [128, 512, 7, 7]     --                   True
│    │    └─Conv2d (conv1)               [128, 256, 14, 14]   [128, 512, 7, 7]     1,179,648            True
│    │    └─BatchNorm2d (bn1)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv2)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn2)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─Sequential (downsample)      [128, 256, 14, 14]   [128, 512, 7, 7]     132,096              True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    └─BasicBlock (1)                    [128, 512, 7, 7]     [128, 512, 7, 7]     --                   True
│    │    └─Conv2d (conv1)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn1)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv2)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn2)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
├─AdaptiveAvgPool2d (avgpool)            [128, 512, 7, 7]     [128, 512, 1, 1]     --                   --
├─Linear (fc)                            [128, 512]           [128, 200]           102,600              True
========================================================================================================================
Total params: 11,279,112
Trainable params: 11,279,112
Non-trainable params: 0
Total mult-adds (G): 232.15
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 5086.85
Params size (MB): 45.12
Estimated Total Size (MB): 5209.03
========================================================================================================================
Epoch 0/19
----------
Epoch: [0][  1/625]     Time  0.866 ( 0.866)    Data  0.433 ( 0.433)    Loss 5.5148e+00 (5.5148e+00)    Acc@1   0.00 (  0.00)   Acc@5   1.56 (  1.56)
STAGE:2024-03-26 13:43:04 17048:3488 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-03-26 13:43:06 17048:3488 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-03-26 13:43:06 17048:3488 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-03-26 13:43:10 17048:3488 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-03-26 13:43:12 17048:3488 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-03-26 13:43:12 17048:3488 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Epoch: [0][101/625]     Time  0.664 ( 0.704)    Data  0.199 ( 0.296)    Loss 3.3637e+00 (4.1149e+00)    Acc@1  22.66 ( 14.93)   Acc@5  57.03 ( 34.32)
Epoch: [0][201/625]     Time  0.650 ( 0.691)    Data  0.192 ( 0.251)    Loss 3.0555e+00 (3.6570e+00)    Acc@1  32.03 ( 21.40)   Acc@5  55.47 ( 44.36)
Epoch: [0][301/625]     Time  0.660 ( 0.680)    Data  0.196 ( 0.232)    Loss 2.5190e+00 (3.4058e+00)    Acc@1  35.94 ( 25.28)   Acc@5  66.41 ( 49.73)
Epoch: [0][401/625]     Time  0.651 ( 0.674)    Data  0.192 ( 0.223)    Loss 2.6326e+00 (3.2442e+00)    Acc@1  32.81 ( 28.00)   Acc@5  66.41 ( 52.90)
Epoch: [0][501/625]     Time  0.659 ( 0.670)    Data  0.191 ( 0.217)    Loss 3.0050e+00 (3.1406e+00)    Acc@1  30.47 ( 29.74)   Acc@5  55.47 ( 55.02)
Epoch: [0][601/625]     Time  0.650 ( 0.668)    Data  0.193 ( 0.213)    Loss 2.6760e+00 (3.0500e+00)    Acc@1  36.72 ( 31.34)   Acc@5  64.06 ( 56.81)
train Loss: 3.0341 Acc: 0.3161
Epoch: [0][  1/625]     Time  0.412 ( 0.667)    Data  0.265 ( 0.212)    Loss 2.6607e+00 (3.0335e+00)    Acc@1  38.28 ( 31.62)   Acc@5  65.62 ( 57.13)
Epoch: [0][101/625]     Time  0.337 ( 0.622)    Data  0.192 ( 0.210)    Loss 2.4940e+00 (2.9889e+00)    Acc@1  38.28 ( 32.42)   Acc@5  66.41 ( 58.09)
val Loss: 2.7075 Acc: 0.3723

Epoch 1/19
----------
Epoch: [1][  1/625]     Time  0.829 ( 0.829)    Data  0.363 ( 0.363)    Loss 2.2938e+00 (2.2938e+00)    Acc@1  42.19 ( 42.19)   Acc@5  71.09 ( 71.09)
Epoch: [1][101/625]     Time  0.652 ( 0.656)    Data  0.190 ( 0.194)    Loss 2.0925e+00 (2.3687e+00)    Acc@1  51.56 ( 43.51)   Acc@5  74.22 ( 70.01)
Epoch: [1][201/625]     Time  0.613 ( 0.654)    Data  0.160 ( 0.193)    Loss 2.3209e+00 (2.3878e+00)    Acc@1  45.31 ( 43.33)   Acc@5  71.88 ( 69.38)
Epoch: [1][301/625]     Time  0.625 ( 0.644)    Data  0.160 ( 0.183)    Loss 2.3441e+00 (2.3929e+00)    Acc@1  40.62 ( 43.16)   Acc@5  69.53 ( 69.35)
Epoch: [1][401/625]     Time  0.625 ( 0.638)    Data  0.160 ( 0.177)    Loss 2.4718e+00 (2.3778e+00)    Acc@1  41.41 ( 43.55)   Acc@5  66.41 ( 69.49)
Epoch: [1][501/625]     Time  0.626 ( 0.635)    Data  0.160 ( 0.174)    Loss 2.9058e+00 (2.3705e+00)    Acc@1  35.94 ( 43.83)   Acc@5  60.94 ( 69.61)
Epoch: [1][601/625]     Time  0.619 ( 0.633)    Data  0.160 ( 0.172)    Loss 1.9229e+00 (2.3661e+00)    Acc@1  53.91 ( 43.99)   Acc@5  74.22 ( 69.61)
train Loss: 2.3644 Acc: 0.4400
Epoch: [1][  1/625]     Time  0.360 ( 0.632)    Data  0.214 ( 0.171)    Loss 2.4243e+00 (2.3645e+00)    Acc@1  45.31 ( 44.00)   Acc@5  73.44 ( 69.64)
Epoch: [1][101/625]     Time  0.317 ( 0.588)    Data  0.172 ( 0.170)    Loss 2.5902e+00 (2.3785e+00)    Acc@1  45.31 ( 43.70)   Acc@5  70.31 ( 69.47)
val Loss: 2.4707 Acc: 0.4183

Epoch 2/19
----------
Epoch: [2][  1/625]     Time  0.907 ( 0.907)    Data  0.447 ( 0.447)    Loss 1.9045e+00 (1.9045e+00)    Acc@1  54.69 ( 54.69)   Acc@5  80.47 ( 80.47)
Epoch: [2][101/625]     Time  0.620 ( 0.626)    Data  0.161 ( 0.165)    Loss 2.1905e+00 (2.1084e+00)    Acc@1  43.75 ( 49.11)   Acc@5  71.09 ( 74.16)
Epoch: [2][201/625]     Time  0.620 ( 0.624)    Data  0.160 ( 0.163)    Loss 1.8072e+00 (2.1229e+00)    Acc@1  57.81 ( 49.03)   Acc@5  82.81 ( 73.76)
Epoch: [2][301/625]     Time  0.628 ( 0.624)    Data  0.162 ( 0.163)    Loss 2.0286e+00 (2.1172e+00)    Acc@1  57.81 ( 49.23)   Acc@5  75.78 ( 73.79)
Epoch: [2][401/625]     Time  0.615 ( 0.624)    Data  0.162 ( 0.162)    Loss 1.8338e+00 (2.1233e+00)    Acc@1  56.25 ( 49.10)   Acc@5  75.78 ( 73.76)
Epoch: [2][501/625]     Time  0.633 ( 0.624)    Data  0.168 ( 0.163)    Loss 2.1033e+00 (2.1360e+00)    Acc@1  53.12 ( 48.91)   Acc@5  76.56 ( 73.59)
Epoch: [2][601/625]     Time  0.627 ( 0.624)    Data  0.163 ( 0.163)    Loss 2.0112e+00 (2.1444e+00)    Acc@1  48.44 ( 48.70)   Acc@5  75.00 ( 73.45)
train Loss: 2.1442 Acc: 0.4867
Epoch: [2][  1/625]     Time  0.363 ( 0.624)    Data  0.215 ( 0.163)    Loss 2.4843e+00 (2.1447e+00)    Acc@1  42.19 ( 48.66)   Acc@5  70.31 ( 73.45)
Epoch: [2][101/625]     Time  0.310 ( 0.581)    Data  0.163 ( 0.163)    Loss 2.5124e+00 (2.1737e+00)    Acc@1  42.19 ( 48.11)   Acc@5  68.75 ( 72.97)
val Loss: 2.3669 Acc: 0.4442

Epoch 3/19
----------
Epoch: [3][  1/625]     Time  0.740 ( 0.740)    Data  0.275 ( 0.275)    Loss 2.0693e+00 (2.0693e+00)    Acc@1  52.34 ( 52.34)   Acc@5  71.88 ( 71.88)
Epoch: [3][101/625]     Time  0.621 ( 0.625)    Data  0.162 ( 0.164)    Loss 1.7344e+00 (1.9186e+00)    Acc@1  55.47 ( 53.82)   Acc@5  78.91 ( 76.91)
Epoch: [3][201/625]     Time  0.616 ( 0.625)    Data  0.162 ( 0.163)    Loss 2.1852e+00 (1.9490e+00)    Acc@1  46.09 ( 53.00)   Acc@5  72.66 ( 76.45)
Epoch: [3][301/625]     Time  0.633 ( 0.625)    Data  0.166 ( 0.163)    Loss 2.0396e+00 (1.9787e+00)    Acc@1  48.44 ( 52.21)   Acc@5  77.34 ( 76.08)
Epoch: [3][401/625]     Time  0.631 ( 0.625)    Data  0.164 ( 0.164)    Loss 2.1233e+00 (1.9833e+00)    Acc@1  48.44 ( 52.16)   Acc@5  68.75 ( 75.88)
Epoch: [3][501/625]     Time  0.616 ( 0.625)    Data  0.162 ( 0.164)    Loss 2.1968e+00 (1.9912e+00)    Acc@1  46.88 ( 51.96)   Acc@5  75.00 ( 75.81)
Epoch: [3][601/625]     Time  0.616 ( 0.625)    Data  0.162 ( 0.164)    Loss 2.0200e+00 (1.9906e+00)    Acc@1  47.66 ( 51.91)   Acc@5  78.91 ( 75.79)
train Loss: 1.9901 Acc: 0.5193
Epoch: [3][  1/625]     Time  0.363 ( 0.625)    Data  0.215 ( 0.164)    Loss 2.2153e+00 (1.9905e+00)    Acc@1  48.44 ( 51.93)   Acc@5  67.19 ( 75.80)
Epoch: [3][101/625]     Time  0.311 ( 0.581)    Data  0.166 ( 0.164)    Loss 2.5080e+00 (2.0274e+00)    Acc@1  43.75 ( 51.25)   Acc@5  67.19 ( 75.30)
val Loss: 2.2871 Acc: 0.4642

Epoch 4/19
----------
Epoch: [4][  1/625]     Time  0.911 ( 0.911)    Data  0.454 ( 0.454)    Loss 1.8675e+00 (1.8675e+00)    Acc@1  49.22 ( 49.22)   Acc@5  77.34 ( 77.34)
Epoch: [4][101/625]     Time  0.627 ( 0.627)    Data  0.162 ( 0.166)    Loss 1.6969e+00 (1.8171e+00)    Acc@1  57.81 ( 55.41)   Acc@5  82.81 ( 78.60)
Epoch: [4][201/625]     Time  0.615 ( 0.625)    Data  0.162 ( 0.164)    Loss 2.0867e+00 (1.8325e+00)    Acc@1  50.78 ( 55.20)   Acc@5  73.44 ( 78.18)
Epoch: [4][301/625]     Time  0.625 ( 0.624)    Data  0.160 ( 0.163)    Loss 1.7473e+00 (1.8423e+00)    Acc@1  57.81 ( 55.05)   Acc@5  81.25 ( 77.98)
Epoch: [4][401/625]     Time  0.616 ( 0.624)    Data  0.160 ( 0.163)    Loss 2.1145e+00 (1.8608e+00)    Acc@1  47.66 ( 54.62)   Acc@5  73.44 ( 77.70)
Epoch: [4][501/625]     Time  0.615 ( 0.624)    Data  0.162 ( 0.163)    Loss 1.8121e+00 (1.8699e+00)    Acc@1  57.81 ( 54.52)   Acc@5  78.12 ( 77.72)
Epoch: [4][601/625]     Time  0.615 ( 0.624)    Data  0.161 ( 0.163)    Loss 2.1861e+00 (1.8721e+00)    Acc@1  45.31 ( 54.47)   Acc@5  70.31 ( 77.69)
train Loss: 1.8743 Acc: 0.5443
Epoch: [4][  1/625]     Time  0.360 ( 0.623)    Data  0.214 ( 0.163)    Loss 2.0817e+00 (1.8746e+00)    Acc@1  49.22 ( 54.42)   Acc@5  77.34 ( 77.67)
Epoch: [4][101/625]     Time  0.308 ( 0.580)    Data  0.160 ( 0.163)    Loss 2.4737e+00 (1.9328e+00)    Acc@1  40.62 ( 53.36)   Acc@5  69.53 ( 76.77)
val Loss: 2.3006 Acc: 0.4671

Epoch 5/19
----------
Epoch: [5][  1/625]     Time  0.857 ( 0.857)    Data  0.392 ( 0.392)    Loss 1.6144e+00 (1.6144e+00)    Acc@1  63.28 ( 63.28)   Acc@5  82.03 ( 82.03)
Epoch: [5][101/625]     Time  0.614 ( 0.624)    Data  0.160 ( 0.163)    Loss 1.6106e+00 (1.7243e+00)    Acc@1  60.16 ( 57.74)   Acc@5  84.38 ( 80.11)
Epoch: [5][201/625]     Time  0.624 ( 0.622)    Data  0.159 ( 0.161)    Loss 1.7117e+00 (1.7192e+00)    Acc@1  54.69 ( 57.73)   Acc@5  78.12 ( 80.23)
Epoch: [5][301/625]     Time  0.613 ( 0.622)    Data  0.159 ( 0.161)    Loss 1.8789e+00 (1.7295e+00)    Acc@1  53.12 ( 57.56)   Acc@5  79.69 ( 80.10)
Epoch: [5][401/625]     Time  0.618 ( 0.622)    Data  0.160 ( 0.161)    Loss 1.6648e+00 (1.7418e+00)    Acc@1  62.50 ( 57.22)   Acc@5  79.69 ( 79.90)
Epoch: [5][501/625]     Time  0.624 ( 0.621)    Data  0.159 ( 0.161)    Loss 1.8300e+00 (1.7511e+00)    Acc@1  54.69 ( 56.98)   Acc@5  75.78 ( 79.74)
Epoch: [5][601/625]     Time  0.614 ( 0.621)    Data  0.160 ( 0.160)    Loss 1.8929e+00 (1.7653e+00)    Acc@1  53.12 ( 56.72)   Acc@5  78.91 ( 79.49)
train Loss: 1.7652 Acc: 0.5670
Epoch: [5][  1/625]     Time  0.356 ( 0.621)    Data  0.210 ( 0.161)    Loss 2.4499e+00 (1.7663e+00)    Acc@1  49.22 ( 56.69)   Acc@5  67.97 ( 79.49)
Epoch: [5][101/625]     Time  0.306 ( 0.578)    Data  0.159 ( 0.160)    Loss 2.2297e+00 (1.8383e+00)    Acc@1  49.22 ( 55.35)   Acc@5  70.31 ( 78.43)
val Loss: 2.2833 Acc: 0.4734

Epoch 6/19
----------
Epoch: [6][  1/625]     Time  0.734 ( 0.734)    Data  0.273 ( 0.273)    Loss 1.6390e+00 (1.6390e+00)    Acc@1  57.81 ( 57.81)   Acc@5  84.38 ( 84.38)
Epoch: [6][101/625]     Time  0.623 ( 0.624)    Data  0.159 ( 0.162)    Loss 1.5729e+00 (1.6182e+00)    Acc@1  57.81 ( 59.61)   Acc@5  82.03 ( 81.71)
Epoch: [6][201/625]     Time  0.624 ( 0.623)    Data  0.160 ( 0.162)    Loss 1.7958e+00 (1.6188e+00)    Acc@1  58.59 ( 59.69)   Acc@5  78.91 ( 81.74)
Epoch: [6][301/625]     Time  0.613 ( 0.622)    Data  0.160 ( 0.161)    Loss 1.9750e+00 (1.6380e+00)    Acc@1  50.00 ( 59.32)   Acc@5  77.34 ( 81.42)
Epoch: [6][401/625]     Time  0.624 ( 0.622)    Data  0.159 ( 0.161)    Loss 1.7768e+00 (1.6510e+00)    Acc@1  57.03 ( 59.00)   Acc@5  79.69 ( 81.31)
Epoch: [6][501/625]     Time  0.617 ( 0.622)    Data  0.160 ( 0.161)    Loss 1.8645e+00 (1.6645e+00)    Acc@1  52.34 ( 58.79)   Acc@5  78.12 ( 81.13)
Epoch: [6][601/625]     Time  0.620 ( 0.622)    Data  0.161 ( 0.161)    Loss 1.8178e+00 (1.6742e+00)    Acc@1  57.03 ( 58.62)   Acc@5  75.00 ( 81.00)
train Loss: 1.6775 Acc: 0.5856
Epoch: [6][  1/625]     Time  0.485 ( 0.622)    Data  0.337 ( 0.161)    Loss 2.1465e+00 (1.6783e+00)    Acc@1  49.22 ( 58.55)   Acc@5  74.22 ( 80.89)
Epoch: [6][101/625]     Time  0.306 ( 0.578)    Data  0.161 ( 0.161)    Loss 2.5205e+00 (1.7637e+00)    Acc@1  42.19 ( 57.06)   Acc@5  64.06 ( 79.66)
val Loss: 2.2885 Acc: 0.4780

Epoch 7/19
----------
Epoch: [7][  1/625]     Time  0.741 ( 0.741)    Data  0.274 ( 0.274)    Loss 1.6314e+00 (1.6314e+00)    Acc@1  59.38 ( 59.38)   Acc@5  79.69 ( 79.69)
Epoch: [7][101/625]     Time  0.621 ( 0.622)    Data  0.160 ( 0.161)    Loss 1.3846e+00 (1.5721e+00)    Acc@1  64.06 ( 61.30)   Acc@5  82.81 ( 82.66)
Epoch: [7][201/625]     Time  0.618 ( 0.622)    Data  0.159 ( 0.161)    Loss 1.4573e+00 (1.5697e+00)    Acc@1  66.41 ( 61.05)   Acc@5  85.16 ( 82.53)
Epoch: [7][301/625]     Time  0.628 ( 0.622)    Data  0.161 ( 0.160)    Loss 1.6217e+00 (1.5713e+00)    Acc@1  62.50 ( 60.96)   Acc@5  81.25 ( 82.53)
Epoch: [7][401/625]     Time  0.469 ( 0.617)    Data  0.177 ( 0.167)    Loss 1.8141e+00 (1.5821e+00)    Acc@1  53.91 ( 60.69)   Acc@5  79.69 ( 82.39)
Epoch: [7][501/625]     Time  0.471 ( 0.598)    Data  0.230 ( 0.172)    Loss 1.7305e+00 (1.6036e+00)    Acc@1  55.47 ( 60.32)   Acc@5  80.47 ( 81.97)
Epoch: [7][601/625]     Time  0.401 ( 0.569)    Data  0.162 ( 0.174)    Loss 1.5580e+00 (1.6133e+00)    Acc@1  63.28 ( 60.10)   Acc@5  82.03 ( 81.81)
train Loss: 1.6135 Acc: 0.6009
Epoch: [7][  1/625]     Time  0.278 ( 0.563)    Data  0.200 ( 0.174)    Loss 2.5732e+00 (1.6150e+00)    Acc@1  44.53 ( 60.07)   Acc@5  68.75 ( 81.79)
Epoch: [7][101/625]     Time  0.256 ( 0.522)    Data  0.174 ( 0.175)    Loss 2.0194e+00 (1.7043e+00)    Acc@1  55.47 ( 58.45)   Acc@5  77.34 ( 80.52)
val Loss: 2.2513 Acc: 0.4856

Epoch 8/19
----------
Epoch: [8][  1/625]     Time  1.227 ( 1.227)    Data  0.877 ( 0.877)    Loss 1.5280e+00 (1.5280e+00)    Acc@1  64.06 ( 64.06)   Acc@5  79.69 ( 79.69)
Epoch: [8][101/625]     Time  0.680 ( 0.670)    Data  0.216 ( 0.227)    Loss 1.4858e+00 (1.4549e+00)    Acc@1  67.19 ( 63.50)   Acc@5  81.25 ( 84.25)
Epoch: [8][201/625]     Time  0.651 ( 0.645)    Data  0.195 ( 0.224)    Loss 1.7622e+00 (1.4766e+00)    Acc@1  56.25 ( 63.06)   Acc@5  78.12 ( 83.75)
Epoch: [8][301/625]     Time  0.824 ( 0.670)    Data  0.268 ( 0.228)    Loss 1.7726e+00 (1.4870e+00)    Acc@1  62.50 ( 62.90)   Acc@5  80.47 ( 83.61)
Epoch: [8][401/625]     Time  0.432 ( 0.635)    Data  0.191 ( 0.224)    Loss 1.7315e+00 (1.4979e+00)    Acc@1  57.81 ( 62.79)   Acc@5  81.25 ( 83.42)
Epoch: [8][501/625]     Time  0.431 ( 0.595)    Data  0.191 ( 0.217)    Loss 1.5513e+00 (1.5157e+00)    Acc@1  57.81 ( 62.39)   Acc@5  81.25 ( 83.11)
Epoch: [8][601/625]     Time  0.432 ( 0.568)    Data  0.191 ( 0.213)    Loss 1.6415e+00 (1.5266e+00)    Acc@1  57.81 ( 62.20)   Acc@5  82.03 ( 82.92)
train Loss: 1.5287 Acc: 0.6218
Epoch: [8][  1/625]     Time  0.312 ( 0.562)    Data  0.233 ( 0.213)    Loss 1.9852e+00 (1.5294e+00)    Acc@1  57.81 ( 62.17)   Acc@5  78.12 ( 82.86)
Epoch: [8][101/625]     Time  0.276 ( 0.523)    Data  0.189 ( 0.210)    Loss 2.1481e+00 (1.6333e+00)    Acc@1  46.88 ( 60.31)   Acc@5  71.88 ( 81.44)
val Loss: 2.2660 Acc: 0.4897

Epoch 9/19
----------
Epoch: [9][  1/625]     Time  0.772 ( 0.772)    Data  0.308 ( 0.308)    Loss 1.2770e+00 (1.2770e+00)    Acc@1  67.97 ( 67.97)   Acc@5  85.94 ( 85.94)
Epoch: [9][101/625]     Time  0.643 ( 0.656)    Data  0.189 ( 0.194)    Loss 1.4493e+00 (1.3765e+00)    Acc@1  66.41 ( 65.39)   Acc@5  80.47 ( 84.76)
Epoch: [9][201/625]     Time  0.657 ( 0.654)    Data  0.192 ( 0.193)    Loss 1.8674e+00 (1.4158e+00)    Acc@1  51.56 ( 64.58)   Acc@5  76.56 ( 84.49)
Epoch: [9][301/625]     Time  0.658 ( 0.654)    Data  0.196 ( 0.193)    Loss 1.2228e+00 (1.4344e+00)    Acc@1  71.88 ( 64.03)   Acc@5  84.38 ( 84.07)
Epoch: [9][401/625]     Time  0.650 ( 0.654)    Data  0.192 ( 0.193)    Loss 1.4822e+00 (1.4547e+00)    Acc@1  61.72 ( 63.61)   Acc@5  81.25 ( 83.77)
Epoch: [9][501/625]     Time  0.647 ( 0.654)    Data  0.190 ( 0.193)    Loss 1.3050e+00 (1.4581e+00)    Acc@1  65.62 ( 63.54)   Acc@5  85.16 ( 83.74)
Epoch: [9][601/625]     Time  0.625 ( 0.650)    Data  0.161 ( 0.189)    Loss 1.3275e+00 (1.4713e+00)    Acc@1  65.62 ( 63.23)   Acc@5  87.50 ( 83.58)
train Loss: 1.4747 Acc: 0.6319
Epoch: [9][  1/625]     Time  0.359 ( 0.649)    Data  0.213 ( 0.188)    Loss 2.3808e+00 (1.4761e+00)    Acc@1  47.66 ( 63.16)   Acc@5  70.31 ( 83.49)
Epoch: [9][101/625]     Time  0.307 ( 0.601)    Data  0.159 ( 0.184)    Loss 2.3032e+00 (1.5857e+00)    Acc@1  50.00 ( 61.20)   Acc@5  74.22 ( 82.05)
val Loss: 2.2688 Acc: 0.4882

Epoch 10/19
----------
Epoch: [10][  1/625]    Time  0.884 ( 0.884)    Data  0.420 ( 0.420)    Loss 1.4277e+00 (1.4277e+00)    Acc@1  64.84 ( 64.84)   Acc@5  82.03 ( 82.03)
Epoch: [10][101/625]    Time  0.615 ( 0.624)    Data  0.161 ( 0.163)    Loss 9.5984e-01 (1.3523e+00)    Acc@1  73.44 ( 66.52)   Acc@5  90.62 ( 85.16)
Epoch: [10][201/625]    Time  0.620 ( 0.623)    Data  0.160 ( 0.162)    Loss 1.1810e+00 (1.3611e+00)    Acc@1  71.09 ( 66.18)   Acc@5  89.06 ( 85.19)
Epoch: [10][301/625]    Time  0.628 ( 0.623)    Data  0.163 ( 0.162)    Loss 1.6299e+00 (1.3909e+00)    Acc@1  60.16 ( 65.52)   Acc@5  78.12 ( 84.66)
Epoch: [10][401/625]    Time  0.614 ( 0.623)    Data  0.161 ( 0.161)    Loss 1.6093e+00 (1.4007e+00)    Acc@1  64.06 ( 65.23)   Acc@5  82.81 ( 84.61)
Epoch: [10][501/625]    Time  0.618 ( 0.622)    Data  0.160 ( 0.161)    Loss 1.7155e+00 (1.4165e+00)    Acc@1  54.69 ( 64.87)   Acc@5  79.69 ( 84.40)
Epoch: [10][601/625]    Time  0.624 ( 0.622)    Data  0.160 ( 0.161)    Loss 1.1817e+00 (1.4265e+00)    Acc@1  74.22 ( 64.57)   Acc@5  89.84 ( 84.28)
train Loss: 1.4278 Acc: 0.6454
Epoch: [10][  1/625]    Time  0.359 ( 0.622)    Data  0.213 ( 0.161)    Loss 2.3975e+00 (1.4294e+00)    Acc@1  48.44 ( 64.51)   Acc@5  70.31 ( 84.23)
Epoch: [10][101/625]    Time  0.308 ( 0.579)    Data  0.161 ( 0.161)    Loss 2.0643e+00 (1.5574e+00)    Acc@1  46.88 ( 62.30)   Acc@5  79.69 ( 82.65)
val Loss: 2.3526 Acc: 0.4838

Epoch 11/19
----------
Epoch: [11][  1/625]    Time  0.743 ( 0.743)    Data  0.276 ( 0.276)    Loss 1.1358e+00 (1.1358e+00)    Acc@1  70.31 ( 70.31)   Acc@5  89.84 ( 89.84)
Epoch: [11][101/625]    Time  0.615 ( 0.624)    Data  0.161 ( 0.163)    Loss 1.2145e+00 (1.3133e+00)    Acc@1  65.62 ( 67.04)   Acc@5  89.06 ( 85.78)
Epoch: [11][201/625]    Time  0.626 ( 0.624)    Data  0.161 ( 0.162)    Loss 1.2937e+00 (1.3158e+00)    Acc@1  70.31 ( 66.84)   Acc@5  85.94 ( 85.61)
Epoch: [11][301/625]    Time  0.625 ( 0.623)    Data  0.161 ( 0.162)    Loss 1.2717e+00 (1.3339e+00)    Acc@1  65.62 ( 66.46)   Acc@5  88.28 ( 85.38)
Epoch: [11][401/625]    Time  0.630 ( 0.623)    Data  0.168 ( 0.162)    Loss 1.3639e+00 (1.3470e+00)    Acc@1  67.19 ( 66.17)   Acc@5  85.16 ( 85.14)
Epoch: [11][501/625]    Time  0.628 ( 0.624)    Data  0.162 ( 0.162)    Loss 1.3339e+00 (1.3549e+00)    Acc@1  70.31 ( 65.95)   Acc@5  83.59 ( 85.13)
Epoch: [11][601/625]    Time  0.616 ( 0.624)    Data  0.162 ( 0.163)    Loss 1.6061e+00 (1.3703e+00)    Acc@1  56.25 ( 65.69)   Acc@5  85.16 ( 84.87)
train Loss: 1.3743 Acc: 0.6560
Epoch: [11][  1/625]    Time  0.362 ( 0.623)    Data  0.216 ( 0.163)    Loss 2.3900e+00 (1.3759e+00)    Acc@1  44.53 ( 65.57)   Acc@5  67.97 ( 84.83)
Epoch: [11][101/625]    Time  0.313 ( 0.581)    Data  0.169 ( 0.163)    Loss 2.4431e+00 (1.5180e+00)    Acc@1  48.44 ( 63.17)   Acc@5  75.78 ( 83.12)
val Loss: 2.3936 Acc: 0.4865

Epoch 12/19
----------
Epoch: [12][  1/625]    Time  0.877 ( 0.877)    Data  0.412 ( 0.412)    Loss 1.1333e+00 (1.1333e+00)    Acc@1  68.75 ( 68.75)   Acc@5  88.28 ( 88.28)
Epoch: [12][101/625]    Time  0.625 ( 0.626)    Data  0.162 ( 0.165)    Loss 9.8089e-01 (1.2494e+00)    Acc@1  73.44 ( 68.21)   Acc@5  89.84 ( 86.62)
Epoch: [12][201/625]    Time  0.616 ( 0.625)    Data  0.163 ( 0.164)    Loss 1.2890e+00 (1.2615e+00)    Acc@1  67.19 ( 68.18)   Acc@5  83.59 ( 86.33)
Epoch: [12][301/625]    Time  0.627 ( 0.626)    Data  0.162 ( 0.165)    Loss 1.4063e+00 (1.2777e+00)    Acc@1  64.06 ( 67.77)   Acc@5  82.03 ( 86.11)
Epoch: [12][401/625]    Time  0.616 ( 0.626)    Data  0.162 ( 0.164)    Loss 1.3570e+00 (1.2960e+00)    Acc@1  63.28 ( 67.42)   Acc@5  87.50 ( 85.88)
Epoch: [12][501/625]    Time  0.624 ( 0.626)    Data  0.171 ( 0.164)    Loss 1.2909e+00 (1.3077e+00)    Acc@1  67.97 ( 67.14)   Acc@5  83.59 ( 85.71)
Epoch: [12][601/625]    Time  0.622 ( 0.626)    Data  0.167 ( 0.164)    Loss 1.4001e+00 (1.3218e+00)    Acc@1  67.97 ( 66.90)   Acc@5  82.03 ( 85.53)
train Loss: 1.3215 Acc: 0.6691
Epoch: [12][  1/625]    Time  0.361 ( 0.625)    Data  0.215 ( 0.164)    Loss 2.5852e+00 (1.3236e+00)    Acc@1  42.19 ( 66.87)   Acc@5  71.09 ( 85.51)
Epoch: [12][101/625]    Time  0.309 ( 0.582)    Data  0.162 ( 0.164)    Loss 2.5101e+00 (1.4676e+00)    Acc@1  46.09 ( 64.27)   Acc@5  65.62 ( 83.71)
val Loss: 2.3496 Acc: 0.4865

Epoch 13/19
----------
Epoch: [13][  1/625]    Time  0.745 ( 0.745)    Data  0.276 ( 0.276)    Loss 9.1677e-01 (9.1677e-01)    Acc@1  77.34 ( 77.34)   Acc@5  90.62 ( 90.62)
Epoch: [13][101/625]    Time  0.616 ( 0.626)    Data  0.161 ( 0.165)    Loss 1.5849e+00 (1.2265e+00)    Acc@1  62.50 ( 69.36)   Acc@5  80.47 ( 86.32)
Epoch: [13][201/625]    Time  0.627 ( 0.625)    Data  0.161 ( 0.163)    Loss 1.2352e+00 (1.2443e+00)    Acc@1  69.53 ( 68.84)   Acc@5  86.72 ( 86.31)
Epoch: [13][301/625]    Time  0.615 ( 0.624)    Data  0.161 ( 0.163)    Loss 1.3981e+00 (1.2511e+00)    Acc@1  66.41 ( 68.53)   Acc@5  86.72 ( 86.34)
Epoch: [13][401/625]    Time  0.627 ( 0.624)    Data  0.164 ( 0.163)    Loss 1.2181e+00 (1.2649e+00)    Acc@1  66.41 ( 68.17)   Acc@5  87.50 ( 86.25)
Epoch: [13][501/625]    Time  0.616 ( 0.624)    Data  0.163 ( 0.162)    Loss 9.9575e-01 (1.2716e+00)    Acc@1  71.09 ( 68.07)   Acc@5  89.84 ( 86.20)
Epoch: [13][601/625]    Time  0.615 ( 0.624)    Data  0.161 ( 0.162)    Loss 1.6353e+00 (1.2779e+00)    Acc@1  56.25 ( 67.89)   Acc@5  82.81 ( 86.12)
train Loss: 1.2800 Acc: 0.6782
Epoch: [13][  1/625]    Time  0.358 ( 0.623)    Data  0.212 ( 0.162)    Loss 2.2790e+00 (1.2816e+00)    Acc@1  46.88 ( 67.78)   Acc@5  75.78 ( 86.11)
Epoch: [13][101/625]    Time  0.305 ( 0.580)    Data  0.160 ( 0.162)    Loss 2.2472e+00 (1.4280e+00)    Acc@1  47.66 ( 65.23)   Acc@5  75.78 ( 84.31)
val Loss: 2.3633 Acc: 0.4896

Epoch 14/19
----------
Epoch: [14][  1/625]    Time  0.877 ( 0.877)    Data  0.412 ( 0.412)    Loss 1.0285e+00 (1.0285e+00)    Acc@1  71.88 ( 71.88)   Acc@5  88.28 ( 88.28)
Epoch: [14][101/625]    Time  0.614 ( 0.624)    Data  0.160 ( 0.163)    Loss 1.0700e+00 (1.1741e+00)    Acc@1  71.88 ( 70.75)   Acc@5  87.50 ( 87.02)
Epoch: [14][201/625]    Time  0.614 ( 0.623)    Data  0.159 ( 0.162)    Loss 1.2111e+00 (1.1920e+00)    Acc@1  65.62 ( 69.96)   Acc@5  89.06 ( 87.19)
Epoch: [14][301/625]    Time  0.651 ( 0.622)    Data  0.195 ( 0.161)    Loss 1.2381e+00 (1.2106e+00)    Acc@1  67.19 ( 69.61)   Acc@5  87.50 ( 87.04)
Epoch: [14][401/625]    Time  0.631 ( 0.622)    Data  0.173 ( 0.161)    Loss 1.1269e+00 (1.2199e+00)    Acc@1  71.09 ( 69.25)   Acc@5  86.72 ( 87.05)
Epoch: [14][501/625]    Time  0.625 ( 0.622)    Data  0.160 ( 0.161)    Loss 1.4926e+00 (1.2230e+00)    Acc@1  67.19 ( 69.21)   Acc@5  83.59 ( 87.02)
Epoch: [14][601/625]    Time  0.625 ( 0.622)    Data  0.160 ( 0.161)    Loss 1.4644e+00 (1.2327e+00)    Acc@1  67.19 ( 68.97)   Acc@5  84.38 ( 86.90)
train Loss: 1.2364 Acc: 0.6886
Epoch: [14][  1/625]    Time  0.359 ( 0.621)    Data  0.213 ( 0.161)    Loss 2.2158e+00 (1.2380e+00)    Acc@1  48.44 ( 68.83)   Acc@5  75.78 ( 86.83)
Epoch: [14][101/625]    Time  0.308 ( 0.578)    Data  0.161 ( 0.161)    Loss 2.2153e+00 (1.3931e+00)    Acc@1  55.47 ( 66.17)   Acc@5  78.91 ( 84.89)
val Loss: 2.3773 Acc: 0.4917

Epoch 15/19
----------
Epoch: [15][  1/625]    Time  0.737 ( 0.737)    Data  0.272 ( 0.272)    Loss 9.5855e-01 (9.5855e-01)    Acc@1  75.78 ( 75.78)   Acc@5  90.62 ( 90.62)
Epoch: [15][101/625]    Time  0.616 ( 0.624)    Data  0.160 ( 0.163)    Loss 1.0683e+00 (1.1575e+00)    Acc@1  73.44 ( 70.68)   Acc@5  90.62 ( 87.88)
Epoch: [15][201/625]    Time  0.623 ( 0.622)    Data  0.160 ( 0.161)    Loss 1.1562e+00 (1.1617e+00)    Acc@1  71.09 ( 70.56)   Acc@5  86.72 ( 87.88)
Epoch: [15][301/625]    Time  0.625 ( 0.622)    Data  0.161 ( 0.161)    Loss 1.5769e+00 (1.1646e+00)    Acc@1  59.38 ( 70.46)   Acc@5  82.03 ( 87.79)
Epoch: [15][401/625]    Time  0.615 ( 0.622)    Data  0.161 ( 0.161)    Loss 1.2192e+00 (1.1839e+00)    Acc@1  69.53 ( 70.07)   Acc@5  89.06 ( 87.42)
Epoch: [15][501/625]    Time  0.613 ( 0.622)    Data  0.159 ( 0.161)    Loss 1.6423e+00 (1.1987e+00)    Acc@1  62.50 ( 69.80)   Acc@5  74.22 ( 87.20)
Epoch: [15][601/625]    Time  0.625 ( 0.622)    Data  0.160 ( 0.161)    Loss 1.1667e+00 (1.2101e+00)    Acc@1  67.19 ( 69.54)   Acc@5  87.50 ( 87.04)
train Loss: 1.2109 Acc: 0.6949
Epoch: [15][  1/625]    Time  0.358 ( 0.621)    Data  0.212 ( 0.161)    Loss 2.4279e+00 (1.2128e+00)    Acc@1  51.56 ( 69.46)   Acc@5  72.66 ( 87.01)
Epoch: [15][101/625]    Time  0.305 ( 0.578)    Data  0.158 ( 0.161)    Loss 2.7431e+00 (1.3768e+00)    Acc@1  48.44 ( 66.66)   Acc@5  70.31 ( 84.98)
val Loss: 2.4095 Acc: 0.4888

Epoch 16/19
----------
Epoch: [16][  1/625]    Time  0.873 ( 0.873)    Data  0.408 ( 0.408)    Loss 1.2123e+00 (1.2123e+00)    Acc@1  71.09 ( 71.09)   Acc@5  85.94 ( 85.94)
Epoch: [16][101/625]    Time  0.626 ( 0.624)    Data  0.160 ( 0.163)    Loss 9.1854e-01 (1.1209e+00)    Acc@1  78.91 ( 71.77)   Acc@5  91.41 ( 88.06)
Epoch: [16][201/625]    Time  0.658 ( 0.629)    Data  0.197 ( 0.168)    Loss 1.1028e+00 (1.1271e+00)    Acc@1  71.88 ( 71.54)   Acc@5  89.84 ( 88.10)
Epoch: [16][301/625]    Time  0.662 ( 0.638)    Data  0.194 ( 0.176)    Loss 1.2149e+00 (1.1421e+00)    Acc@1  67.97 ( 71.13)   Acc@5  89.06 ( 87.93)
Epoch: [16][401/625]    Time  0.659 ( 0.642)    Data  0.191 ( 0.180)    Loss 1.0054e+00 (1.1533e+00)    Acc@1  73.44 ( 70.95)   Acc@5  92.19 ( 87.75)
Epoch: [16][501/625]    Time  0.655 ( 0.645)    Data  0.190 ( 0.183)    Loss 1.1262e+00 (1.1604e+00)    Acc@1  71.88 ( 70.72)   Acc@5  91.41 ( 87.70)
Epoch: [16][601/625]    Time  0.648 ( 0.647)    Data  0.190 ( 0.184)    Loss 1.0993e+00 (1.1721e+00)    Acc@1  71.09 ( 70.40)   Acc@5  90.62 ( 87.55)
train Loss: 1.1743 Acc: 0.7033
Epoch: [16][  1/625]    Time  0.413 ( 0.647)    Data  0.265 ( 0.185)    Loss 2.0904e+00 (1.1758e+00)    Acc@1  56.25 ( 70.31)   Acc@5  72.66 ( 87.49)
Epoch: [16][101/625]    Time  0.338 ( 0.604)    Data  0.191 ( 0.186)    Loss 2.1511e+00 (1.3466e+00)    Acc@1  54.69 ( 67.46)   Acc@5  78.12 ( 85.48)
val Loss: 2.3940 Acc: 0.4966

Epoch 17/19
----------
Epoch: [17][  1/625]    Time  0.772 ( 0.772)    Data  0.306 ( 0.306)    Loss 1.2377e+00 (1.2377e+00)    Acc@1  69.53 ( 69.53)   Acc@5  85.94 ( 85.94)
Epoch: [17][101/625]    Time  0.438 ( 0.572)    Data  0.196 ( 0.216)    Loss 9.2742e-01 (1.0951e+00)    Acc@1  78.12 ( 73.03)   Acc@5  90.62 ( 88.07)
Epoch: [17][201/625]    Time  0.655 ( 0.613)    Data  0.191 ( 0.205)    Loss 1.4669e+00 (1.1052e+00)    Acc@1  63.28 ( 72.45)   Acc@5  84.38 ( 87.92)
Epoch: [17][301/625]    Time  0.653 ( 0.627)    Data  0.191 ( 0.201)    Loss 1.1919e+00 (1.1085e+00)    Acc@1  72.66 ( 72.28)   Acc@5  87.50 ( 88.04)
Epoch: [17][401/625]    Time  0.661 ( 0.635)    Data  0.193 ( 0.199)    Loss 9.4984e-01 (1.1230e+00)    Acc@1  75.00 ( 71.85)   Acc@5  85.94 ( 87.98)
Epoch: [17][501/625]    Time  0.665 ( 0.639)    Data  0.199 ( 0.198)    Loss 1.0152e+00 (1.1321e+00)    Acc@1  71.88 ( 71.66)   Acc@5  89.84 ( 87.88)
Epoch: [17][601/625]    Time  0.622 ( 0.639)    Data  0.161 ( 0.195)    Loss 1.2992e+00 (1.1424e+00)    Acc@1  71.88 ( 71.45)   Acc@5  86.72 ( 87.76)
train Loss: 1.1437 Acc: 0.7141
Epoch: [17][  1/625]    Time  0.362 ( 0.638)    Data  0.217 ( 0.194)    Loss 2.5299e+00 (1.1459e+00)    Acc@1  45.31 ( 71.36)   Acc@5  75.78 ( 87.72)
Epoch: [17][101/625]    Time  0.308 ( 0.593)    Data  0.163 ( 0.189)    Loss 2.6458e+00 (1.3207e+00)    Acc@1  49.22 ( 68.33)   Acc@5  67.97 ( 85.70)
val Loss: 2.4262 Acc: 0.4918

Epoch 18/19
----------
Epoch: [18][  1/625]    Time  0.877 ( 0.877)    Data  0.411 ( 0.411)    Loss 7.9210e-01 (7.9210e-01)    Acc@1  81.25 ( 81.25)   Acc@5  90.62 ( 90.62)
Epoch: [18][101/625]    Time  0.629 ( 0.627)    Data  0.163 ( 0.165)    Loss 1.4264e+00 (1.0878e+00)    Acc@1  65.62 ( 72.62)   Acc@5  81.25 ( 88.47)
Epoch: [18][201/625]    Time  0.613 ( 0.626)    Data  0.159 ( 0.164)    Loss 1.0868e+00 (1.0981e+00)    Acc@1  72.66 ( 72.25)   Acc@5  85.94 ( 88.23)
Epoch: [18][301/625]    Time  0.627 ( 0.625)    Data  0.160 ( 0.164)    Loss 9.8394e-01 (1.1014e+00)    Acc@1  71.88 ( 72.37)   Acc@5  88.28 ( 88.19)
Epoch: [18][401/625]    Time  0.624 ( 0.625)    Data  0.160 ( 0.164)    Loss 1.3387e+00 (1.1029e+00)    Acc@1  69.53 ( 72.26)   Acc@5  86.72 ( 88.21)
Epoch: [18][501/625]    Time  0.656 ( 0.625)    Data  0.197 ( 0.163)    Loss 9.8833e-01 (1.1115e+00)    Acc@1  75.00 ( 71.93)   Acc@5  91.41 ( 88.17)
Epoch: [18][601/625]    Time  0.623 ( 0.624)    Data  0.161 ( 0.163)    Loss 9.7588e-01 (1.1207e+00)    Acc@1  77.34 ( 71.72)   Acc@5  89.06 ( 88.11)
train Loss: 1.1215 Acc: 0.7169
Epoch: [18][  1/625]    Time  0.366 ( 0.624)    Data  0.218 ( 0.163)    Loss 2.4068e+00 (1.1235e+00)    Acc@1  50.78 ( 71.66)   Acc@5  71.09 ( 88.08)
Epoch: [18][101/625]    Time  0.307 ( 0.580)    Data  0.160 ( 0.163)    Loss 2.4302e+00 (1.3077e+00)    Acc@1  54.69 ( 68.54)   Acc@5  78.12 ( 85.98)
val Loss: 2.4519 Acc: 0.4910

Epoch 19/19
----------
Epoch: [19][  1/625]    Time  0.747 ( 0.747)    Data  0.279 ( 0.279)    Loss 1.1971e+00 (1.1971e+00)    Acc@1  73.44 ( 73.44)   Acc@5  83.59 ( 83.59)
Epoch: [19][101/625]    Time  0.615 ( 0.624)    Data  0.161 ( 0.163)    Loss 1.0033e+00 (1.0012e+00)    Acc@1  77.34 ( 74.37)   Acc@5  88.28 ( 89.73)
Epoch: [19][201/625]    Time  0.616 ( 0.624)    Data  0.162 ( 0.163)    Loss 1.0726e+00 (1.0318e+00)    Acc@1  73.44 ( 73.85)   Acc@5  86.72 ( 89.22)
Epoch: [19][301/625]    Time  0.627 ( 0.624)    Data  0.161 ( 0.162)    Loss 8.6880e-01 (1.0423e+00)    Acc@1  78.12 ( 73.58)   Acc@5  94.53 ( 89.08)
Epoch: [19][401/625]    Time  0.629 ( 0.624)    Data  0.163 ( 0.162)    Loss 1.2502e+00 (1.0574e+00)    Acc@1  68.75 ( 73.26)   Acc@5  87.50 ( 88.85)
Epoch: [19][501/625]    Time  0.634 ( 0.624)    Data  0.172 ( 0.162)    Loss 7.7532e-01 (1.0663e+00)    Acc@1  81.25 ( 73.03)   Acc@5  95.31 ( 88.75)
Epoch: [19][601/625]    Time  0.630 ( 0.624)    Data  0.163 ( 0.162)    Loss 1.2620e+00 (1.0756e+00)    Acc@1  67.19 ( 72.83)   Acc@5  85.94 ( 88.60)
train Loss: 1.0759 Acc: 0.7282
Epoch: [19][  1/625]    Time  0.366 ( 0.623)    Data  0.218 ( 0.163)    Loss 2.3641e+00 (1.0779e+00)    Acc@1  48.44 ( 72.78)   Acc@5  75.78 ( 88.60)
Epoch: [19][101/625]    Time  0.315 ( 0.580)    Data  0.167 ( 0.163)    Loss 2.4760e+00 (1.2774e+00)    Acc@1  49.22 ( 69.41)   Acc@5  75.00 ( 86.36)
val Loss: 2.4926 Acc: 0.4863

Training complete in 146m 3s
Best val Acc: 0.496600
Test Loss: 0.484651

Test Accuracy of n01443537: 68% (68/100)
Test Accuracy of n01629819: 70% (68/97)
Test Accuracy of n01641577: 56% (59/105)
Test Accuracy of n01644900: 38% (42/110)
Test Accuracy of n01698640: 55% (48/87)
Test Accuracy of n01742172: 47% (50/105)
Test Accuracy of n01768244: 54% (58/106)
Test Accuracy of n01770393: 34% (36/105)
Test Accuracy of n01774384: 70% (68/97)
Test Accuracy of n01774750: 59% (59/100)
Test Accuracy of n01784675: 41% (44/105)
Test Accuracy of n01855672: 55% (41/74)
Test Accuracy of n01882714: 71% (73/102)
Test Accuracy of n01910747: 72% (70/96)
Test Accuracy of n01917289: 47% (43/90)
Test Accuracy of n01944390: 43% (40/92)
Test Accuracy of n01945685: 33% (34/102)
Test Accuracy of n01950731: 51% (46/90)
Test Accuracy of n01983481: 44% (44/98)
Test Accuracy of n01984695: 52% (43/82)
Test Accuracy of n02002724: 68% (64/93)
Test Accuracy of n02056570: 77% (73/94)
Test Accuracy of n02058221: 79% (69/87)
Test Accuracy of n02074367: 74% (80/107)
Test Accuracy of n02085620: 35% (38/106)
Test Accuracy of n02094433: 54% (54/100)
Test Accuracy of n02099601: 47% (52/110)
Test Accuracy of n02099712: 40% (38/93)
Test Accuracy of n02106662: 56% (47/83)
Test Accuracy of n02113799: 49% (46/93)
Test Accuracy of n02123045: 42% (46/108)
Test Accuracy of n02123394: 54% (64/117)
Test Accuracy of n02124075: 41% (46/112)
Test Accuracy of n02125311: 41% (36/86)
Test Accuracy of n02129165: 65% (56/85)
Test Accuracy of n02132136: 69% (59/85)
Test Accuracy of n02165456: 81% (90/111)
Test Accuracy of n02190166: 48% (48/99)
Test Accuracy of n02206856: 50% (56/110)
Test Accuracy of n02226429: 35% (39/111)
Test Accuracy of n02231487: 57% (50/87)
Test Accuracy of n02233338: 44% (47/106)
Test Accuracy of n02236044: 29% (32/109)
Test Accuracy of n02268443: 28% (32/113)
Test Accuracy of n02279972: 95% (94/98)
Test Accuracy of n02281406: 71% (75/105)
Test Accuracy of n02321529: 49% (50/102)
Test Accuracy of n02364673: 47% (46/96)
Test Accuracy of n02395406: 40% (40/98)
Test Accuracy of n02403003: 18% (19/104)
Test Accuracy of n02410509: 68% (65/95)
Test Accuracy of n02415577: 47% (35/73)
Test Accuracy of n02423022: 60% (73/120)
Test Accuracy of n02437312: 50% (51/102)
Test Accuracy of n02480495: 57% (56/98)
Test Accuracy of n02481823: 60% (71/118)
Test Accuracy of n02486410: 44% (49/111)
Test Accuracy of n02504458: 56% (59/105)
Test Accuracy of n02509815: 75% (74/98)
Test Accuracy of n02666196: 43% (45/103)
Test Accuracy of n02669723: 68% (76/111)
Test Accuracy of n02699494: 49% (52/106)
Test Accuracy of n02730930: 37% (39/105)
Test Accuracy of n02769748: 27% (26/95)
Test Accuracy of n02788148:  8% ( 9/103)
Test Accuracy of n02791270: 35% (34/96)
Test Accuracy of n02793495: 65% (64/97)
Test Accuracy of n02795169: 42% (37/87)
Test Accuracy of n02802426: 87% (84/96)
Test Accuracy of n02808440: 52% (53/101)
Test Accuracy of n02814533: 31% (30/95)
Test Accuracy of n02814860: 76% (61/80)
Test Accuracy of n02815834: 48% (48/98)
Test Accuracy of n02823428: 53% (52/97)
Test Accuracy of n02837789: 46% (48/104)
Test Accuracy of n02841315: 28% (27/96)
Test Accuracy of n02843684: 47% (46/96)
Test Accuracy of n02883205: 31% (37/117)
Test Accuracy of n02892201: 56% (52/92)
Test Accuracy of n02906734: 22% (20/88)
Test Accuracy of n02909870: 24% (25/104)
Test Accuracy of n02917067: 74% (80/108)
Test Accuracy of n02927161: 41% (43/103)
Test Accuracy of n02948072: 55% (54/98)
Test Accuracy of n02950826: 26% (26/99)
Test Accuracy of n02963159: 54% (63/116)
Test Accuracy of n02977058: 49% (46/93)
Test Accuracy of n02988304: 40% (44/108)
Test Accuracy of n02999410: 29% (26/87)
Test Accuracy of n03014705: 46% (52/112)
Test Accuracy of n03026506: 61% (55/89)
Test Accuracy of n03042490: 62% (72/116)
Test Accuracy of n03085013: 58% (67/114)
Test Accuracy of n03089624: 49% (53/108)
Test Accuracy of n03100240: 38% (37/96)
Test Accuracy of n03126707: 46% (43/92)
Test Accuracy of n03160309: 36% (35/95)
Test Accuracy of n03179701: 43% (40/92)
Test Accuracy of n03201208: 53% (52/98)
Test Accuracy of n03250847: 36% (38/105)
Test Accuracy of n03255030: 19% (20/104)
Test Accuracy of n03355925: 51% (56/109)
Test Accuracy of n03388043: 30% (27/89)
Test Accuracy of n03393912: 77% (65/84)
Test Accuracy of n03400231: 54% (58/107)
Test Accuracy of n03404251: 38% (37/95)
Test Accuracy of n03424325: 44% (39/88)
Test Accuracy of n03444034: 61% (53/86)
Test Accuracy of n03447447: 67% (65/97)
Test Accuracy of n03544143: 45% (48/106)
Test Accuracy of n03584254: 60% (64/105)
Test Accuracy of n03599486: 59% (61/102)
Test Accuracy of n03617480: 44% (43/96)
Test Accuracy of n03637318: 48% (44/91)
Test Accuracy of n03649909: 35% (34/97)
Test Accuracy of n03662601: 74% (69/93)
Test Accuracy of n03670208: 57% (61/106)
Test Accuracy of n03706229: 71% (80/112)
Test Accuracy of n03733131: 67% (64/95)
Test Accuracy of n03763968: 32% (37/113)
Test Accuracy of n03770439: 32% (34/105)
Test Accuracy of n03796401: 53% (52/97)
Test Accuracy of n03804744: 28% (23/82)
Test Accuracy of n03814639: 47% (43/91)
Test Accuracy of n03837869: 58% (60/103)
Test Accuracy of n03838899: 35% (36/102)
Test Accuracy of n03854065: 56% (55/97)
Test Accuracy of n03891332: 32% (31/95)
Test Accuracy of n03902125: 42% (36/85)
Test Accuracy of n03930313: 53% (59/111)
Test Accuracy of n03937543: 41% (44/105)
Test Accuracy of n03970156: 20% (23/115)
Test Accuracy of n03976657: 30% (34/112)
Test Accuracy of n03977966: 54% (54/100)
Test Accuracy of n03980874: 41% (39/94)
Test Accuracy of n03983396: 29% (28/96)
Test Accuracy of n03992509: 36% (35/96)
Test Accuracy of n04008634: 36% (36/98)
Test Accuracy of n04023962: 42% (35/82)
Test Accuracy of n04067472: 19% (18/94)
Test Accuracy of n04070727: 56% (58/103)
Test Accuracy of n04074963: 41% (36/86)
Test Accuracy of n04099969: 40% (42/105)
Test Accuracy of n04118538: 78% (69/88)
Test Accuracy of n04133789: 52% (59/113)
Test Accuracy of n04146614: 73% (82/111)
Test Accuracy of n04149813: 60% (59/97)
Test Accuracy of n04179913: 47% (47/99)
Test Accuracy of n04251144: 53% (42/78)
Test Accuracy of n04254777: 40% (48/119)
Test Accuracy of n04259630: 47% (53/112)
Test Accuracy of n04265275: 40% (45/112)
Test Accuracy of n04275548: 54% (51/94)
Test Accuracy of n04285008: 65% (62/94)
Test Accuracy of n04311004: 64% (63/98)
Test Accuracy of n04328186: 44% (46/103)
Test Accuracy of n04356056: 36% (41/113)
Test Accuracy of n04366367: 39% (43/108)
Test Accuracy of n04371430: 38% (42/110)
Test Accuracy of n04376876: 26% (27/102)
Test Accuracy of n04398044: 28% (26/90)
Test Accuracy of n04399382: 56% (50/89)
Test Accuracy of n04417672: 42% (50/117)
Test Accuracy of n04456115: 49% (43/87)
Test Accuracy of n04465501: 45% (50/109)
Test Accuracy of n04486054: 73% (82/112)
Test Accuracy of n04487081: 75% (78/104)
Test Accuracy of n04501370: 51% (67/131)
Test Accuracy of n04507155: 26% (31/115)
Test Accuracy of n04532106: 28% (33/115)
Test Accuracy of n04532670: 69% (63/91)
Test Accuracy of n04540053: 20% (21/101)
Test Accuracy of n04560804: 18% (19/104)
Test Accuracy of n04562935: 60% (53/87)
Test Accuracy of n04596742: 32% (30/93)
Test Accuracy of n04597913: 13% (12/91)
Test Accuracy of n06596364: 65% (64/98)
Test Accuracy of n07579787: 32% (35/109)
Test Accuracy of n07583066: 48% (47/97)
Test Accuracy of n07614500: 32% (37/113)
Test Accuracy of n07615774: 39% (43/108)
Test Accuracy of n07695742: 58% (57/98)
Test Accuracy of n07711569: 36% (35/97)
Test Accuracy of n07715103: 71% (71/100)
Test Accuracy of n07720875: 66% (59/89)
Test Accuracy of n07734744: 59% (57/96)
Test Accuracy of n07747607: 51% (55/106)
Test Accuracy of n07749582: 64% (66/103)
Test Accuracy of n07753592: 57% (56/97)
Test Accuracy of n07768694: 63% (63/100)
Test Accuracy of n07871810: 40% (41/101)
Test Accuracy of n07873807: 81% (81/99)
Test Accuracy of n07875152: 42% (47/110)
Test Accuracy of n07920052: 70% (69/98)
Test Accuracy of n09193705: 58% (56/95)
Test Accuracy of n09246464: 44% (45/101)
Test Accuracy of n09256479: 62% (62/99)
Test Accuracy of n09332890: 26% (28/104)
Test Accuracy of n09428293: 42% (41/97)
Test Accuracy of n12267677: 54% (56/102)

Test Accuracy (Overall): 49% (9838/20000)