python myTorchTrainer.py --data_name 'tiny-imagenet-200' --data_type 'trainonly' --data_path "C:\Users\GeoFly\Documents\rfan\MultiModalClassifier\Dataset" --model_name 'resnetmodel1' --learningratename 'StepLR' --lr 0.1 --momentum 0.9 --wd 1e-4 --optimizer 'SGD'
2.1.1+cu118
Torch Version:  2.1.1+cu118
Torchvision Version:  0.16.1+cpu
Output path: ./outputs/tiny-imagenet-200_resnetmodel1_0910
Num GPUs: 2
Current GPU is 0
NVIDIA GeForce RTX 3060
True
Num training images:  100000
Number of classes:  200
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to C:/Users/GeoFly/Documents/rfan/MultiModalClassifier/torchhome\hub\checkpoints\resnet18-f37072fd.pth
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:09<00:00, 4.94MB/s]
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
ResNet (ResNet)                          [128, 3, 224, 224]   [128, 200]           --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 64, 112, 112]  9,408                True
├─BatchNorm2d (bn1)                      [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
├─ReLU (relu)                            [128, 64, 112, 112]  [128, 64, 112, 112]  --                   --
├─MaxPool2d (maxpool)                    [128, 64, 112, 112]  [128, 64, 56, 56]    --                   --
├─Sequential (layer1)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    └─BasicBlock (0)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn1)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn2)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    └─BasicBlock (1)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn1)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn2)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
├─Sequential (layer2)                    [128, 64, 56, 56]    [128, 128, 28, 28]   --                   True
│    └─BasicBlock (0)                    [128, 64, 56, 56]    [128, 128, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 128, 28, 28]   73,728               True
│    │    └─BatchNorm2d (bn1)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─Sequential (downsample)      [128, 64, 56, 56]    [128, 128, 28, 28]   8,448                True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    └─BasicBlock (1)                    [128, 128, 28, 28]   [128, 128, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn1)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
├─Sequential (layer3)                    [128, 128, 28, 28]   [128, 256, 14, 14]   --                   True
│    └─BasicBlock (0)                    [128, 128, 28, 28]   [128, 256, 14, 14]   --                   True
│    │    └─Conv2d (conv1)               [128, 128, 28, 28]   [128, 256, 14, 14]   294,912              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─Sequential (downsample)      [128, 128, 28, 28]   [128, 256, 14, 14]   33,280               True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    └─BasicBlock (1)                    [128, 256, 14, 14]   [128, 256, 14, 14]   --                   True
│    │    └─Conv2d (conv1)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
├─Sequential (layer4)                    [128, 256, 14, 14]   [128, 512, 7, 7]     --                   True
│    └─BasicBlock (0)                    [128, 256, 14, 14]   [128, 512, 7, 7]     --                   True
│    │    └─Conv2d (conv1)               [128, 256, 14, 14]   [128, 512, 7, 7]     1,179,648            True
│    │    └─BatchNorm2d (bn1)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv2)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn2)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─Sequential (downsample)      [128, 256, 14, 14]   [128, 512, 7, 7]     132,096              True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    └─BasicBlock (1)                    [128, 512, 7, 7]     [128, 512, 7, 7]     --                   True
│    │    └─Conv2d (conv1)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn1)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv2)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn2)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
├─AdaptiveAvgPool2d (avgpool)            [128, 512, 7, 7]     [128, 512, 1, 1]     --                   --
├─Linear (fc)                            [128, 512]           [128, 200]           102,600              True
========================================================================================================================
Total params: 11,279,112
Trainable params: 11,279,112
Non-trainable params: 0
Total mult-adds (G): 232.15
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 5086.85
Params size (MB): 45.12
Estimated Total Size (MB): 5209.03
========================================================================================================================
=> no checkpoint found at 'outputs/tiny-imagenet-200_resnet50_0319/model_best.pth.tar'
Epoch 0/39
----------
Epoch: [0][  1/625]     Time  1.774 ( 1.774)    Data  1.334 ( 1.334)    Loss 5.5515e+00 (5.5515e+00)    Acc@1   0.00 (  0.00)   Acc@5   0.78 (  0.78)
STAGE:2024-03-25 21:34:45 7196:12368 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-03-25 21:34:50 7196:12368 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-03-25 21:34:50 7196:12368 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-03-25 21:34:56 7196:12368 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-03-25 21:35:01 7196:12368 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-03-25 21:35:01 7196:12368 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Epoch: [0][101/625]     Time  1.524 ( 1.598)    Data  1.065 ( 1.141)    Loss 5.1380e+00 (5.3645e+00)    Acc@1   0.78 (  2.24)   Acc@5   8.59 (  8.39)
Epoch: [0][201/625]     Time  1.511 ( 1.607)    Data  1.047 ( 1.142)    Loss 4.6977e+00 (5.0557e+00)    Acc@1   3.91 (  3.63)   Acc@5  12.50 ( 12.62)
Epoch: [0][301/625]     Time  1.568 ( 1.631)    Data  1.103 ( 1.173)    Loss 4.2470e+00 (4.8571e+00)    Acc@1   6.25 (  4.84)   Acc@5  26.56 ( 16.05)
Epoch: [0][401/625]     Time  1.579 ( 1.633)    Data  1.107 ( 1.172)    Loss 4.2441e+00 (4.7078e+00)    Acc@1   8.59 (  6.00)   Acc@5  25.00 ( 18.90)
Epoch: [0][501/625]     Time  1.684 ( 1.623)    Data  1.212 ( 1.160)    Loss 4.1599e+00 (4.5834e+00)    Acc@1  10.94 (  7.23)   Acc@5  29.69 ( 21.63)
Epoch: [0][601/625]     Time  1.532 ( 1.614)    Data  1.064 ( 1.150)    Loss 3.8593e+00 (4.4699e+00)    Acc@1  10.94 (  8.51)   Acc@5  38.28 ( 24.18)
train Loss: 4.4459 Acc: 0.0882
Epoch: [0][  1/625]     Time  1.314 ( 1.612)    Data  1.169 ( 1.148)    Loss 4.0304e+00 (4.4452e+00)    Acc@1   7.03 (  8.82)   Acc@5  33.59 ( 24.75)
Epoch: [0][101/625]     Time  1.223 ( 1.563)    Data  1.073 ( 1.141)    Loss 3.9303e+00 (4.3819e+00)    Acc@1  15.62 (  9.53)   Acc@5  35.94 ( 26.12)
val Loss: 3.9742 Acc: 0.1416

Epoch 1/39
----------
Epoch: [1][  1/625]     Time  1.061 ( 1.061)    Data  0.588 ( 0.588)    Loss 3.6528e+00 (3.6528e+00)    Acc@1  21.88 ( 21.88)   Acc@5  42.97 ( 42.97)
Epoch: [1][101/625]     Time  0.690 ( 0.734)    Data  0.227 ( 0.267)    Loss 3.5130e+00 (3.6861e+00)    Acc@1  21.09 ( 17.88)   Acc@5  43.75 ( 41.67)
Epoch: [1][201/625]     Time  0.702 ( 0.720)    Data  0.237 ( 0.252)    Loss 3.6805e+00 (3.6896e+00)    Acc@1  13.28 ( 18.09)   Acc@5  42.97 ( 41.86)
Epoch: [1][301/625]     Time  0.695 ( 0.720)    Data  0.228 ( 0.252)    Loss 3.6412e+00 (3.6402e+00)    Acc@1  17.97 ( 19.25)   Acc@5  42.19 ( 43.30)
Epoch: [1][401/625]     Time  0.690 ( 0.717)    Data  0.223 ( 0.250)    Loss 3.3009e+00 (3.5897e+00)    Acc@1  26.56 ( 20.20)   Acc@5  52.34 ( 44.54)
Epoch: [1][501/625]     Time  0.688 ( 0.713)    Data  0.221 ( 0.246)    Loss 3.3935e+00 (3.5452e+00)    Acc@1  17.19 ( 20.93)   Acc@5  50.00 ( 45.57)
Epoch: [1][601/625]     Time  0.686 ( 0.709)    Data  0.221 ( 0.243)    Loss 2.9363e+00 (3.5089e+00)    Acc@1  33.59 ( 21.60)   Acc@5  60.94 ( 46.43)
train Loss: 3.4976 Acc: 0.2178
Epoch: [1][  1/625]     Time  0.448 ( 0.708)    Data  0.297 ( 0.243)    Loss 3.1674e+00 (3.4971e+00)    Acc@1  27.34 ( 21.79)   Acc@5  56.25 ( 46.71)
Epoch: [1][101/625]     Time  0.371 ( 0.662)    Data  0.220 ( 0.240)    Loss 3.6453e+00 (3.5133e+00)    Acc@1  19.53 ( 21.73)   Acc@5  42.19 ( 46.54)
val Loss: 3.5960 Acc: 0.2141

Epoch 2/39
----------
Epoch: [2][  1/625]     Time  0.859 ( 0.859)    Data  0.391 ( 0.391)    Loss 3.0332e+00 (3.0332e+00)    Acc@1  32.03 ( 32.03)   Acc@5  53.12 ( 53.12)
Epoch: [2][101/625]     Time  0.686 ( 0.695)    Data  0.220 ( 0.231)    Loss 3.2738e+00 (3.1716e+00)    Acc@1  24.22 ( 27.60)   Acc@5  52.34 ( 54.00)
Epoch: [2][201/625]     Time  0.691 ( 0.697)    Data  0.226 ( 0.233)    Loss 2.7922e+00 (3.1560e+00)    Acc@1  36.72 ( 27.81)   Acc@5  60.16 ( 54.25)
Epoch: [2][301/625]     Time  0.683 ( 0.696)    Data  0.222 ( 0.232)    Loss 2.9225e+00 (3.1464e+00)    Acc@1  28.91 ( 28.25)   Acc@5  60.94 ( 54.40)
Epoch: [2][401/625]     Time  0.690 ( 0.695)    Data  0.224 ( 0.232)    Loss 3.0685e+00 (3.1360e+00)    Acc@1  32.81 ( 28.41)   Acc@5  54.69 ( 54.67)
Epoch: [2][501/625]     Time  0.677 ( 0.695)    Data  0.220 ( 0.231)    Loss 2.9690e+00 (3.1128e+00)    Acc@1  31.25 ( 28.74)   Acc@5  61.72 ( 55.17)
Epoch: [2][601/625]     Time  0.689 ( 0.695)    Data  0.223 ( 0.231)    Loss 3.1484e+00 (3.0956e+00)    Acc@1  32.81 ( 29.23)   Acc@5  54.69 ( 55.65)
train Loss: 3.0916 Acc: 0.2931
Epoch: [2][  1/625]     Time  0.453 ( 0.694)    Data  0.303 ( 0.231)    Loss 3.2325e+00 (3.0919e+00)    Acc@1  32.03 ( 29.31)   Acc@5  60.16 ( 55.73)
Epoch: [2][101/625]     Time  0.375 ( 0.651)    Data  0.222 ( 0.231)    Loss 3.2633e+00 (3.1235e+00)    Acc@1  31.25 ( 28.81)   Acc@5  54.69 ( 55.14)
val Loss: 3.3352 Acc: 0.2573

Epoch 3/39
----------
Epoch: [3][  1/625]     Time  1.009 ( 1.009)    Data  0.543 ( 0.543)    Loss 2.7270e+00 (2.7270e+00)    Acc@1  27.34 ( 27.34)   Acc@5  67.97 ( 67.97)
Epoch: [3][101/625]     Time  0.689 ( 0.697)    Data  0.223 ( 0.230)    Loss 2.9422e+00 (2.8660e+00)    Acc@1  28.91 ( 33.54)   Acc@5  57.81 ( 60.55)
Epoch: [3][201/625]     Time  0.688 ( 0.698)    Data  0.223 ( 0.232)    Loss 3.0463e+00 (2.8676e+00)    Acc@1  31.25 ( 33.61)   Acc@5  59.38 ( 60.28)
Epoch: [3][301/625]     Time  0.701 ( 0.695)    Data  0.236 ( 0.230)    Loss 3.0103e+00 (2.8957e+00)    Acc@1  32.03 ( 33.19)   Acc@5  56.25 ( 59.69)
Epoch: [3][401/625]     Time  0.690 ( 0.695)    Data  0.222 ( 0.230)    Loss 2.4695e+00 (2.8927e+00)    Acc@1  40.62 ( 33.25)   Acc@5  67.97 ( 59.75)
Epoch: [3][501/625]     Time  0.687 ( 0.694)    Data  0.221 ( 0.230)    Loss 2.9550e+00 (2.8831e+00)    Acc@1  32.81 ( 33.42)   Acc@5  57.81 ( 59.87)
Epoch: [3][601/625]     Time  0.693 ( 0.695)    Data  0.226 ( 0.230)    Loss 2.9851e+00 (2.8705e+00)    Acc@1  29.69 ( 33.61)   Acc@5  57.81 ( 60.17)
train Loss: 2.8668 Acc: 0.3368
Epoch: [3][  1/625]     Time  0.451 ( 0.694)    Data  0.303 ( 0.230)    Loss 3.0599e+00 (2.8671e+00)    Acc@1  32.81 ( 33.68)   Acc@5  53.12 ( 60.24)
Epoch: [3][101/625]     Time  0.369 ( 0.651)    Data  0.221 ( 0.230)    Loss 3.3684e+00 (2.9128e+00)    Acc@1  19.53 ( 32.89)   Acc@5  52.34 ( 59.32)
val Loss: 3.1924 Acc: 0.2827

Epoch 4/39
----------
Epoch: [4][  1/625]     Time  0.867 ( 0.867)    Data  0.399 ( 0.399)    Loss 2.8504e+00 (2.8504e+00)    Acc@1  35.16 ( 35.16)   Acc@5  59.38 ( 59.38)
Epoch: [4][101/625]     Time  0.688 ( 0.700)    Data  0.223 ( 0.236)    Loss 2.6403e+00 (2.6824e+00)    Acc@1  38.28 ( 37.35)   Acc@5  66.41 ( 63.81)
Epoch: [4][201/625]     Time  0.686 ( 0.695)    Data  0.221 ( 0.231)    Loss 2.5570e+00 (2.6926e+00)    Acc@1  37.50 ( 36.85)   Acc@5  67.97 ( 63.73)
Epoch: [4][301/625]     Time  0.684 ( 0.694)    Data  0.225 ( 0.230)    Loss 2.7953e+00 (2.6985e+00)    Acc@1  35.94 ( 36.87)   Acc@5  61.72 ( 63.69)
Epoch: [4][401/625]     Time  0.681 ( 0.695)    Data  0.222 ( 0.231)    Loss 2.4955e+00 (2.6989e+00)    Acc@1  45.31 ( 36.97)   Acc@5  70.31 ( 63.58)
Epoch: [4][501/625]     Time  0.681 ( 0.694)    Data  0.224 ( 0.230)    Loss 2.8256e+00 (2.6989e+00)    Acc@1  35.94 ( 37.06)   Acc@5  64.84 ( 63.64)
Epoch: [4][601/625]     Time  0.687 ( 0.694)    Data  0.222 ( 0.230)    Loss 2.5284e+00 (2.6971e+00)    Acc@1  39.06 ( 37.14)   Acc@5  64.84 ( 63.61)
train Loss: 2.6956 Acc: 0.3715
Epoch: [4][  1/625]     Time  0.454 ( 0.694)    Data  0.306 ( 0.230)    Loss 3.1047e+00 (2.6962e+00)    Acc@1  33.59 ( 37.15)   Acc@5  54.69 ( 63.63)
Epoch: [4][101/625]     Time  0.373 ( 0.663)    Data  0.225 ( 0.242)    Loss 2.8627e+00 (2.7345e+00)    Acc@1  39.06 ( 36.61)   Acc@5  62.50 ( 62.97)
val Loss: 2.9808 Acc: 0.3301

Epoch 5/39
----------
Epoch: [5][  1/625]     Time  1.028 ( 1.028)    Data  0.570 ( 0.570)    Loss 2.5239e+00 (2.5239e+00)    Acc@1  41.41 ( 41.41)   Acc@5  65.62 ( 65.62)
Epoch: [5][101/625]     Time  0.693 ( 0.707)    Data  0.227 ( 0.243)    Loss 2.7840e+00 (2.5372e+00)    Acc@1  32.81 ( 40.45)   Acc@5  63.28 ( 66.60)
Epoch: [5][201/625]     Time  0.690 ( 0.704)    Data  0.232 ( 0.240)    Loss 2.8781e+00 (2.5653e+00)    Acc@1  35.16 ( 39.94)   Acc@5  57.03 ( 66.18)
Epoch: [5][301/625]     Time  0.696 ( 0.702)    Data  0.231 ( 0.238)    Loss 2.6567e+00 (2.5677e+00)    Acc@1  38.28 ( 39.63)   Acc@5  64.84 ( 66.25)
Epoch: [5][401/625]     Time  0.696 ( 0.702)    Data  0.231 ( 0.238)    Loss 3.0887e+00 (2.5724e+00)    Acc@1  27.34 ( 39.62)   Acc@5  55.47 ( 66.08)
Epoch: [5][501/625]     Time  0.699 ( 0.704)    Data  0.233 ( 0.240)    Loss 2.4960e+00 (2.5709e+00)    Acc@1  38.28 ( 39.49)   Acc@5  68.75 ( 66.17)
Epoch: [5][601/625]     Time  0.694 ( 0.704)    Data  0.229 ( 0.240)    Loss 2.6156e+00 (2.5711e+00)    Acc@1  35.16 ( 39.41)   Acc@5  66.41 ( 66.09)
train Loss: 2.5707 Acc: 0.3943
Epoch: [5][  1/625]     Time  0.483 ( 0.703)    Data  0.336 ( 0.240)    Loss 2.8107e+00 (2.5710e+00)    Acc@1  30.47 ( 39.42)   Acc@5  64.06 ( 66.04)
Epoch: [5][101/625]     Time  0.391 ( 0.660)    Data  0.244 ( 0.239)    Loss 2.6189e+00 (2.6133e+00)    Acc@1  35.94 ( 38.68)   Acc@5  63.28 ( 65.29)
val Loss: 2.8962 Acc: 0.3387

Epoch 6/39
----------
Epoch: [6][  1/625]     Time  0.880 ( 0.880)    Data  0.414 ( 0.414)    Loss 2.4040e+00 (2.4040e+00)    Acc@1  46.88 ( 46.88)   Acc@5  68.75 ( 68.75)
Epoch: [6][101/625]     Time  0.699 ( 0.708)    Data  0.233 ( 0.245)    Loss 2.3739e+00 (2.4460e+00)    Acc@1  42.97 ( 42.00)   Acc@5  66.41 ( 68.11)
Epoch: [6][201/625]     Time  0.694 ( 0.708)    Data  0.230 ( 0.244)    Loss 2.5706e+00 (2.4507e+00)    Acc@1  38.28 ( 42.06)   Acc@5  66.41 ( 67.99)
Epoch: [6][301/625]     Time  0.695 ( 0.710)    Data  0.230 ( 0.246)    Loss 2.4456e+00 (2.4502e+00)    Acc@1  39.84 ( 42.06)   Acc@5  69.53 ( 68.01)
Epoch: [6][401/625]     Time  0.698 ( 0.709)    Data  0.233 ( 0.244)    Loss 2.1610e+00 (2.4612e+00)    Acc@1  41.41 ( 41.73)   Acc@5  75.00 ( 67.89)
Epoch: [6][501/625]     Time  0.697 ( 0.708)    Data  0.232 ( 0.243)    Loss 2.5417e+00 (2.4655e+00)    Acc@1  38.28 ( 41.67)   Acc@5  62.50 ( 67.90)
Epoch: [6][601/625]     Time  0.696 ( 0.707)    Data  0.231 ( 0.243)    Loss 2.5052e+00 (2.4724e+00)    Acc@1  42.97 ( 41.59)   Acc@5  63.28 ( 67.78)
train Loss: 2.4733 Acc: 0.4156
Epoch: [6][  1/625]     Time  0.619 ( 0.707)    Data  0.471 ( 0.244)    Loss 3.0014e+00 (2.4741e+00)    Acc@1  31.25 ( 41.54)   Acc@5  53.91 ( 67.71)
Epoch: [6][101/625]     Time  0.376 ( 0.663)    Data  0.227 ( 0.243)    Loss 2.7350e+00 (2.5265e+00)    Acc@1  38.28 ( 40.62)   Acc@5  61.72 ( 66.76)
val Loss: 2.8547 Acc: 0.3479

Epoch 7/39
----------
Epoch: [7][  1/625]     Time  0.929 ( 0.929)    Data  0.467 ( 0.467)    Loss 2.0476e+00 (2.0476e+00)    Acc@1  46.88 ( 46.88)   Acc@5  77.34 ( 77.34)
Epoch: [7][101/625]     Time  0.698 ( 0.711)    Data  0.231 ( 0.247)    Loss 2.4263e+00 (2.3304e+00)    Acc@1  42.19 ( 44.69)   Acc@5  70.31 ( 70.18)
Epoch: [7][201/625]     Time  0.697 ( 0.709)    Data  0.231 ( 0.245)    Loss 2.2809e+00 (2.3430e+00)    Acc@1  42.19 ( 44.55)   Acc@5  71.88 ( 70.01)
Epoch: [7][301/625]     Time  0.783 ( 0.723)    Data  0.314 ( 0.271)    Loss 1.9456e+00 (2.3600e+00)    Acc@1  50.00 ( 44.17)   Acc@5  72.66 ( 69.68)
Epoch: [7][401/625]     Time  0.698 ( 0.720)    Data  0.234 ( 0.265)    Loss 2.0991e+00 (2.3700e+00)    Acc@1  46.09 ( 43.79)   Acc@5  75.78 ( 69.47)
Epoch: [7][501/625]     Time  0.696 ( 0.722)    Data  0.230 ( 0.265)    Loss 2.0574e+00 (2.3707e+00)    Acc@1  55.47 ( 43.70)   Acc@5  73.44 ( 69.47)
Epoch: [7][601/625]     Time  0.700 ( 0.720)    Data  0.236 ( 0.262)    Loss 2.2857e+00 (2.3731e+00)    Acc@1  49.22 ( 43.60)   Acc@5  69.53 ( 69.45)
train Loss: 2.3743 Acc: 0.4359
Epoch: [7][  1/625]     Time  0.508 ( 0.719)    Data  0.358 ( 0.262)    Loss 2.7835e+00 (2.3749e+00)    Acc@1  35.16 ( 43.58)   Acc@5  60.16 ( 69.43)
Epoch: [7][101/625]     Time  0.379 ( 0.674)    Data  0.231 ( 0.259)    Loss 2.6133e+00 (2.4423e+00)    Acc@1  38.28 ( 42.44)   Acc@5  67.19 ( 68.31)
val Loss: 2.8700 Acc: 0.3508

Epoch 8/39
----------
Epoch: [8][  1/625]     Time  1.042 ( 1.042)    Data  0.575 ( 0.575)    Loss 2.2965e+00 (2.2965e+00)    Acc@1  43.75 ( 43.75)   Acc@5  69.53 ( 69.53)
Epoch: [8][101/625]     Time  0.696 ( 0.731)    Data  0.234 ( 0.268)    Loss 2.3179e+00 (2.2337e+00)    Acc@1  47.66 ( 46.53)   Acc@5  67.97 ( 72.04)
Epoch: [8][201/625]     Time  0.696 ( 0.720)    Data  0.230 ( 0.257)    Loss 2.1717e+00 (2.2491e+00)    Acc@1  50.78 ( 46.05)   Acc@5  77.34 ( 71.83)
Epoch: [8][301/625]     Time  0.700 ( 0.719)    Data  0.235 ( 0.256)    Loss 2.4832e+00 (2.2672e+00)    Acc@1  42.97 ( 45.74)   Acc@5  66.41 ( 71.51)
Epoch: [8][401/625]     Time  1.402 ( 0.759)    Data  0.939 ( 0.295)    Loss 2.4841e+00 (2.2818e+00)    Acc@1  39.84 ( 45.46)   Acc@5  68.75 ( 71.16)
Epoch: [8][501/625]     Time  1.397 ( 0.893)    Data  0.932 ( 0.428)    Loss 2.4117e+00 (2.2907e+00)    Acc@1  42.97 ( 45.30)   Acc@5  66.41 ( 70.92)
Epoch: [8][601/625]     Time  1.401 ( 0.982)    Data  0.936 ( 0.518)    Loss 2.3019e+00 (2.2959e+00)    Acc@1  48.44 ( 45.12)   Acc@5  74.22 ( 70.85)
train Loss: 2.2971 Acc: 0.4510
Epoch: [8][  1/625]     Time  1.192 ( 0.999)    Data  1.044 ( 0.535)    Loss 2.7868e+00 (2.2979e+00)    Acc@1  40.62 ( 45.09)   Acc@5  60.16 ( 70.83)
Epoch: [8][101/625]     Time  1.088 ( 1.014)    Data  0.940 ( 0.592)    Loss 2.7231e+00 (2.3564e+00)    Acc@1  42.19 ( 44.02)   Acc@5  64.06 ( 69.82)
val Loss: 2.7187 Acc: 0.3759

Epoch 9/39
----------
Epoch: [9][  1/625]     Time  1.330 ( 1.330)    Data  0.867 ( 0.867)    Loss 2.1448e+00 (2.1448e+00)    Acc@1  49.22 ( 49.22)   Acc@5  71.88 ( 71.88)
Epoch: [9][101/625]     Time  1.153 ( 1.158)    Data  0.689 ( 0.694)    Loss 2.2459e+00 (2.2001e+00)    Acc@1  40.62 ( 47.18)   Acc@5  75.00 ( 72.65)
Epoch: [9][201/625]     Time  1.151 ( 1.162)    Data  0.690 ( 0.698)    Loss 2.2855e+00 (2.2107e+00)    Acc@1  44.53 ( 46.91)   Acc@5  70.31 ( 72.40)
Epoch: [9][301/625]     Time  1.147 ( 1.160)    Data  0.685 ( 0.696)    Loss 2.2747e+00 (2.2311e+00)    Acc@1  46.09 ( 46.41)   Acc@5  72.66 ( 72.08)
Epoch: [9][401/625]     Time  1.136 ( 1.157)    Data  0.671 ( 0.694)    Loss 2.2776e+00 (2.2403e+00)    Acc@1  45.31 ( 46.27)   Acc@5  72.66 ( 71.88)
Epoch: [9][501/625]     Time  1.143 ( 1.157)    Data  0.679 ( 0.694)    Loss 2.4717e+00 (2.2489e+00)    Acc@1  46.09 ( 46.12)   Acc@5  69.53 ( 71.69)
Epoch: [9][601/625]     Time  1.190 ( 1.156)    Data  0.722 ( 0.693)    Loss 2.3107e+00 (2.2504e+00)    Acc@1  50.00 ( 46.15)   Acc@5  74.22 ( 71.67)
train Loss: 2.2507 Acc: 0.4613
Epoch: [9][  1/625]     Time  0.483 ( 1.154)    Data  0.333 ( 0.692)    Loss 2.8266e+00 (2.2516e+00)    Acc@1  31.25 ( 46.10)   Acc@5  67.19 ( 71.66)
Epoch: [9][101/625]     Time  0.375 ( 1.049)    Data  0.228 ( 0.629)    Loss 2.8083e+00 (2.3298e+00)    Acc@1  33.59 ( 44.75)   Acc@5  60.16 ( 70.25)
val Loss: 2.8136 Acc: 0.3619

Epoch 10/39
----------
Epoch: [10][  1/625]    Time  0.989 ( 0.989)    Data  0.533 ( 0.533)    Loss 1.9320e+00 (1.9320e+00)    Acc@1  52.34 ( 52.34)   Acc@5  75.00 ( 75.00)
Epoch: [10][101/625]    Time  0.686 ( 0.722)    Data  0.227 ( 0.260)    Loss 2.5500e+00 (2.1067e+00)    Acc@1  42.19 ( 48.97)   Acc@5  67.19 ( 74.13)
Epoch: [10][201/625]    Time  0.706 ( 0.709)    Data  0.239 ( 0.247)    Loss 2.4276e+00 (2.1392e+00)    Acc@1  49.22 ( 48.37)   Acc@5  69.53 ( 73.63)
Epoch: [10][301/625]    Time  0.695 ( 0.706)    Data  0.229 ( 0.243)    Loss 1.9798e+00 (2.1508e+00)    Acc@1  46.88 ( 47.96)   Acc@5  74.22 ( 73.45)
Epoch: [10][401/625]    Time  0.695 ( 0.712)    Data  0.229 ( 0.249)    Loss 2.2623e+00 (2.1682e+00)    Acc@1  49.22 ( 47.63)   Acc@5  73.44 ( 73.17)
Epoch: [10][501/625]    Time  0.695 ( 0.710)    Data  0.236 ( 0.246)    Loss 2.3811e+00 (2.1793e+00)    Acc@1  49.22 ( 47.56)   Acc@5  71.88 ( 72.99)
Epoch: [10][601/625]    Time  0.696 ( 0.708)    Data  0.231 ( 0.245)    Loss 2.2484e+00 (2.1827e+00)    Acc@1  49.22 ( 47.52)   Acc@5  72.66 ( 72.98)
train Loss: 2.1865 Acc: 0.4747
Epoch: [10][  1/625]    Time  0.478 ( 0.710)    Data  0.331 ( 0.247)    Loss 2.6663e+00 (2.1873e+00)    Acc@1  44.53 ( 47.46)   Acc@5  64.84 ( 72.88)
Epoch: [10][101/625]    Time  0.381 ( 0.666)    Data  0.232 ( 0.246)    Loss 2.8085e+00 (2.2543e+00)    Acc@1  37.50 ( 46.26)   Acc@5  58.59 ( 71.79)
val Loss: 2.6702 Acc: 0.3891

Epoch 11/39
----------
Epoch: [11][  1/625]    Time  0.889 ( 0.889)    Data  0.419 ( 0.419)    Loss 1.9587e+00 (1.9587e+00)    Acc@1  55.47 ( 55.47)   Acc@5  76.56 ( 76.56)
Epoch: [11][101/625]    Time  0.707 ( 0.710)    Data  0.242 ( 0.245)    Loss 1.9797e+00 (2.0493e+00)    Acc@1  53.12 ( 50.71)   Acc@5  76.56 ( 74.89)
Epoch: [11][201/625]    Time  0.689 ( 0.717)    Data  0.231 ( 0.253)    Loss 2.0855e+00 (2.1001e+00)    Acc@1  43.75 ( 49.49)   Acc@5  76.56 ( 74.16)
Epoch: [11][301/625]    Time  0.689 ( 0.712)    Data  0.231 ( 0.248)    Loss 2.1818e+00 (2.1168e+00)    Acc@1  48.44 ( 49.08)   Acc@5  77.34 ( 73.78)
Epoch: [11][401/625]    Time  0.697 ( 0.710)    Data  0.232 ( 0.245)    Loss 2.1076e+00 (2.1256e+00)    Acc@1  47.66 ( 48.92)   Acc@5  71.09 ( 73.63)
Epoch: [11][501/625]    Time  0.701 ( 0.711)    Data  0.234 ( 0.247)    Loss 2.3681e+00 (2.1317e+00)    Acc@1  49.22 ( 48.71)   Acc@5  70.31 ( 73.55)
Epoch: [11][601/625]    Time  0.693 ( 0.710)    Data  0.235 ( 0.245)    Loss 2.4614e+00 (2.1422e+00)    Acc@1  44.53 ( 48.55)   Acc@5  67.19 ( 73.46)
train Loss: 2.1436 Acc: 0.4853
Epoch: [11][  1/625]    Time  0.484 ( 0.710)    Data  0.335 ( 0.246)    Loss 2.4132e+00 (2.1440e+00)    Acc@1  39.84 ( 48.52)   Acc@5  67.97 ( 73.41)
Epoch: [11][101/625]    Time  0.381 ( 0.667)    Data  0.230 ( 0.246)    Loss 2.4119e+00 (2.2108e+00)    Acc@1  40.62 ( 47.24)   Acc@5  65.62 ( 72.31)
val Loss: 2.6305 Acc: 0.3904

Epoch 12/39
----------
Epoch: [12][  1/625]    Time  1.048 ( 1.048)    Data  0.584 ( 0.584)    Loss 1.9031e+00 (1.9031e+00)    Acc@1  48.44 ( 48.44)   Acc@5  80.47 ( 80.47)
Epoch: [12][101/625]    Time  0.699 ( 0.718)    Data  0.234 ( 0.251)    Loss 2.0975e+00 (2.0111e+00)    Acc@1  51.56 ( 50.97)   Acc@5  70.31 ( 75.95)
Epoch: [12][201/625]    Time  0.685 ( 0.710)    Data  0.228 ( 0.244)    Loss 1.8178e+00 (2.0427e+00)    Acc@1  55.47 ( 50.41)   Acc@5  78.12 ( 75.36)
Epoch: [12][301/625]    Time  0.696 ( 0.712)    Data  0.230 ( 0.247)    Loss 2.0172e+00 (2.0675e+00)    Acc@1  53.12 ( 49.69)   Acc@5  80.47 ( 75.06)
Epoch: [12][401/625]    Time  0.696 ( 0.710)    Data  0.231 ( 0.245)    Loss 2.1073e+00 (2.0861e+00)    Acc@1  48.44 ( 49.47)   Acc@5  75.78 ( 74.58)
Epoch: [12][501/625]    Time  0.709 ( 0.712)    Data  0.252 ( 0.247)    Loss 2.2055e+00 (2.0912e+00)    Acc@1  45.31 ( 49.34)   Acc@5  67.97 ( 74.43)
Epoch: [12][601/625]    Time  0.698 ( 0.711)    Data  0.231 ( 0.246)    Loss 2.0864e+00 (2.1032e+00)    Acc@1  46.88 ( 49.16)   Acc@5  73.44 ( 74.20)
train Loss: 2.1064 Acc: 0.4911
Epoch: [12][  1/625]    Time  0.484 ( 0.710)    Data  0.333 ( 0.246)    Loss 2.8366e+00 (2.1075e+00)    Acc@1  32.03 ( 49.08)   Acc@5  64.06 ( 74.10)
Epoch: [12][101/625]    Time  0.380 ( 0.666)    Data  0.231 ( 0.245)    Loss 2.5927e+00 (2.1778e+00)    Acc@1  41.41 ( 47.83)   Acc@5  65.62 ( 72.88)
val Loss: 2.6291 Acc: 0.3965

Epoch 13/39
----------
Epoch: [13][  1/625]    Time  0.937 ( 0.937)    Data  0.477 ( 0.477)    Loss 1.8419e+00 (1.8419e+00)    Acc@1  51.56 ( 51.56)   Acc@5  77.34 ( 77.34)
Epoch: [13][101/625]    Time  0.691 ( 0.728)    Data  0.234 ( 0.264)    Loss 1.8803e+00 (2.0034e+00)    Acc@1  53.91 ( 51.07)   Acc@5  76.56 ( 76.11)
Epoch: [13][201/625]    Time  0.798 ( 0.717)    Data  0.334 ( 0.252)    Loss 1.9724e+00 (2.0273e+00)    Acc@1  48.44 ( 50.54)   Acc@5  76.56 ( 75.47)
Epoch: [13][301/625]    Time  0.700 ( 0.718)    Data  0.234 ( 0.254)    Loss 1.9200e+00 (2.0356e+00)    Acc@1  51.56 ( 50.55)   Acc@5  74.22 ( 75.38)
Epoch: [13][401/625]    Time  0.693 ( 0.715)    Data  0.234 ( 0.250)    Loss 2.3269e+00 (2.0461e+00)    Acc@1  43.75 ( 50.41)   Acc@5  70.31 ( 75.23)
Epoch: [13][501/625]    Time  0.701 ( 0.712)    Data  0.234 ( 0.248)    Loss 2.2579e+00 (2.0523e+00)    Acc@1  47.66 ( 50.17)   Acc@5  71.88 ( 75.14)
Epoch: [13][601/625]    Time  0.699 ( 0.714)    Data  0.234 ( 0.250)    Loss 1.9117e+00 (2.0565e+00)    Acc@1  50.78 ( 50.12)   Acc@5  76.56 ( 75.02)
train Loss: 2.0592 Acc: 0.5007
Epoch: [13][  1/625]    Time  0.488 ( 0.713)    Data  0.338 ( 0.249)    Loss 3.0333e+00 (2.0607e+00)    Acc@1  33.59 ( 50.04)   Acc@5  56.25 ( 74.94)
Epoch: [13][101/625]    Time  0.378 ( 0.668)    Data  0.230 ( 0.248)    Loss 2.9454e+00 (2.1647e+00)    Acc@1  35.94 ( 48.16)   Acc@5  64.06 ( 73.35)
val Loss: 2.8225 Acc: 0.3654

Epoch 14/39
----------
Epoch: [14][  1/625]    Time  0.984 ( 0.984)    Data  0.527 ( 0.527)    Loss 2.1483e+00 (2.1483e+00)    Acc@1  44.53 ( 44.53)   Acc@5  74.22 ( 74.22)
Epoch: [14][101/625]    Time  0.697 ( 0.739)    Data  0.230 ( 0.275)    Loss 1.6313e+00 (1.9780e+00)    Acc@1  61.72 ( 51.83)   Acc@5  78.91 ( 76.55)
Epoch: [14][201/625]    Time  0.699 ( 0.723)    Data  0.232 ( 0.259)    Loss 2.1532e+00 (1.9764e+00)    Acc@1  47.66 ( 51.85)   Acc@5  72.66 ( 76.41)
Epoch: [14][301/625]    Time  0.700 ( 0.717)    Data  0.234 ( 0.253)    Loss 1.9403e+00 (1.9991e+00)    Acc@1  57.81 ( 51.47)   Acc@5  77.34 ( 75.88)
Epoch: [14][401/625]    Time  0.800 ( 0.719)    Data  0.332 ( 0.254)    Loss 2.2573e+00 (2.0114e+00)    Acc@1  46.09 ( 51.22)   Acc@5  72.66 ( 75.58)
Epoch: [14][501/625]    Time  0.695 ( 0.716)    Data  0.230 ( 0.252)    Loss 1.9115e+00 (2.0210e+00)    Acc@1  54.69 ( 50.97)   Acc@5  75.78 ( 75.38)
Epoch: [14][601/625]    Time  0.688 ( 0.718)    Data  0.230 ( 0.254)    Loss 1.8173e+00 (2.0279e+00)    Acc@1  53.91 ( 50.78)   Acc@5  78.91 ( 75.27)
train Loss: 2.0315 Acc: 0.5071
Epoch: [14][  1/625]    Time  0.482 ( 0.717)    Data  0.333 ( 0.254)    Loss 2.8206e+00 (2.0328e+00)    Acc@1  34.38 ( 50.69)   Acc@5  62.50 ( 75.18)
Epoch: [14][101/625]    Time  0.392 ( 0.671)    Data  0.242 ( 0.251)    Loss 2.8206e+00 (2.1341e+00)    Acc@1  39.84 ( 48.85)   Acc@5  64.06 ( 73.56)
val Loss: 2.7709 Acc: 0.3738

Epoch 15/39
----------
Epoch: [15][  1/625]    Time  0.831 ( 0.831)    Data  0.369 ( 0.369)    Loss 1.8779e+00 (1.8779e+00)    Acc@1  52.34 ( 52.34)   Acc@5  79.69 ( 79.69)
Epoch: [15][101/625]    Time  0.695 ( 0.712)    Data  0.228 ( 0.247)    Loss 2.0687e+00 (1.9110e+00)    Acc@1  46.88 ( 52.92)   Acc@5  72.66 ( 77.22)
Epoch: [15][201/625]    Time  0.696 ( 0.714)    Data  0.228 ( 0.249)    Loss 1.9505e+00 (1.9272e+00)    Acc@1  52.34 ( 52.72)   Acc@5  74.22 ( 76.82)
Epoch: [15][301/625]    Time  0.698 ( 0.710)    Data  0.230 ( 0.245)    Loss 2.0721e+00 (1.9546e+00)    Acc@1  46.88 ( 52.19)   Acc@5  72.66 ( 76.39)
Epoch: [15][401/625]    Time  1.022 ( 0.713)    Data  0.564 ( 0.248)    Loss 2.1036e+00 (1.9772e+00)    Acc@1  46.88 ( 51.78)   Acc@5  68.75 ( 76.18)
Epoch: [15][501/625]    Time  0.698 ( 0.711)    Data  0.232 ( 0.246)    Loss 2.2428e+00 (1.9877e+00)    Acc@1  44.53 ( 51.61)   Acc@5  75.00 ( 76.03)
Epoch: [15][601/625]    Time  0.696 ( 0.710)    Data  0.230 ( 0.245)    Loss 1.8806e+00 (1.9960e+00)    Acc@1  53.91 ( 51.36)   Acc@5  78.12 ( 75.92)
train Loss: 1.9992 Acc: 0.5129
Epoch: [15][  1/625]    Time  0.622 ( 0.709)    Data  0.475 ( 0.245)    Loss 2.7133e+00 (2.0004e+00)    Acc@1  41.41 ( 51.27)   Acc@5  64.84 ( 75.82)
Epoch: [15][101/625]    Time  0.383 ( 0.669)    Data  0.233 ( 0.248)    Loss 2.8826e+00 (2.0961e+00)    Acc@1  35.94 ( 49.67)   Acc@5  63.28 ( 74.29)
val Loss: 2.7060 Acc: 0.3945

Epoch 16/39
----------
Epoch: [16][  1/625]    Time  0.841 ( 0.841)    Data  0.375 ( 0.375)    Loss 1.8740e+00 (1.8740e+00)    Acc@1  53.91 ( 53.91)   Acc@5  76.56 ( 76.56)
Epoch: [16][101/625]    Time  0.693 ( 0.710)    Data  0.230 ( 0.245)    Loss 2.0740e+00 (1.8798e+00)    Acc@1  47.66 ( 54.18)   Acc@5  71.09 ( 77.85)
Epoch: [16][201/625]    Time  0.688 ( 0.716)    Data  0.230 ( 0.251)    Loss 2.2937e+00 (1.9040e+00)    Acc@1  42.19 ( 53.76)   Acc@5  71.09 ( 77.62)
Epoch: [16][301/625]    Time  0.698 ( 0.711)    Data  0.232 ( 0.247)    Loss 1.6067e+00 (1.9196e+00)    Acc@1  59.38 ( 53.26)   Acc@5  84.38 ( 77.31)
Epoch: [16][401/625]    Time  0.696 ( 0.708)    Data  0.230 ( 0.243)    Loss 1.9696e+00 (1.9359e+00)    Acc@1  51.56 ( 52.83)   Acc@5  76.56 ( 76.92)
Epoch: [16][501/625]    Time  0.696 ( 0.710)    Data  0.230 ( 0.245)    Loss 1.8889e+00 (1.9534e+00)    Acc@1  55.47 ( 52.45)   Acc@5  78.12 ( 76.67)
Epoch: [16][601/625]    Time  0.700 ( 0.709)    Data  0.234 ( 0.244)    Loss 2.1160e+00 (1.9634e+00)    Acc@1  50.00 ( 52.14)   Acc@5  72.66 ( 76.55)
train Loss: 1.9662 Acc: 0.5210
Epoch: [16][  1/625]    Time  0.481 ( 0.708)    Data  0.332 ( 0.244)    Loss 2.7243e+00 (1.9675e+00)    Acc@1  36.72 ( 52.07)   Acc@5  60.94 ( 76.46)
Epoch: [16][101/625]    Time  0.374 ( 0.664)    Data  0.226 ( 0.243)    Loss 2.4671e+00 (2.0651e+00)    Acc@1  45.31 ( 50.34)   Acc@5  64.84 ( 74.81)
val Loss: 2.6801 Acc: 0.3965

Epoch 17/39
----------
Epoch: [17][  1/625]    Time  1.000 ( 1.000)    Data  0.534 ( 0.534)    Loss 1.6598e+00 (1.6598e+00)    Acc@1  60.94 ( 60.94)   Acc@5  81.25 ( 81.25)
Epoch: [17][101/625]    Time  0.696 ( 0.710)    Data  0.230 ( 0.245)    Loss 1.8253e+00 (1.8361e+00)    Acc@1  54.69 ( 55.09)   Acc@5  78.91 ( 78.67)
Epoch: [17][201/625]    Time  0.692 ( 0.706)    Data  0.227 ( 0.242)    Loss 1.9915e+00 (1.8743e+00)    Acc@1  47.66 ( 54.21)   Acc@5  77.34 ( 77.96)
Epoch: [17][301/625]    Time  0.703 ( 0.708)    Data  0.238 ( 0.244)    Loss 2.0387e+00 (1.9031e+00)    Acc@1  50.00 ( 53.51)   Acc@5  75.00 ( 77.53)
Epoch: [17][401/625]    Time  0.693 ( 0.706)    Data  0.227 ( 0.241)    Loss 1.9583e+00 (1.9199e+00)    Acc@1  49.22 ( 53.13)   Acc@5  79.69 ( 77.20)
Epoch: [17][501/625]    Time  0.693 ( 0.707)    Data  0.228 ( 0.243)    Loss 1.9573e+00 (1.9282e+00)    Acc@1  50.78 ( 52.97)   Acc@5  74.22 ( 77.03)
Epoch: [17][601/625]    Time  0.697 ( 0.706)    Data  0.231 ( 0.242)    Loss 1.9279e+00 (1.9370e+00)    Acc@1  55.47 ( 52.86)   Acc@5  78.91 ( 76.84)
train Loss: 1.9404 Acc: 0.5282
Epoch: [17][  1/625]    Time  0.479 ( 0.706)    Data  0.332 ( 0.242)    Loss 2.5576e+00 (1.9414e+00)    Acc@1  39.84 ( 52.80)   Acc@5  65.62 ( 76.75)
Epoch: [17][101/625]    Time  0.375 ( 0.661)    Data  0.227 ( 0.240)    Loss 3.0001e+00 (2.0380e+00)    Acc@1  39.06 ( 51.03)   Acc@5  60.16 ( 75.20)
val Loss: 2.6428 Acc: 0.4006

Epoch 18/39
----------
Epoch: [18][  1/625]    Time  0.887 ( 0.887)    Data  0.419 ( 0.419)    Loss 1.7360e+00 (1.7360e+00)    Acc@1  62.50 ( 62.50)   Acc@5  80.47 ( 80.47)
Epoch: [18][101/625]    Time  0.695 ( 0.734)    Data  0.230 ( 0.268)    Loss 1.8661e+00 (1.8683e+00)    Acc@1  54.69 ( 54.70)   Acc@5  84.38 ( 78.06)
Epoch: [18][201/625]    Time  0.699 ( 0.720)    Data  0.232 ( 0.255)    Loss 2.1590e+00 (1.8730e+00)    Acc@1  41.41 ( 54.38)   Acc@5  72.66 ( 78.13)
Epoch: [18][301/625]    Time  0.701 ( 0.718)    Data  0.231 ( 0.253)    Loss 1.9415e+00 (1.8790e+00)    Acc@1  50.00 ( 54.19)   Acc@5  76.56 ( 77.97)
Epoch: [18][401/625]    Time  0.713 ( 0.715)    Data  0.246 ( 0.250)    Loss 1.7566e+00 (1.8965e+00)    Acc@1  54.69 ( 53.81)   Acc@5  84.38 ( 77.66)
Epoch: [18][501/625]    Time  0.699 ( 0.713)    Data  0.233 ( 0.248)    Loss 2.0119e+00 (1.9128e+00)    Acc@1  52.34 ( 53.49)   Acc@5  71.88 ( 77.35)
Epoch: [18][601/625]    Time  0.696 ( 0.713)    Data  0.229 ( 0.248)    Loss 1.9983e+00 (1.9192e+00)    Acc@1  48.44 ( 53.25)   Acc@5  80.47 ( 77.23)
train Loss: 1.9209 Acc: 0.5321
Epoch: [18][  1/625]    Time  0.481 ( 0.712)    Data  0.333 ( 0.247)    Loss 2.7539e+00 (1.9222e+00)    Acc@1  34.38 ( 53.18)   Acc@5  64.06 ( 77.19)
Epoch: [18][101/625]    Time  0.376 ( 0.667)    Data  0.228 ( 0.246)    Loss 2.5738e+00 (2.0153e+00)    Acc@1  45.31 ( 51.41)   Acc@5  68.75 ( 75.68)
val Loss: 2.5896 Acc: 0.4084

Epoch 19/39
----------
Epoch: [19][  1/625]    Time  1.066 ( 1.066)    Data  0.599 ( 0.599)    Loss 1.7315e+00 (1.7315e+00)    Acc@1  53.91 ( 53.91)   Acc@5  81.25 ( 81.25)
Epoch: [19][101/625]    Time  0.696 ( 0.728)    Data  0.232 ( 0.263)    Loss 1.6879e+00 (1.8070e+00)    Acc@1  57.81 ( 55.59)   Acc@5  82.03 ( 79.04)
Epoch: [19][201/625]    Time  0.695 ( 0.718)    Data  0.232 ( 0.254)    Loss 2.1747e+00 (1.8341e+00)    Acc@1  47.66 ( 54.96)   Acc@5  74.22 ( 78.58)
Epoch: [19][301/625]    Time  0.696 ( 0.713)    Data  0.231 ( 0.249)    Loss 2.1263e+00 (1.8549e+00)    Acc@1  52.34 ( 54.59)   Acc@5  76.56 ( 78.33)
Epoch: [19][401/625]    Time  0.698 ( 0.713)    Data  0.232 ( 0.248)    Loss 2.1388e+00 (1.8689e+00)    Acc@1  52.34 ( 54.31)   Acc@5  75.00 ( 78.02)
Epoch: [19][501/625]    Time  0.709 ( 0.711)    Data  0.246 ( 0.246)    Loss 1.7795e+00 (1.8775e+00)    Acc@1  50.00 ( 54.08)   Acc@5  83.59 ( 77.96)
Epoch: [19][601/625]    Time  0.694 ( 0.710)    Data  0.228 ( 0.245)    Loss 2.0185e+00 (1.8903e+00)    Acc@1  53.12 ( 53.86)   Acc@5  74.22 ( 77.71)
train Loss: 1.8953 Acc: 0.5375
Epoch: [19][  1/625]    Time  0.483 ( 0.712)    Data  0.335 ( 0.248)    Loss 2.3000e+00 (1.8959e+00)    Acc@1  50.00 ( 53.75)   Acc@5  69.53 ( 77.59)
Epoch: [19][101/625]    Time  0.380 ( 0.667)    Data  0.229 ( 0.246)    Loss 2.5971e+00 (2.0021e+00)    Acc@1  41.41 ( 51.85)   Acc@5  67.97 ( 75.90)
val Loss: 2.6622 Acc: 0.4012

Epoch 20/39
----------
Epoch: [20][  1/625]    Time  0.943 ( 0.943)    Data  0.476 ( 0.476)    Loss 1.8577e+00 (1.8577e+00)    Acc@1  49.22 ( 49.22)   Acc@5  78.91 ( 78.91)
Epoch: [20][101/625]    Time  0.695 ( 0.712)    Data  0.229 ( 0.248)    Loss 1.8380e+00 (1.7768e+00)    Acc@1  60.16 ( 56.34)   Acc@5  78.12 ( 79.59)
Epoch: [20][201/625]    Time  0.692 ( 0.713)    Data  0.227 ( 0.249)    Loss 2.0540e+00 (1.8206e+00)    Acc@1  52.34 ( 55.26)   Acc@5  74.22 ( 78.81)
Epoch: [20][301/625]    Time  0.709 ( 0.715)    Data  0.245 ( 0.250)    Loss 1.7262e+00 (1.8389e+00)    Acc@1  57.03 ( 54.94)   Acc@5  81.25 ( 78.52)
Epoch: [20][401/625]    Time  1.046 ( 0.715)    Data  0.580 ( 0.251)    Loss 2.3318e+00 (1.8472e+00)    Acc@1  49.22 ( 54.75)   Acc@5  70.31 ( 78.32)
Epoch: [20][501/625]    Time  0.698 ( 0.715)    Data  0.231 ( 0.250)    Loss 1.8298e+00 (1.8595e+00)    Acc@1  55.47 ( 54.47)   Acc@5  80.47 ( 78.10)
Epoch: [20][601/625]    Time  0.697 ( 0.714)    Data  0.232 ( 0.249)    Loss 2.0934e+00 (1.8683e+00)    Acc@1  49.22 ( 54.26)   Acc@5  76.56 ( 77.97)
train Loss: 1.8697 Acc: 0.5423
Epoch: [20][  1/625]    Time  0.478 ( 0.713)    Data  0.330 ( 0.249)    Loss 2.8220e+00 (1.8712e+00)    Acc@1  37.50 ( 54.20)   Acc@5  60.94 ( 77.96)
Epoch: [20][101/625]    Time  0.377 ( 0.670)    Data  0.230 ( 0.249)    Loss 2.7264e+00 (1.9881e+00)    Acc@1  35.16 ( 52.03)   Acc@5  60.94 ( 76.04)
val Loss: 2.7142 Acc: 0.3863

Epoch 21/39
----------
Epoch: [21][  1/625]    Time  0.986 ( 0.986)    Data  0.525 ( 0.525)    Loss 1.5989e+00 (1.5989e+00)    Acc@1  59.38 ( 59.38)   Acc@5  82.03 ( 82.03)
Epoch: [21][101/625]    Time  0.696 ( 0.715)    Data  0.233 ( 0.251)    Loss 1.7291e+00 (1.7845e+00)    Acc@1  57.03 ( 56.10)   Acc@5  81.25 ( 79.17)
Epoch: [21][201/625]    Time  1.032 ( 0.711)    Data  0.574 ( 0.247)    Loss 2.0013e+00 (1.7946e+00)    Acc@1  53.91 ( 56.12)   Acc@5  78.12 ( 79.11)
Epoch: [21][301/625]    Time  0.692 ( 0.715)    Data  0.233 ( 0.250)    Loss 1.8441e+00 (1.8159e+00)    Acc@1  58.59 ( 55.69)   Acc@5  80.47 ( 78.90)
Epoch: [21][401/625]    Time  0.699 ( 0.712)    Data  0.233 ( 0.248)    Loss 1.7313e+00 (1.8280e+00)    Acc@1  57.81 ( 55.34)   Acc@5  80.47 ( 78.64)
Epoch: [21][501/625]    Time  0.696 ( 0.713)    Data  0.230 ( 0.249)    Loss 1.7988e+00 (1.8397e+00)    Acc@1  55.47 ( 55.02)   Acc@5  80.47 ( 78.47)
Epoch: [21][601/625]    Time  0.709 ( 0.712)    Data  0.252 ( 0.248)    Loss 1.7932e+00 (1.8469e+00)    Acc@1  57.03 ( 54.78)   Acc@5  81.25 ( 78.36)
train Loss: 1.8489 Acc: 0.5477
Epoch: [21][  1/625]    Time  0.484 ( 0.711)    Data  0.335 ( 0.248)    Loss 2.9449e+00 (1.8507e+00)    Acc@1  37.50 ( 54.74)   Acc@5  60.16 ( 78.27)
Epoch: [21][101/625]    Time  0.384 ( 0.667)    Data  0.232 ( 0.247)    Loss 2.8893e+00 (1.9604e+00)    Acc@1  39.84 ( 52.71)   Acc@5  62.50 ( 76.47)
val Loss: 2.6624 Acc: 0.3984

Epoch 22/39
----------
Epoch: [22][  1/625]    Time  0.840 ( 0.840)    Data  0.379 ( 0.379)    Loss 1.6813e+00 (1.6813e+00)    Acc@1  57.03 ( 57.03)   Acc@5  80.47 ( 80.47)
Epoch: [22][101/625]    Time  0.696 ( 0.731)    Data  0.230 ( 0.265)    Loss 1.7671e+00 (1.7591e+00)    Acc@1  53.12 ( 56.88)   Acc@5  73.44 ( 79.78)
Epoch: [22][201/625]    Time  0.696 ( 0.720)    Data  0.231 ( 0.255)    Loss 1.6787e+00 (1.7664e+00)    Acc@1  60.16 ( 56.76)   Acc@5  81.25 ( 79.77)
Epoch: [22][301/625]    Time  0.697 ( 0.719)    Data  0.230 ( 0.254)    Loss 1.4969e+00 (1.7874e+00)    Acc@1  65.62 ( 56.24)   Acc@5  83.59 ( 79.32)
Epoch: [22][401/625]    Time  0.697 ( 0.715)    Data  0.231 ( 0.250)    Loss 1.6452e+00 (1.8101e+00)    Acc@1  57.03 ( 55.58)   Acc@5  84.38 ( 79.01)
Epoch: [22][501/625]    Time  0.713 ( 0.714)    Data  0.246 ( 0.249)    Loss 1.9867e+00 (1.8248e+00)    Acc@1  50.78 ( 55.27)   Acc@5  75.00 ( 78.71)
Epoch: [22][601/625]    Time  0.698 ( 0.715)    Data  0.233 ( 0.250)    Loss 1.6476e+00 (1.8316e+00)    Acc@1  60.16 ( 55.18)   Acc@5  82.81 ( 78.54)
train Loss: 1.8321 Acc: 0.5515
Epoch: [22][  1/625]    Time  0.503 ( 0.714)    Data  0.355 ( 0.250)    Loss 2.5142e+00 (1.8332e+00)    Acc@1  43.75 ( 55.13)   Acc@5  67.19 ( 78.50)
Epoch: [22][101/625]    Time  0.378 ( 0.669)    Data  0.229 ( 0.248)    Loss 2.8059e+00 (1.9540e+00)    Acc@1  35.16 ( 53.04)   Acc@5  59.38 ( 76.73)
val Loss: 2.6921 Acc: 0.4034

Epoch 23/39
----------
Epoch: [23][  1/625]    Time  0.984 ( 0.984)    Data  0.522 ( 0.522)    Loss 1.9792e+00 (1.9792e+00)    Acc@1  46.09 ( 46.09)   Acc@5  75.00 ( 75.00)
Epoch: [23][101/625]    Time  0.689 ( 0.722)    Data  0.230 ( 0.257)    Loss 1.6204e+00 (1.7310e+00)    Acc@1  60.16 ( 56.99)   Acc@5  80.47 ( 80.58)
Epoch: [23][201/625]    Time  0.701 ( 0.712)    Data  0.235 ( 0.247)    Loss 1.8150e+00 (1.7385e+00)    Acc@1  57.81 ( 57.18)   Acc@5  75.00 ( 80.31)
Epoch: [23][301/625]    Time  0.693 ( 0.711)    Data  0.230 ( 0.247)    Loss 1.8685e+00 (1.7520e+00)    Acc@1  55.47 ( 56.82)   Acc@5  78.12 ( 80.08)
Epoch: [23][401/625]    Time  0.692 ( 0.712)    Data  0.233 ( 0.248)    Loss 1.7397e+00 (1.7729e+00)    Acc@1  56.25 ( 56.41)   Acc@5  82.03 ( 79.75)
Epoch: [23][501/625]    Time  0.695 ( 0.710)    Data  0.229 ( 0.246)    Loss 2.0199e+00 (1.7940e+00)    Acc@1  55.47 ( 56.03)   Acc@5  69.53 ( 79.37)
Epoch: [23][601/625]    Time  0.699 ( 0.711)    Data  0.232 ( 0.247)    Loss 2.0546e+00 (1.8080e+00)    Acc@1  48.44 ( 55.72)   Acc@5  74.22 ( 79.11)
train Loss: 1.8108 Acc: 0.5569
Epoch: [23][  1/625]    Time  0.516 ( 0.711)    Data  0.366 ( 0.247)    Loss 2.4958e+00 (1.8118e+00)    Acc@1  38.28 ( 55.66)   Acc@5  66.41 ( 79.04)
Epoch: [23][101/625]    Time  0.380 ( 0.666)    Data  0.234 ( 0.246)    Loss 2.8938e+00 (1.9194e+00)    Acc@1  40.62 ( 53.63)   Acc@5  62.50 ( 77.28)
val Loss: 2.5681 Acc: 0.4127

Epoch 24/39
----------
Epoch: [24][  1/625]    Time  0.879 ( 0.879)    Data  0.422 ( 0.422)    Loss 1.8535e+00 (1.8535e+00)    Acc@1  50.00 ( 50.00)   Acc@5  75.00 ( 75.00)
Epoch: [24][101/625]    Time  0.691 ( 0.716)    Data  0.227 ( 0.252)    Loss 2.0585e+00 (1.6983e+00)    Acc@1  50.78 ( 58.25)   Acc@5  76.56 ( 80.71)
Epoch: [24][201/625]    Time  0.698 ( 0.716)    Data  0.230 ( 0.251)    Loss 1.6109e+00 (1.7268e+00)    Acc@1  62.50 ( 57.45)   Acc@5  78.91 ( 80.24)
Epoch: [24][301/625]    Time  0.712 ( 0.713)    Data  0.245 ( 0.249)    Loss 1.9499e+00 (1.7532e+00)    Acc@1  53.91 ( 56.61)   Acc@5  79.69 ( 79.84)
Epoch: [24][401/625]    Time  0.695 ( 0.713)    Data  0.230 ( 0.249)    Loss 1.8195e+00 (1.7710e+00)    Acc@1  58.59 ( 56.18)   Acc@5  78.12 ( 79.53)
Epoch: [24][501/625]    Time  0.700 ( 0.711)    Data  0.234 ( 0.246)    Loss 1.8797e+00 (1.7882e+00)    Acc@1  54.69 ( 55.92)   Acc@5  78.91 ( 79.29)
Epoch: [24][601/625]    Time  0.695 ( 0.709)    Data  0.231 ( 0.245)    Loss 2.0770e+00 (1.7979e+00)    Acc@1  53.12 ( 55.71)   Acc@5  77.34 ( 79.13)
train Loss: 1.7994 Acc: 0.5571
Epoch: [24][  1/625]    Time  0.478 ( 0.709)    Data  0.329 ( 0.245)    Loss 2.6613e+00 (1.8007e+00)    Acc@1  46.09 ( 55.70)   Acc@5  66.41 ( 79.09)
Epoch: [24][101/625]    Time  0.381 ( 0.666)    Data  0.229 ( 0.246)    Loss 2.8880e+00 (1.9400e+00)    Acc@1  37.50 ( 53.31)   Acc@5  66.41 ( 77.02)
val Loss: 2.7926 Acc: 0.3886

Epoch 25/39
----------
Epoch: [25][  1/625]    Time  1.001 ( 1.001)    Data  0.531 ( 0.531)    Loss 1.6988e+00 (1.6988e+00)    Acc@1  53.12 ( 53.12)   Acc@5  83.59 ( 83.59)
Epoch: [25][101/625]    Time  0.695 ( 0.707)    Data  0.229 ( 0.243)    Loss 1.9363e+00 (1.7233e+00)    Acc@1  50.00 ( 57.29)   Acc@5  78.91 ( 80.28)
Epoch: [25][201/625]    Time  0.690 ( 0.712)    Data  0.225 ( 0.248)    Loss 1.8622e+00 (1.7306e+00)    Acc@1  57.03 ( 57.18)   Acc@5  76.56 ( 80.34)
Epoch: [25][301/625]    Time  0.687 ( 0.706)    Data  0.223 ( 0.242)    Loss 1.7371e+00 (1.7496e+00)    Acc@1  54.69 ( 56.89)   Acc@5  83.59 ( 80.06)
Epoch: [25][401/625]    Time  0.678 ( 0.704)    Data  0.221 ( 0.240)    Loss 2.0364e+00 (1.7599e+00)    Acc@1  51.56 ( 56.59)   Acc@5  74.22 ( 79.90)
Epoch: [25][501/625]    Time  0.677 ( 0.703)    Data  0.221 ( 0.239)    Loss 1.8611e+00 (1.7657e+00)    Acc@1  53.91 ( 56.47)   Acc@5  77.34 ( 79.72)
Epoch: [25][601/625]    Time  0.691 ( 0.702)    Data  0.224 ( 0.238)    Loss 2.0724e+00 (1.7814e+00)    Acc@1  50.78 ( 56.16)   Acc@5  71.09 ( 79.46)
train Loss: 1.7879 Acc: 0.5600
Epoch: [25][  1/625]    Time  0.451 ( 0.702)    Data  0.303 ( 0.238)    Loss 2.7279e+00 (1.7894e+00)    Acc@1  33.59 ( 55.96)   Acc@5  62.50 ( 79.33)
Epoch: [25][101/625]    Time  0.372 ( 0.656)    Data  0.220 ( 0.236)    Loss 2.6630e+00 (1.9099e+00)    Acc@1  39.84 ( 53.90)   Acc@5  64.84 ( 77.51)
val Loss: 2.6650 Acc: 0.4108

Epoch 26/39
----------
Epoch: [26][  1/625]    Time  0.816 ( 0.816)    Data  0.351 ( 0.351)    Loss 1.5148e+00 (1.5148e+00)    Acc@1  62.50 ( 62.50)   Acc@5  80.47 ( 80.47)
Epoch: [26][101/625]    Time  0.680 ( 0.703)    Data  0.222 ( 0.239)    Loss 1.5629e+00 (1.6847e+00)    Acc@1  61.72 ( 58.35)   Acc@5  81.25 ( 80.82)
Epoch: [26][201/625]    Time  0.686 ( 0.698)    Data  0.223 ( 0.234)    Loss 1.7178e+00 (1.6991e+00)    Acc@1  57.03 ( 57.88)   Acc@5  78.91 ( 80.59)
Epoch: [26][301/625]    Time  0.681 ( 0.701)    Data  0.221 ( 0.237)    Loss 1.9848e+00 (1.7173e+00)    Acc@1  49.22 ( 57.45)   Acc@5  78.91 ( 80.32)
Epoch: [26][401/625]    Time  0.702 ( 0.700)    Data  0.234 ( 0.236)    Loss 1.4880e+00 (1.7312e+00)    Acc@1  62.50 ( 57.27)   Acc@5  85.16 ( 80.10)
Epoch: [26][501/625]    Time  1.029 ( 0.699)    Data  0.562 ( 0.235)    Loss 1.9686e+00 (1.7361e+00)    Acc@1  53.91 ( 57.21)   Acc@5  76.56 ( 79.98)
Epoch: [26][601/625]    Time  0.889 ( 0.700)    Data  0.425 ( 0.236)    Loss 1.6375e+00 (1.7525e+00)    Acc@1  60.16 ( 56.73)   Acc@5  80.47 ( 79.78)
train Loss: 1.7556 Acc: 0.5666
Epoch: [26][  1/625]    Time  0.456 ( 0.699)    Data  0.305 ( 0.235)    Loss 2.8234e+00 (1.7573e+00)    Acc@1  35.94 ( 56.63)   Acc@5  64.06 ( 79.70)
Epoch: [26][101/625]    Time  0.376 ( 0.654)    Data  0.223 ( 0.234)    Loss 2.5466e+00 (1.8771e+00)    Acc@1  41.41 ( 54.43)   Acc@5  66.41 ( 77.95)
val Loss: 2.6427 Acc: 0.4052

Epoch 27/39
----------
Epoch: [27][  1/625]    Time  0.973 ( 0.973)    Data  0.504 ( 0.504)    Loss 1.6840e+00 (1.6840e+00)    Acc@1  57.81 ( 57.81)   Acc@5  81.25 ( 81.25)
Epoch: [27][101/625]    Time  0.678 ( 0.711)    Data  0.220 ( 0.247)    Loss 2.1957e+00 (1.6593e+00)    Acc@1  40.62 ( 58.74)   Acc@5  76.56 ( 81.42)
Epoch: [27][201/625]    Time  0.687 ( 0.702)    Data  0.221 ( 0.238)    Loss 1.8232e+00 (1.6871e+00)    Acc@1  50.78 ( 58.13)   Acc@5  79.69 ( 80.84)
Epoch: [27][301/625]    Time  0.690 ( 0.702)    Data  0.223 ( 0.238)    Loss 1.4947e+00 (1.7062e+00)    Acc@1  60.94 ( 57.71)   Acc@5  87.50 ( 80.53)
Epoch: [27][401/625]    Time  0.683 ( 0.703)    Data  0.223 ( 0.239)    Loss 1.4632e+00 (1.7155e+00)    Acc@1  61.72 ( 57.54)   Acc@5  85.16 ( 80.38)
Epoch: [27][501/625]    Time  0.688 ( 0.703)    Data  0.224 ( 0.239)    Loss 1.7397e+00 (1.7310e+00)    Acc@1  53.91 ( 57.12)   Acc@5  78.12 ( 79.99)
Epoch: [27][601/625]    Time  0.686 ( 0.704)    Data  0.222 ( 0.241)    Loss 2.1703e+00 (1.7445e+00)    Acc@1  41.41 ( 56.82)   Acc@5  74.22 ( 79.80)
train Loss: 1.7472 Acc: 0.5674
Epoch: [27][  1/625]    Time  0.455 ( 0.704)    Data  0.307 ( 0.240)    Loss 3.0447e+00 (1.7492e+00)    Acc@1  36.72 ( 56.70)   Acc@5  61.72 ( 79.74)
Epoch: [27][101/625]    Time  0.372 ( 0.660)    Data  0.224 ( 0.240)    Loss 3.0081e+00 (1.8874e+00)    Acc@1  36.72 ( 54.42)   Acc@5  57.03 ( 77.74)
val Loss: 2.7517 Acc: 0.4017

Epoch 28/39
----------
Epoch: [28][  1/625]    Time  0.808 ( 0.808)    Data  0.349 ( 0.349)    Loss 1.4628e+00 (1.4628e+00)    Acc@1  63.28 ( 63.28)   Acc@5  84.38 ( 84.38)
Epoch: [28][101/625]    Time  0.688 ( 0.751)    Data  0.223 ( 0.285)    Loss 1.4554e+00 (1.6647e+00)    Acc@1  64.84 ( 58.75)   Acc@5  86.72 ( 81.18)
Epoch: [28][201/625]    Time  0.686 ( 0.732)    Data  0.228 ( 0.267)    Loss 1.8111e+00 (1.6687e+00)    Acc@1  54.69 ( 58.64)   Acc@5  80.47 ( 81.19)
Epoch: [28][301/625]    Time  0.694 ( 0.722)    Data  0.229 ( 0.257)    Loss 1.9354e+00 (1.7014e+00)    Acc@1  54.69 ( 57.96)   Acc@5  75.78 ( 80.69)
Epoch: [28][401/625]    Time  0.694 ( 0.720)    Data  0.229 ( 0.256)    Loss 1.7236e+00 (1.7143e+00)    Acc@1  55.47 ( 57.56)   Acc@5  82.81 ( 80.45)
Epoch: [28][501/625]    Time  0.691 ( 0.717)    Data  0.225 ( 0.253)    Loss 1.5050e+00 (1.7204e+00)    Acc@1  63.28 ( 57.39)   Acc@5  82.81 ( 80.35)
Epoch: [28][601/625]    Time  0.684 ( 0.714)    Data  0.225 ( 0.250)    Loss 1.9168e+00 (1.7342e+00)    Acc@1  52.34 ( 57.09)   Acc@5  77.34 ( 80.11)
train Loss: 1.7364 Acc: 0.5705
Epoch: [28][  1/625]    Time  0.618 ( 0.715)    Data  0.469 ( 0.252)    Loss 2.7250e+00 (1.7380e+00)    Acc@1  33.59 ( 57.02)   Acc@5  65.62 ( 80.06)
Epoch: [28][101/625]    Time  0.375 ( 0.670)    Data  0.227 ( 0.249)    Loss 2.6591e+00 (1.8737e+00)    Acc@1  41.41 ( 54.64)   Acc@5  66.41 ( 77.95)
val Loss: 2.7186 Acc: 0.3976

Epoch 29/39
----------
Epoch: [29][  1/625]    Time  0.829 ( 0.829)    Data  0.364 ( 0.364)    Loss 1.3415e+00 (1.3415e+00)    Acc@1  67.97 ( 67.97)   Acc@5  87.50 ( 87.50)
Epoch: [29][101/625]    Time  0.695 ( 0.711)    Data  0.227 ( 0.246)    Loss 1.6526e+00 (1.6393e+00)    Acc@1  58.59 ( 59.41)   Acc@5  81.25 ( 81.54)
Epoch: [29][201/625]    Time  0.691 ( 0.710)    Data  0.227 ( 0.245)    Loss 1.4179e+00 (1.6489e+00)    Acc@1  61.72 ( 58.81)   Acc@5  85.94 ( 81.53)
Epoch: [29][301/625]    Time  1.378 ( 0.751)    Data  0.917 ( 0.285)    Loss 1.6693e+00 (1.6658e+00)    Acc@1  57.81 ( 58.47)   Acc@5  78.91 ( 81.34)
Epoch: [29][401/625]    Time  1.387 ( 0.916)    Data  0.913 ( 0.449)    Loss 1.6441e+00 (1.6931e+00)    Acc@1  60.16 ( 57.98)   Acc@5  78.91 ( 80.86)
Epoch: [29][501/625]    Time  1.395 ( 1.015)    Data  0.936 ( 0.548)    Loss 1.6780e+00 (1.7068e+00)    Acc@1  60.16 ( 57.77)   Acc@5  82.03 ( 80.65)
Epoch: [29][601/625]    Time  1.371 ( 1.078)    Data  0.912 ( 0.612)    Loss 1.4746e+00 (1.7153e+00)    Acc@1  63.28 ( 57.54)   Acc@5  84.38 ( 80.55)
train Loss: 1.7203 Acc: 0.5744
Epoch: [29][  1/625]    Time  1.136 ( 1.093)    Data  0.989 ( 0.627)    Loss 2.5097e+00 (1.7215e+00)    Acc@1  42.97 ( 57.42)   Acc@5  70.31 ( 80.45)
Epoch: [29][101/625]    Time  1.483 ( 1.091)    Data  1.336 ( 0.668)    Loss 2.4818e+00 (1.8424e+00)    Acc@1  42.19 ( 55.21)   Acc@5  70.31 ( 78.61)
val Loss: 2.6098 Acc: 0.4108

Epoch 30/39
----------
Epoch: [30][  1/625]    Time  1.399 ( 1.399)    Data  0.935 ( 0.935)    Loss 1.6438e+00 (1.6438e+00)    Acc@1  60.94 ( 60.94)   Acc@5  82.81 ( 82.81)
Epoch: [30][101/625]    Time  0.990 ( 1.032)    Data  0.534 ( 0.572)    Loss 1.2853e+00 (1.4319e+00)    Acc@1  66.41 ( 64.76)   Acc@5  85.16 ( 84.70)
Epoch: [30][201/625]    Time  1.009 ( 1.035)    Data  0.551 ( 0.574)    Loss 1.2030e+00 (1.3697e+00)    Acc@1  70.31 ( 66.46)   Acc@5  87.50 ( 85.40)
Epoch: [30][301/625]    Time  0.997 ( 1.032)    Data  0.538 ( 0.572)    Loss 1.1772e+00 (1.3438e+00)    Acc@1  71.09 ( 67.22)   Acc@5  88.28 ( 85.67)
Epoch: [30][401/625]    Time  1.030 ( 1.032)    Data  0.570 ( 0.572)    Loss 1.2878e+00 (1.3211e+00)    Acc@1  67.97 ( 67.78)   Acc@5  83.59 ( 85.98)
Epoch: [30][501/625]    Time  1.062 ( 1.031)    Data  0.603 ( 0.571)    Loss 1.0339e+00 (1.3044e+00)    Acc@1  74.22 ( 68.14)   Acc@5  89.84 ( 86.19)
Epoch: [30][601/625]    Time  0.995 ( 1.032)    Data  0.530 ( 0.571)    Loss 1.0169e+00 (1.2922e+00)    Acc@1  75.78 ( 68.46)   Acc@5  91.41 ( 86.31)
train Loss: 1.2883 Acc: 0.6854
Epoch: [30][  1/625]    Time  0.452 ( 1.030)    Data  0.305 ( 0.570)    Loss 2.0070e+00 (1.2894e+00)    Acc@1  51.56 ( 68.51)   Acc@5  73.44 ( 86.32)
Epoch: [30][101/625]    Time  0.372 ( 0.941)    Data  0.226 ( 0.524)    Loss 2.1331e+00 (1.4035e+00)    Acc@1  50.78 ( 66.02)   Acc@5  76.56 ( 84.67)
val Loss: 2.1195 Acc: 0.5055

Epoch 31/39
----------
Epoch: [31][  1/625]    Time  0.893 ( 0.893)    Data  0.427 ( 0.427)    Loss 1.0913e+00 (1.0913e+00)    Acc@1  72.66 ( 72.66)   Acc@5  90.62 ( 90.62)
Epoch: [31][101/625]    Time  0.684 ( 0.724)    Data  0.224 ( 0.260)    Loss 1.2046e+00 (1.1582e+00)    Acc@1  67.19 ( 71.25)   Acc@5  87.50 ( 87.95)
Epoch: [31][201/625]    Time  0.687 ( 0.709)    Data  0.223 ( 0.246)    Loss 1.2030e+00 (1.1577e+00)    Acc@1  73.44 ( 71.50)   Acc@5  85.94 ( 87.73)
Epoch: [31][301/625]    Time  0.694 ( 0.710)    Data  0.227 ( 0.247)    Loss 1.1246e+00 (1.1532e+00)    Acc@1  71.09 ( 71.64)   Acc@5  90.62 ( 87.82)
Epoch: [31][401/625]    Time  0.689 ( 0.706)    Data  0.222 ( 0.243)    Loss 1.0325e+00 (1.1586e+00)    Acc@1  73.44 ( 71.66)   Acc@5  88.28 ( 87.71)
Epoch: [31][501/625]    Time  0.689 ( 0.704)    Data  0.223 ( 0.240)    Loss 1.2914e+00 (1.1536e+00)    Acc@1  70.31 ( 71.75)   Acc@5  81.25 ( 87.80)
Epoch: [31][601/625]    Time  0.686 ( 0.704)    Data  0.223 ( 0.241)    Loss 1.2416e+00 (1.1523e+00)    Acc@1  67.19 ( 71.75)   Acc@5  89.06 ( 87.85)
train Loss: 1.1514 Acc: 0.7177
Epoch: [31][  1/625]    Time  0.455 ( 0.703)    Data  0.307 ( 0.240)    Loss 1.9160e+00 (1.1526e+00)    Acc@1  54.69 ( 71.75)   Acc@5  75.00 ( 87.82)
Epoch: [31][101/625]    Time  0.412 ( 0.659)    Data  0.261 ( 0.239)    Loss 2.3041e+00 (1.2861e+00)    Acc@1  48.44 ( 68.88)   Acc@5  75.00 ( 86.02)
val Loss: 2.0885 Acc: 0.5161

Epoch 32/39
----------
Epoch: [32][  1/625]    Time  1.383 ( 1.383)    Data  0.556 ( 0.556)    Loss 1.3819e+00 (1.3819e+00)    Acc@1  62.50 ( 62.50)   Acc@5  86.72 ( 86.72)
Epoch: [32][101/625]    Time  0.688 ( 0.728)    Data  0.221 ( 0.259)    Loss 1.0776e+00 (1.0952e+00)    Acc@1  73.44 ( 73.38)   Acc@5  89.06 ( 88.66)
Epoch: [32][201/625]    Time  0.691 ( 0.712)    Data  0.226 ( 0.245)    Loss 1.1564e+00 (1.0986e+00)    Acc@1  72.66 ( 73.33)   Acc@5  84.38 ( 88.48)
Epoch: [32][301/625]    Time  0.690 ( 0.706)    Data  0.223 ( 0.240)    Loss 9.3546e-01 (1.0950e+00)    Acc@1  75.00 ( 73.31)   Acc@5  91.41 ( 88.57)
Epoch: [32][401/625]    Time  0.685 ( 0.708)    Data  0.226 ( 0.242)    Loss 1.1612e+00 (1.0938e+00)    Acc@1  71.88 ( 73.25)   Acc@5  85.94 ( 88.54)
Epoch: [32][501/625]    Time  0.689 ( 0.704)    Data  0.223 ( 0.239)    Loss 1.1179e+00 (1.0866e+00)    Acc@1  72.66 ( 73.44)   Acc@5  89.84 ( 88.71)
Epoch: [32][601/625]    Time  0.680 ( 0.703)    Data  0.223 ( 0.238)    Loss 1.2430e+00 (1.0863e+00)    Acc@1  67.19 ( 73.39)   Acc@5  86.72 ( 88.72)
train Loss: 1.0859 Acc: 0.7339
Epoch: [32][  1/625]    Time  0.461 ( 0.705)    Data  0.313 ( 0.241)    Loss 2.0533e+00 (1.0874e+00)    Acc@1  54.69 ( 73.36)   Acc@5  80.47 ( 88.72)
Epoch: [32][101/625]    Time  0.374 ( 0.659)    Data  0.223 ( 0.239)    Loss 1.9172e+00 (1.2257e+00)    Acc@1  53.12 ( 70.29)   Acc@5  78.12 ( 86.82)
val Loss: 2.0979 Acc: 0.5145

Epoch 33/39
----------
Epoch: [33][  1/625]    Time  0.819 ( 0.819)    Data  0.354 ( 0.354)    Loss 1.0663e+00 (1.0663e+00)    Acc@1  75.78 ( 75.78)   Acc@5  89.84 ( 89.84)
Epoch: [33][101/625]    Time  0.688 ( 0.701)    Data  0.222 ( 0.237)    Loss 1.2085e+00 (1.0509e+00)    Acc@1  74.22 ( 74.47)   Acc@5  84.38 ( 88.95)
Epoch: [33][201/625]    Time  0.693 ( 0.704)    Data  0.228 ( 0.240)    Loss 1.0809e+00 (1.0412e+00)    Acc@1  71.09 ( 74.51)   Acc@5  89.84 ( 89.10)
Epoch: [33][301/625]    Time  0.689 ( 0.700)    Data  0.222 ( 0.236)    Loss 1.2430e+00 (1.0512e+00)    Acc@1  69.53 ( 74.21)   Acc@5  82.81 ( 88.95)
Epoch: [33][401/625]    Time  0.688 ( 0.698)    Data  0.222 ( 0.234)    Loss 1.0347e+00 (1.0495e+00)    Acc@1  71.88 ( 74.15)   Acc@5  89.84 ( 89.03)
Epoch: [33][501/625]    Time  0.687 ( 0.700)    Data  0.222 ( 0.236)    Loss 8.2642e-01 (1.0482e+00)    Acc@1  77.34 ( 74.27)   Acc@5  94.53 ( 89.04)
Epoch: [33][601/625]    Time  0.688 ( 0.699)    Data  0.223 ( 0.235)    Loss 8.0268e-01 (1.0470e+00)    Acc@1  82.81 ( 74.30)   Acc@5  92.19 ( 89.06)
train Loss: 1.0490 Acc: 0.7424
Epoch: [33][  1/625]    Time  0.452 ( 0.698)    Data  0.303 ( 0.235)    Loss 2.1241e+00 (1.0507e+00)    Acc@1  50.00 ( 74.20)   Acc@5  72.66 ( 89.02)
Epoch: [33][101/625]    Time  0.369 ( 0.656)    Data  0.222 ( 0.236)    Loss 2.2673e+00 (1.1977e+00)    Acc@1  48.44 ( 71.10)   Acc@5  71.88 ( 87.06)
val Loss: 2.1063 Acc: 0.5197

Epoch 34/39
----------
Epoch: [34][  1/625]    Time  1.030 ( 1.030)    Data  0.563 ( 0.563)    Loss 1.1384e+00 (1.1384e+00)    Acc@1  71.09 ( 71.09)   Acc@5  85.16 ( 85.16)
Epoch: [34][101/625]    Time  0.692 ( 0.709)    Data  0.226 ( 0.240)    Loss 8.3815e-01 (1.0119e+00)    Acc@1  79.69 ( 75.13)   Acc@5  91.41 ( 89.53)
Epoch: [34][201/625]    Time  0.694 ( 0.701)    Data  0.227 ( 0.235)    Loss 9.4130e-01 (1.0010e+00)    Acc@1  76.56 ( 75.45)   Acc@5  89.84 ( 89.70)
Epoch: [34][301/625]    Time  0.690 ( 0.703)    Data  0.224 ( 0.237)    Loss 1.0803e+00 (1.0042e+00)    Acc@1  71.88 ( 75.41)   Acc@5  89.06 ( 89.63)
Epoch: [34][401/625]    Time  0.685 ( 0.702)    Data  0.226 ( 0.236)    Loss 1.0945e+00 (9.9982e-01)    Acc@1  75.00 ( 75.49)   Acc@5  87.50 ( 89.66)
Epoch: [34][501/625]    Time  0.690 ( 0.704)    Data  0.224 ( 0.239)    Loss 8.8032e-01 (1.0042e+00)    Acc@1  75.78 ( 75.37)   Acc@5  91.41 ( 89.56)
Epoch: [34][601/625]    Time  0.691 ( 0.703)    Data  0.224 ( 0.238)    Loss 1.0485e+00 (1.0092e+00)    Acc@1  73.44 ( 75.27)   Acc@5  89.06 ( 89.50)
train Loss: 1.0103 Acc: 0.7525
Epoch: [34][  1/625]    Time  0.465 ( 0.702)    Data  0.316 ( 0.238)    Loss 2.0206e+00 (1.0120e+00)    Acc@1  54.69 ( 75.21)   Acc@5  74.22 ( 89.45)
Epoch: [34][101/625]    Time  0.371 ( 0.658)    Data  0.223 ( 0.237)    Loss 2.1004e+00 (1.1713e+00)    Acc@1  55.47 ( 71.90)   Acc@5  76.56 ( 87.41)
val Loss: 2.1625 Acc: 0.5131

Epoch 35/39
----------
Epoch: [35][  1/625]    Time  0.832 ( 0.832)    Data  0.365 ( 0.365)    Loss 9.5770e-01 (9.5770e-01)    Acc@1  76.56 ( 76.56)   Acc@5  89.84 ( 89.84)
Epoch: [35][101/625]    Time  0.690 ( 0.717)    Data  0.226 ( 0.252)    Loss 9.2441e-01 (9.8904e-01)    Acc@1  79.69 ( 75.73)   Acc@5  89.06 ( 89.71)
Epoch: [35][201/625]    Time  0.692 ( 0.708)    Data  0.225 ( 0.244)    Loss 1.2950e+00 (1.0050e+00)    Acc@1  67.97 ( 75.36)   Acc@5  85.94 ( 89.37)
Epoch: [35][301/625]    Time  0.694 ( 0.710)    Data  0.225 ( 0.246)    Loss 1.0394e+00 (1.0006e+00)    Acc@1  75.00 ( 75.56)   Acc@5  90.62 ( 89.46)
Epoch: [35][401/625]    Time  0.758 ( 0.708)    Data  0.298 ( 0.244)    Loss 1.1147e+00 (9.9270e-01)    Acc@1  74.22 ( 75.67)   Acc@5  88.28 ( 89.65)
Epoch: [35][501/625]    Time  0.690 ( 0.706)    Data  0.225 ( 0.242)    Loss 9.9913e-01 (9.9251e-01)    Acc@1  75.00 ( 75.65)   Acc@5  88.28 ( 89.64)
Epoch: [35][601/625]    Time  0.683 ( 0.706)    Data  0.225 ( 0.242)    Loss 9.0101e-01 (9.8832e-01)    Acc@1  78.91 ( 75.71)   Acc@5  89.84 ( 89.75)
train Loss: 0.9878 Acc: 0.7571
Epoch: [35][  1/625]    Time  0.453 ( 0.706)    Data  0.305 ( 0.242)    Loss 1.9679e+00 (9.8940e-01)    Acc@1  48.44 ( 75.67)   Acc@5  75.78 ( 89.74)
Epoch: [35][101/625]    Time  0.374 ( 0.661)    Data  0.227 ( 0.241)    Loss 2.1538e+00 (1.1484e+00)    Acc@1  49.22 ( 72.36)   Acc@5  75.00 ( 87.65)
val Loss: 2.1352 Acc: 0.5185

Epoch 36/39
----------
Epoch: [36][  1/625]    Time  0.975 ( 0.975)    Data  0.509 ( 0.509)    Loss 9.5964e-01 (9.5964e-01)    Acc@1  75.00 ( 75.00)   Acc@5  90.62 ( 90.62)
Epoch: [36][101/625]    Time  0.691 ( 0.723)    Data  0.225 ( 0.259)    Loss 9.3162e-01 (9.4043e-01)    Acc@1  78.12 ( 77.15)   Acc@5  89.06 ( 90.36)
Epoch: [36][201/625]    Time  0.691 ( 0.709)    Data  0.224 ( 0.245)    Loss 1.0689e+00 (9.4597e-01)    Acc@1  78.12 ( 77.01)   Acc@5  86.72 ( 90.24)
Epoch: [36][301/625]    Time  0.698 ( 0.705)    Data  0.232 ( 0.241)    Loss 8.6965e-01 (9.5216e-01)    Acc@1  78.12 ( 76.64)   Acc@5  93.75 ( 90.10)
Epoch: [36][401/625]    Time  0.687 ( 0.707)    Data  0.223 ( 0.243)    Loss 9.4794e-01 (9.5534e-01)    Acc@1  75.00 ( 76.52)   Acc@5  89.06 ( 89.99)
Epoch: [36][501/625]    Time  0.854 ( 0.705)    Data  0.389 ( 0.241)    Loss 7.7758e-01 (9.5385e-01)    Acc@1  82.81 ( 76.59)   Acc@5  92.19 ( 90.09)
Epoch: [36][601/625]    Time  0.691 ( 0.703)    Data  0.225 ( 0.239)    Loss 9.7905e-01 (9.5543e-01)    Acc@1  75.00 ( 76.52)   Acc@5  91.41 ( 90.07)
train Loss: 0.9559 Acc: 0.7649
Epoch: [36][  1/625]    Time  0.468 ( 0.705)    Data  0.318 ( 0.242)    Loss 2.5222e+00 (9.5839e-01)    Acc@1  46.09 ( 76.44)   Acc@5  71.88 ( 90.04)
Epoch: [36][101/625]    Time  0.374 ( 0.660)    Data  0.226 ( 0.240)    Loss 2.1676e+00 (1.1197e+00)    Acc@1  52.34 ( 73.08)   Acc@5  71.88 ( 88.02)
val Loss: 2.1372 Acc: 0.5171

Epoch 37/39
----------
Epoch: [37][  1/625]    Time  0.833 ( 0.833)    Data  0.370 ( 0.370)    Loss 1.0783e+00 (1.0783e+00)    Acc@1  75.78 ( 75.78)   Acc@5  89.84 ( 89.84)
Epoch: [37][101/625]    Time  0.687 ( 0.700)    Data  0.221 ( 0.236)    Loss 1.0383e+00 (9.2893e-01)    Acc@1  75.00 ( 77.34)   Acc@5  89.06 ( 90.29)
Epoch: [37][201/625]    Time  0.690 ( 0.703)    Data  0.222 ( 0.239)    Loss 9.3864e-01 (9.3795e-01)    Acc@1  76.56 ( 76.94)   Acc@5  91.41 ( 90.15)
Epoch: [37][301/625]    Time  0.694 ( 0.701)    Data  0.228 ( 0.237)    Loss 8.8860e-01 (9.3971e-01)    Acc@1  74.22 ( 76.79)   Acc@5  94.53 ( 90.25)
Epoch: [37][401/625]    Time  0.695 ( 0.699)    Data  0.229 ( 0.236)    Loss 9.8786e-01 (9.4178e-01)    Acc@1  77.34 ( 76.60)   Acc@5  88.28 ( 90.25)
Epoch: [37][501/625]    Time  0.688 ( 0.702)    Data  0.222 ( 0.238)    Loss 1.1752e+00 (9.3976e-01)    Acc@1  73.44 ( 76.70)   Acc@5  85.16 ( 90.31)
Epoch: [37][601/625]    Time  0.692 ( 0.702)    Data  0.225 ( 0.238)    Loss 9.6084e-01 (9.4068e-01)    Acc@1  76.56 ( 76.69)   Acc@5  88.28 ( 90.27)
train Loss: 0.9411 Acc: 0.7671
Epoch: [37][  1/625]    Time  0.456 ( 0.701)    Data  0.306 ( 0.238)    Loss 2.3116e+00 (9.4332e-01)    Acc@1  51.56 ( 76.67)   Acc@5  74.22 ( 90.23)
Epoch: [37][101/625]    Time  0.374 ( 0.659)    Data  0.223 ( 0.239)    Loss 2.0498e+00 (1.1114e+00)    Acc@1  54.69 ( 73.19)   Acc@5  75.00 ( 88.08)
val Loss: 2.1594 Acc: 0.5156

Epoch 38/39
----------
Epoch: [38][  1/625]    Time  0.967 ( 0.967)    Data  0.508 ( 0.508)    Loss 8.1652e-01 (8.1652e-01)    Acc@1  80.47 ( 80.47)   Acc@5  91.41 ( 91.41)
Epoch: [38][101/625]    Time  0.694 ( 0.705)    Data  0.232 ( 0.241)    Loss 8.7785e-01 (9.0888e-01)    Acc@1  78.91 ( 78.17)   Acc@5  91.41 ( 90.56)
Epoch: [38][201/625]    Time  0.691 ( 0.700)    Data  0.226 ( 0.236)    Loss 8.5396e-01 (9.2402e-01)    Acc@1  78.91 ( 77.54)   Acc@5  90.62 ( 90.28)
Epoch: [38][301/625]    Time  0.693 ( 0.702)    Data  0.228 ( 0.238)    Loss 7.7850e-01 (9.1773e-01)    Acc@1  79.69 ( 77.49)   Acc@5  93.75 ( 90.44)
Epoch: [38][401/625]    Time  0.688 ( 0.702)    Data  0.223 ( 0.238)    Loss 7.4552e-01 (9.1373e-01)    Acc@1  80.47 ( 77.61)   Acc@5  93.75 ( 90.55)
Epoch: [38][501/625]    Time  0.691 ( 0.703)    Data  0.224 ( 0.239)    Loss 8.9272e-01 (9.1893e-01)    Acc@1  78.12 ( 77.53)   Acc@5  87.50 ( 90.45)
Epoch: [38][601/625]    Time  0.692 ( 0.702)    Data  0.225 ( 0.238)    Loss 7.6948e-01 (9.2115e-01)    Acc@1  82.03 ( 77.56)   Acc@5  92.97 ( 90.41)
train Loss: 0.9212 Acc: 0.7756
Epoch: [38][  1/625]    Time  0.457 ( 0.701)    Data  0.307 ( 0.237)    Loss 2.2481e+00 (9.2330e-01)    Acc@1  50.00 ( 77.52)   Acc@5  71.09 ( 90.38)
Epoch: [38][101/625]    Time  0.376 ( 0.657)    Data  0.225 ( 0.237)    Loss 2.0794e+00 (1.0931e+00)    Acc@1  53.91 ( 73.93)   Acc@5  75.78 ( 88.19)
val Loss: 2.1481 Acc: 0.5169

Epoch 39/39
----------
Epoch: [39][  1/625]    Time  0.837 ( 0.837)    Data  0.372 ( 0.372)    Loss 7.5979e-01 (7.5979e-01)    Acc@1  83.59 ( 83.59)   Acc@5  92.97 ( 92.97)
Epoch: [39][101/625]    Time  0.687 ( 0.719)    Data  0.221 ( 0.254)    Loss 1.1389e+00 (8.8814e-01)    Acc@1  72.66 ( 78.59)   Acc@5  89.84 ( 91.10)
Epoch: [39][201/625]    Time  0.687 ( 0.708)    Data  0.224 ( 0.244)    Loss 1.0881e+00 (8.9442e-01)    Acc@1  78.12 ( 78.39)   Acc@5  88.28 ( 90.83)
Epoch: [39][301/625]    Time  0.688 ( 0.707)    Data  0.222 ( 0.243)    Loss 9.1092e-01 (8.9840e-01)    Acc@1  81.25 ( 78.32)   Acc@5  88.28 ( 90.83)
Epoch: [39][401/625]    Time  0.691 ( 0.703)    Data  0.224 ( 0.239)    Loss 9.5430e-01 (8.9413e-01)    Acc@1  75.78 ( 78.40)   Acc@5  91.41 ( 90.87)
Epoch: [39][501/625]    Time  0.701 ( 0.701)    Data  0.235 ( 0.237)    Loss 1.1118e+00 (8.9754e-01)    Acc@1  72.66 ( 78.16)   Acc@5  84.38 ( 90.80)
Epoch: [39][601/625]    Time  0.681 ( 0.703)    Data  0.223 ( 0.239)    Loss 9.3901e-01 (9.0276e-01)    Acc@1  78.91 ( 77.99)   Acc@5  88.28 ( 90.73)
train Loss: 0.9025 Acc: 0.7798
Epoch: [39][  1/625]    Time  0.756 ( 0.702)    Data  0.608 ( 0.239)    Loss 2.1889e+00 (9.0455e-01)    Acc@1  50.00 ( 77.94)   Acc@5  78.12 ( 90.71)
Epoch: [39][101/625]    Time  0.375 ( 0.657)    Data  0.224 ( 0.237)    Loss 2.3674e+00 (1.0833e+00)    Acc@1  46.88 ( 74.23)   Acc@5  71.88 ( 88.42)
val Loss: 2.1881 Acc: 0.5129

Training complete in 365m 40s
Best val Acc: 0.519650
Test Loss: 0.421615

Test Accuracy of n01443537: 72% (73/101)
Test Accuracy of n01629819: 84% (85/101)
Test Accuracy of n01641577: 54% (54/100)
Test Accuracy of n01644900: 35% (38/107)
Test Accuracy of n01698640: 55% (57/103)
Test Accuracy of n01742172: 45% (44/97)
Test Accuracy of n01768244: 74% (72/97)
Test Accuracy of n01770393: 49% (44/89)
Test Accuracy of n01774384: 64% (65/101)
Test Accuracy of n01774750: 60% (62/103)
Test Accuracy of n01784675: 46% (46/98)
Test Accuracy of n01855672: 58% (62/106)
Test Accuracy of n01882714: 56% (60/107)
Test Accuracy of n01910747: 72% (70/97)
Test Accuracy of n01917289: 66% (66/99)
Test Accuracy of n01944390: 41% (44/107)
Test Accuracy of n01945685: 34% (39/113)
Test Accuracy of n01950731: 49% (51/103)
Test Accuracy of n01983481: 50% (39/78)
Test Accuracy of n01984695: 44% (41/93)
Test Accuracy of n02002724: 66% (72/109)
Test Accuracy of n02056570: 70% (66/93)
Test Accuracy of n02058221: 63% (71/111)
Test Accuracy of n02074367: 79% (72/91)
Test Accuracy of n02085620: 38% (46/119)
Test Accuracy of n02094433: 63% (63/100)
Test Accuracy of n02099601: 62% (76/122)
Test Accuracy of n02099712: 27% (33/122)
Test Accuracy of n02106662: 52% (48/92)
Test Accuracy of n02113799: 40% (46/113)
Test Accuracy of n02123045: 53% (61/115)
Test Accuracy of n02123394: 64% (64/100)
Test Accuracy of n02124075: 37% (35/93)
Test Accuracy of n02125311: 44% (49/109)
Test Accuracy of n02129165: 59% (55/92)
Test Accuracy of n02132136: 51% (47/91)
Test Accuracy of n02165456: 64% (57/88)
Test Accuracy of n02190166: 50% (48/96)
Test Accuracy of n02206856: 49% (44/89)
Test Accuracy of n02226429: 45% (46/101)
Test Accuracy of n02231487: 54% (52/95)
Test Accuracy of n02233338: 49% (54/109)
Test Accuracy of n02236044: 41% (48/117)
Test Accuracy of n02268443: 39% (42/107)
Test Accuracy of n02279972: 86% (85/98)
Test Accuracy of n02281406: 81% (95/117)
Test Accuracy of n02321529: 51% (45/88)
Test Accuracy of n02364673: 46% (49/105)
Test Accuracy of n02395406: 31% (25/80)
Test Accuracy of n02403003: 38% (44/113)
Test Accuracy of n02410509: 61% (69/112)
Test Accuracy of n02415577: 52% (55/104)
Test Accuracy of n02423022: 73% (61/83)
Test Accuracy of n02437312: 50% (45/90)
Test Accuracy of n02480495: 66% (66/100)
Test Accuracy of n02481823: 64% (70/108)
Test Accuracy of n02486410: 39% (37/94)
Test Accuracy of n02504458: 68% (68/99)
Test Accuracy of n02509815: 75% (68/90)
Test Accuracy of n02666196: 50% (51/102)
Test Accuracy of n02669723: 63% (53/84)
Test Accuracy of n02699494: 50% (52/103)
Test Accuracy of n02730930: 38% (42/110)
Test Accuracy of n02769748: 39% (39/99)
Test Accuracy of n02788148: 27% (28/102)
Test Accuracy of n02791270: 35% (34/97)
Test Accuracy of n02793495: 57% (58/101)
Test Accuracy of n02795169: 38% (43/111)
Test Accuracy of n02802426: 65% (78/119)
Test Accuracy of n02808440: 50% (42/84)
Test Accuracy of n02814533: 51% (53/102)
Test Accuracy of n02814860: 56% (52/92)
Test Accuracy of n02815834: 52% (51/98)
Test Accuracy of n02823428: 42% (46/107)
Test Accuracy of n02837789: 47% (55/117)
Test Accuracy of n02841315: 39% (38/96)
Test Accuracy of n02843684: 57% (56/98)
Test Accuracy of n02883205: 42% (46/108)
Test Accuracy of n02892201: 69% (66/95)
Test Accuracy of n02906734: 37% (35/94)
Test Accuracy of n02909870: 27% (25/91)
Test Accuracy of n02917067: 80% (72/90)
Test Accuracy of n02927161: 54% (51/94)
Test Accuracy of n02948072: 42% (38/89)
Test Accuracy of n02950826: 38% (31/80)
Test Accuracy of n02963159: 40% (43/105)
Test Accuracy of n02977058: 51% (46/89)
Test Accuracy of n02988304: 51% (47/91)
Test Accuracy of n02999410: 27% (28/101)
Test Accuracy of n03014705: 38% (41/107)
Test Accuracy of n03026506: 59% (58/97)
Test Accuracy of n03042490: 69% (75/108)
Test Accuracy of n03085013: 54% (51/93)
Test Accuracy of n03089624: 43% (44/101)
Test Accuracy of n03100240: 40% (36/90)
Test Accuracy of n03126707: 47% (47/98)
Test Accuracy of n03160309: 42% (46/107)
Test Accuracy of n03179701: 56% (55/97)
Test Accuracy of n03201208: 53% (53/99)
Test Accuracy of n03250847: 32% (32/97)
Test Accuracy of n03255030: 28% (24/83)
Test Accuracy of n03355925: 57% (59/102)
Test Accuracy of n03388043: 44% (42/95)
Test Accuracy of n03393912: 80% (74/92)
Test Accuracy of n03400231: 39% (35/88)
Test Accuracy of n03404251: 30% (39/127)
Test Accuracy of n03424325: 38% (39/101)
Test Accuracy of n03444034: 67% (59/88)
Test Accuracy of n03447447: 71% (71/100)
Test Accuracy of n03544143: 62% (51/82)
Test Accuracy of n03584254: 53% (49/92)
Test Accuracy of n03599486: 61% (77/125)
Test Accuracy of n03617480: 51% (50/97)
Test Accuracy of n03637318: 52% (51/98)
Test Accuracy of n03649909: 43% (49/112)
Test Accuracy of n03662601: 74% (68/91)
Test Accuracy of n03670208: 55% (59/106)
Test Accuracy of n03706229: 63% (64/101)
Test Accuracy of n03733131: 65% (57/87)
Test Accuracy of n03763968: 42% (45/105)
Test Accuracy of n03770439: 41% (46/112)
Test Accuracy of n03796401: 54% (51/93)
Test Accuracy of n03804744: 29% (32/110)
Test Accuracy of n03814639: 39% (41/104)
Test Accuracy of n03837869: 63% (65/102)
Test Accuracy of n03838899: 44% (45/101)
Test Accuracy of n03854065: 74% (81/109)
Test Accuracy of n03891332: 41% (43/103)
Test Accuracy of n03902125: 57% (56/97)
Test Accuracy of n03930313: 48% (57/118)
Test Accuracy of n03937543: 30% (29/94)
Test Accuracy of n03970156:  7% ( 8/102)
Test Accuracy of n03976657: 25% (27/106)
Test Accuracy of n03977966: 73% (76/104)
Test Accuracy of n03980874: 52% (59/113)
Test Accuracy of n03983396: 31% (33/105)
Test Accuracy of n03992509: 40% (40/98)
Test Accuracy of n04008634: 29% (30/101)
Test Accuracy of n04023962: 35% (33/94)
Test Accuracy of n04067472: 35% (34/95)
Test Accuracy of n04070727: 57% (58/101)
Test Accuracy of n04074963: 41% (36/87)
Test Accuracy of n04099969: 45% (45/99)
Test Accuracy of n04118538: 72% (67/92)
Test Accuracy of n04133789: 44% (38/85)
Test Accuracy of n04146614: 76% (83/108)
Test Accuracy of n04149813: 82% (73/88)
Test Accuracy of n04179913: 45% (43/95)
Test Accuracy of n04251144: 47% (48/101)
Test Accuracy of n04254777: 45% (39/85)
Test Accuracy of n04259630: 49% (49/100)
Test Accuracy of n04265275: 55% (57/103)
Test Accuracy of n04275548: 59% (58/97)
Test Accuracy of n04285008: 53% (66/123)
Test Accuracy of n04311004: 56% (65/116)
Test Accuracy of n04328186: 52% (56/106)
Test Accuracy of n04356056: 34% (36/103)
Test Accuracy of n04366367: 54% (54/99)
Test Accuracy of n04371430: 32% (31/96)
Test Accuracy of n04376876: 17% (19/108)
Test Accuracy of n04398044: 52% (55/104)
Test Accuracy of n04399382: 55% (52/94)
Test Accuracy of n04417672: 66% (60/90)
Test Accuracy of n04456115: 57% (60/104)
Test Accuracy of n04465501: 51% (58/113)
Test Accuracy of n04486054: 82% (65/79)
Test Accuracy of n04487081: 76% (80/104)
Test Accuracy of n04501370: 39% (41/103)
Test Accuracy of n04507155: 42% (46/107)
Test Accuracy of n04532106: 43% (43/100)
Test Accuracy of n04532670: 61% (63/103)
Test Accuracy of n04540053: 51% (53/103)
Test Accuracy of n04560804: 26% (29/108)
Test Accuracy of n04562935: 76% (80/104)
Test Accuracy of n04596742: 40% (39/96)
Test Accuracy of n04597913: 21% (22/101)
Test Accuracy of n06596364: 53% (49/91)
Test Accuracy of n07579787: 51% (46/90)
Test Accuracy of n07583066: 64% (64/99)
Test Accuracy of n07614500: 31% (26/83)
Test Accuracy of n07615774: 45% (43/94)
Test Accuracy of n07711569: 40% (45/111)
Test Accuracy of n07715103: 63% (56/88)
Test Accuracy of n07720875: 67% (70/104)
Test Accuracy of n07734744: 49% (50/102)
Test Accuracy of n07747607: 58% (60/103)
Test Accuracy of n07749582: 62% (64/102)
Test Accuracy of n07753592: 56% (42/75)
Test Accuracy of n07768694: 78% (80/102)
Test Accuracy of n07871810: 39% (36/92)
Test Accuracy of n07873807: 69% (65/94)
Test Accuracy of n07875152: 61% (66/107)
Test Accuracy of n07920052: 66% (71/106)
Test Accuracy of n09193705: 58% (58/100)
Test Accuracy of n09246464: 42% (51/119)
Test Accuracy of n09256479: 48% (48/98)
Test Accuracy of n09332890: 39% (43/110)
Test Accuracy of n09428293: 44% (46/103)
Test Accuracy of n12267677: 54% (47/86)

Test Accuracy (Overall): 51% (10321/20000)