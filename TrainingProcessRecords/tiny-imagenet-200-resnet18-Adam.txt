PS C:\Users\GeoFly\Github\MultiModalClassifier-1\TorchClassifier> python myTorchTrainer.py --data_name 'tiny-imagenet-200' --data_type 'trainonly' --data_path r"C:\Users\GeoFly\Documents\rfan\MultiModalClassifier\Dataset" --model_name 'mlpmodel1' --learningratename 'StepLR' --lr 0.1 --momentum 0.9 --wd 1e-4 --optimizer 'SGD' --epochs 20
2.1.1+cu118
Torch Version:  2.1.1+cu118
Torchvision Version:  0.16.1+cpu
Output path: ./outputs/tiny-imagenet-200_mlpmodel1_0326
Num GPUs: 2
0
NVIDIA GeForce RTX 3060
Traceback (most recent call last):
  File "C:\Users\GeoFly\Github\MultiModalClassifier-1\TorchClassifier\myTorchTrainer.py", line 478, in <module>
    main()
  File "C:\Users\GeoFly\Github\MultiModalClassifier-1\TorchClassifier\myTorchTrainer.py", line 392, in main
    dataloaders, dataset_sizes, class_names, img_shape = loadTorchdataset(args.data_name,args.data_type, args.data_path, args.img_height, args.img_width, args.batchsize)
  File "c:\users\geofly\github\multimodalclassifier-1\TorchClassifier\Datasetutil\Torchdatasetutil.py", line 169, in loadTorchdataset
    return loadimagefoldertrainonlydataset(name, path, split=['train'])
  File "c:\users\geofly\github\multimodalclassifier-1\TorchClassifier\Datasetutil\Torchdatasetutil.py", line 181, in loadimagefoldertrainonlydataset
    train_data = datasets.ImageFolder(train_dir, transform=data_transform)
  File "C:\Users\GeoFly\AppData\Roaming\Python\Python310\site-packages\torchvision\datasets\folder.py", line 309, in __init__
    super().__init__(
  File "C:\Users\GeoFly\AppData\Roaming\Python\Python310\site-packages\torchvision\datasets\folder.py", line 144, in __init__
    classes, class_to_idx = self.find_classes(self.root)
  File "C:\Users\GeoFly\AppData\Roaming\Python\Python310\site-packages\torchvision\datasets\folder.py", line 218, in find_classes
    return find_classes(directory)
  File "C:\Users\GeoFly\AppData\Roaming\Python\Python310\site-packages\torchvision\datasets\folder.py", line 40, in find_classes
    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())
OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'rC:\\Users\\GeoFly\\Documents\\rfan\\MultiModalClassifier\\Dataset\\tiny-imagenet-200\\train'
PS C:\Users\GeoFly\Github\MultiModalClassifier-1\TorchClassifier> python myTorchTrainer.py --data_name 'tiny-imagenet-200' --data_type 'trainonly' --data_path "C:\Users\GeoFly\Documents\rfan\MultiModalClassifier\Dataset" --model_name 'resnetmodel1' --learningratename 'StepLR' --lr 0.1 --momentum 0.9 --wd 1e-4 --optimizer 'Adam' --epochs 20
2.1.1+cu118
Torch Version:  2.1.1+cu118
Torchvision Version:  0.16.1+cpu
Output path: ./outputs/tiny-imagenet-200_resnetmodel1_0326
Num GPUs: 2
0
NVIDIA GeForce RTX 3060
True
Num training images:  100000
Number of classes:  200
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
ResNet (ResNet)                          [128, 3, 224, 224]   [128, 200]           --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 64, 112, 112]  9,408                True
├─BatchNorm2d (bn1)                      [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
├─ReLU (relu)                            [128, 64, 112, 112]  [128, 64, 112, 112]  --                   --
├─MaxPool2d (maxpool)                    [128, 64, 112, 112]  [128, 64, 56, 56]    --                   --
├─Sequential (layer1)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    └─BasicBlock (0)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn1)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn2)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    └─BasicBlock (1)                    [128, 64, 56, 56]    [128, 64, 56, 56]    --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn1)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [128, 64, 56, 56]    [128, 64, 56, 56]    36,864               True
│    │    └─BatchNorm2d (bn2)            [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    │    └─ReLU (relu)                  [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
├─Sequential (layer2)                    [128, 64, 56, 56]    [128, 128, 28, 28]   --                   True
│    └─BasicBlock (0)                    [128, 64, 56, 56]    [128, 128, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 64, 56, 56]    [128, 128, 28, 28]   73,728               True
│    │    └─BatchNorm2d (bn1)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─Sequential (downsample)      [128, 64, 56, 56]    [128, 128, 28, 28]   8,448                True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    └─BasicBlock (1)                    [128, 128, 28, 28]   [128, 128, 28, 28]   --                   True
│    │    └─Conv2d (conv1)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn1)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
│    │    └─Conv2d (conv2)               [128, 128, 28, 28]   [128, 128, 28, 28]   147,456              True
│    │    └─BatchNorm2d (bn2)            [128, 128, 28, 28]   [128, 128, 28, 28]   256                  True
│    │    └─ReLU (relu)                  [128, 128, 28, 28]   [128, 128, 28, 28]   --                   --
├─Sequential (layer3)                    [128, 128, 28, 28]   [128, 256, 14, 14]   --                   True
│    └─BasicBlock (0)                    [128, 128, 28, 28]   [128, 256, 14, 14]   --                   True
│    │    └─Conv2d (conv1)               [128, 128, 28, 28]   [128, 256, 14, 14]   294,912              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─Sequential (downsample)      [128, 128, 28, 28]   [128, 256, 14, 14]   33,280               True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    └─BasicBlock (1)                    [128, 256, 14, 14]   [128, 256, 14, 14]   --                   True
│    │    └─Conv2d (conv1)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn1)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
│    │    └─Conv2d (conv2)               [128, 256, 14, 14]   [128, 256, 14, 14]   589,824              True
│    │    └─BatchNorm2d (bn2)            [128, 256, 14, 14]   [128, 256, 14, 14]   512                  True
│    │    └─ReLU (relu)                  [128, 256, 14, 14]   [128, 256, 14, 14]   --                   --
├─Sequential (layer4)                    [128, 256, 14, 14]   [128, 512, 7, 7]     --                   True
│    └─BasicBlock (0)                    [128, 256, 14, 14]   [128, 512, 7, 7]     --                   True
│    │    └─Conv2d (conv1)               [128, 256, 14, 14]   [128, 512, 7, 7]     1,179,648            True
│    │    └─BatchNorm2d (bn1)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv2)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn2)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─Sequential (downsample)      [128, 256, 14, 14]   [128, 512, 7, 7]     132,096              True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    └─BasicBlock (1)                    [128, 512, 7, 7]     [128, 512, 7, 7]     --                   True
│    │    └─Conv2d (conv1)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn1)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
│    │    └─Conv2d (conv2)               [128, 512, 7, 7]     [128, 512, 7, 7]     2,359,296            True
│    │    └─BatchNorm2d (bn2)            [128, 512, 7, 7]     [128, 512, 7, 7]     1,024                True
│    │    └─ReLU (relu)                  [128, 512, 7, 7]     [128, 512, 7, 7]     --                   --
├─AdaptiveAvgPool2d (avgpool)            [128, 512, 7, 7]     [128, 512, 1, 1]     --                   --
├─Linear (fc)                            [128, 512]           [128, 200]           102,600              True
========================================================================================================================
Total params: 11,279,112
Trainable params: 11,279,112
Non-trainable params: 0
Total mult-adds (G): 232.15
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 5086.85
Params size (MB): 45.12
Estimated Total Size (MB): 5209.03
========================================================================================================================
Epoch 0/19
----------
Epoch: [0][  1/625]     Time  0.891 ( 0.891)    Data  0.458 ( 0.458)    Loss 5.5031e+00 (5.5031e+00)    Acc@1   0.78 (  0.78)   Acc@5   1.56 (  1.56)
STAGE:2024-03-26 10:52:14 18024:12124 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-03-26 10:52:16 18024:12124 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-03-26 10:52:16 18024:12124 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-03-26 10:52:20 18024:12124 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-03-26 10:52:22 18024:12124 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-03-26 10:52:22 18024:12124 ..\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Epoch: [0][101/625]     Time  0.444 ( 0.523)    Data  0.204 ( 0.279)    Loss 4.1552e+00 (4.6182e+00)    Acc@1  10.16 (  7.68)   Acc@5  33.59 ( 21.36)
Epoch: [0][201/625]     Time  0.682 ( 0.593)    Data  0.218 ( 0.241)    Loss 3.7122e+00 (4.2823e+00)    Acc@1  19.53 ( 11.21)   Acc@5  45.31 ( 29.04)
Epoch: [0][301/625]     Time  0.635 ( 0.611)    Data  0.172 ( 0.222)    Loss 3.5441e+00 (4.0885e+00)    Acc@1  21.88 ( 13.58)   Acc@5  46.09 ( 33.45)
Epoch: [0][401/625]     Time  0.629 ( 0.615)    Data  0.168 ( 0.209)    Loss 3.4098e+00 (3.9434e+00)    Acc@1  25.78 ( 15.60)   Acc@5  46.88 ( 36.69)
Epoch: [0][501/625]     Time  0.632 ( 0.618)    Data  0.167 ( 0.201)    Loss 2.8592e+00 (3.8253e+00)    Acc@1  34.38 ( 17.35)   Acc@5  61.72 ( 39.26)
Epoch: [0][601/625]     Time  0.624 ( 0.620)    Data  0.168 ( 0.196)    Loss 3.3131e+00 (3.7383e+00)    Acc@1  24.22 ( 18.75)   Acc@5  47.66 ( 41.29)
train Loss: 3.7164 Acc: 0.1906
Epoch: [0][  1/625]     Time  0.396 ( 0.620)    Data  0.248 ( 0.195)    Loss 3.5413e+00 (3.7161e+00)    Acc@1  21.88 ( 19.07)   Acc@5  50.00 ( 41.78)
Epoch: [0][101/625]     Time  0.314 ( 0.578)    Data  0.167 ( 0.192)    Loss 3.3813e+00 (3.6675e+00)    Acc@1  22.66 ( 19.82)   Acc@5  46.88 ( 42.98)
val Loss: 3.3455 Acc: 0.2487

Epoch 1/19
----------
Epoch: [1][  1/625]     Time  0.763 ( 0.763)    Data  0.301 ( 0.301)    Loss 3.0960e+00 (3.0960e+00)    Acc@1  28.91 ( 28.91)   Acc@5  53.91 ( 53.91)
Epoch: [1][101/625]     Time  0.625 ( 0.626)    Data  0.162 ( 0.167)    Loss 3.0489e+00 (3.0995e+00)    Acc@1  34.38 ( 28.93)   Acc@5  53.12 ( 55.61)
Epoch: [1][201/625]     Time  0.629 ( 0.625)    Data  0.164 ( 0.166)    Loss 2.6852e+00 (3.0734e+00)    Acc@1  40.62 ( 29.40)   Acc@5  64.06 ( 55.92)
Epoch: [1][301/625]     Time  0.618 ( 0.624)    Data  0.164 ( 0.165)    Loss 3.1762e+00 (3.0327e+00)    Acc@1  34.38 ( 30.20)   Acc@5  57.03 ( 56.84)
Epoch: [1][401/625]     Time  0.617 ( 0.624)    Data  0.164 ( 0.164)    Loss 3.0377e+00 (3.0175e+00)    Acc@1  32.81 ( 30.60)   Acc@5  60.16 ( 57.18)
Epoch: [1][501/625]     Time  0.621 ( 0.624)    Data  0.162 ( 0.164)    Loss 2.7590e+00 (2.9889e+00)    Acc@1  32.81 ( 31.15)   Acc@5  57.81 ( 57.67)
Epoch: [1][601/625]     Time  0.629 ( 0.624)    Data  0.166 ( 0.164)    Loss 2.9123e+00 (2.9698e+00)    Acc@1  32.81 ( 31.58)   Acc@5  61.72 ( 58.10)
train Loss: 2.9656 Acc: 0.3163
Epoch: [1][  1/625]     Time  0.375 ( 0.624)    Data  0.228 ( 0.164)    Loss 2.9412e+00 (2.9655e+00)    Acc@1  32.81 ( 31.63)   Acc@5  57.03 ( 58.20)
Epoch: [1][101/625]     Time  0.305 ( 0.581)    Data  0.161 ( 0.164)    Loss 3.2937e+00 (2.9820e+00)    Acc@1  24.22 ( 31.44)   Acc@5  46.88 ( 57.86)
val Loss: 3.0791 Acc: 0.3012

Epoch 2/19
----------
Epoch: [2][  1/625]     Time  0.932 ( 0.932)    Data  0.465 ( 0.465)    Loss 2.5878e+00 (2.5878e+00)    Acc@1  43.75 ( 43.75)   Acc@5  68.75 ( 68.75)
Epoch: [2][101/625]     Time  0.624 ( 0.627)    Data  0.162 ( 0.166)    Loss 2.8771e+00 (2.7114e+00)    Acc@1  38.28 ( 36.96)   Acc@5  63.28 ( 63.40)
Epoch: [2][201/625]     Time  0.615 ( 0.626)    Data  0.162 ( 0.165)    Loss 2.6808e+00 (2.7052e+00)    Acc@1  35.94 ( 37.02)   Acc@5  62.50 ( 63.45)
Epoch: [2][301/625]     Time  0.620 ( 0.630)    Data  0.162 ( 0.167)    Loss 2.6393e+00 (2.7058e+00)    Acc@1  32.81 ( 36.94)   Acc@5  64.84 ( 63.35)
Epoch: [2][401/625]     Time  0.622 ( 0.628)    Data  0.164 ( 0.166)    Loss 2.3857e+00 (2.6963e+00)    Acc@1  37.50 ( 37.00)   Acc@5  67.19 ( 63.61)
Epoch: [2][501/625]     Time  0.616 ( 0.628)    Data  0.163 ( 0.166)    Loss 2.3862e+00 (2.6922e+00)    Acc@1  44.53 ( 37.03)   Acc@5  71.09 ( 63.74)
Epoch: [2][601/625]     Time  0.617 ( 0.627)    Data  0.162 ( 0.166)    Loss 2.8073e+00 (2.6873e+00)    Acc@1  32.03 ( 37.03)   Acc@5  60.94 ( 63.86)
train Loss: 2.6856 Acc: 0.3706
Epoch: [2][  1/625]     Time  0.378 ( 0.627)    Data  0.231 ( 0.166)    Loss 2.7324e+00 (2.6857e+00)    Acc@1  38.28 ( 37.06)   Acc@5  64.84 ( 63.88)
Epoch: [2][101/625]     Time  0.311 ( 0.583)    Data  0.163 ( 0.166)    Loss 3.1329e+00 (2.7034e+00)    Acc@1  29.69 ( 36.82)   Acc@5  54.69 ( 63.54)
val Loss: 2.8123 Acc: 0.3520

Epoch 3/19
----------
Epoch: [3][  1/625]     Time  0.788 ( 0.788)    Data  0.323 ( 0.323)    Loss 2.7350e+00 (2.7350e+00)    Acc@1  35.94 ( 35.94)   Acc@5  64.06 ( 64.06)
Epoch: [3][101/625]     Time  0.630 ( 0.628)    Data  0.165 ( 0.167)    Loss 2.3358e+00 (2.4103e+00)    Acc@1  46.09 ( 42.26)   Acc@5  70.31 ( 68.29)
Epoch: [3][201/625]     Time  0.622 ( 0.626)    Data  0.163 ( 0.165)    Loss 2.3034e+00 (2.4400e+00)    Acc@1  45.31 ( 41.57)   Acc@5  67.97 ( 67.98)
Epoch: [3][301/625]     Time  0.616 ( 0.626)    Data  0.163 ( 0.165)    Loss 2.4514e+00 (2.4550e+00)    Acc@1  46.09 ( 41.51)   Acc@5  72.66 ( 67.91)
Epoch: [3][401/625]     Time  0.626 ( 0.626)    Data  0.163 ( 0.165)    Loss 2.2401e+00 (2.4650e+00)    Acc@1  42.19 ( 41.41)   Acc@5  72.66 ( 67.70)
Epoch: [3][501/625]     Time  0.619 ( 0.626)    Data  0.165 ( 0.165)    Loss 2.2685e+00 (2.4747e+00)    Acc@1  43.75 ( 41.36)   Acc@5  70.31 ( 67.57)
Epoch: [3][601/625]     Time  0.628 ( 0.626)    Data  0.163 ( 0.165)    Loss 2.5642e+00 (2.4799e+00)    Acc@1  40.62 ( 41.32)   Acc@5  64.06 ( 67.51)
train Loss: 2.4810 Acc: 0.4134
Epoch: [3][  1/625]     Time  0.388 ( 0.625)    Data  0.239 ( 0.165)    Loss 2.5110e+00 (2.4811e+00)    Acc@1  39.84 ( 41.34)   Acc@5  64.06 ( 67.47)
Epoch: [3][101/625]     Time  0.308 ( 0.582)    Data  0.164 ( 0.165)    Loss 2.5702e+00 (2.5171e+00)    Acc@1  40.62 ( 40.74)   Acc@5  65.62 ( 66.90)
val Loss: 2.7211 Acc: 0.3745

Epoch 4/19
----------
Epoch: [4][  1/625]     Time  0.921 ( 0.921)    Data  0.461 ( 0.461)    Loss 1.9449e+00 (1.9449e+00)    Acc@1  57.03 ( 57.03)   Acc@5  75.00 ( 75.00)
Epoch: [4][101/625]     Time  0.627 ( 0.628)    Data  0.163 ( 0.167)    Loss 2.6199e+00 (2.2643e+00)    Acc@1  36.72 ( 45.27)   Acc@5  63.28 ( 71.26)
Epoch: [4][201/625]     Time  0.615 ( 0.626)    Data  0.162 ( 0.165)    Loss 2.1984e+00 (2.3086e+00)    Acc@1  45.31 ( 44.64)   Acc@5  75.00 ( 70.58)
Epoch: [4][301/625]     Time  0.619 ( 0.626)    Data  0.163 ( 0.165)    Loss 2.3154e+00 (2.3194e+00)    Acc@1  45.31 ( 44.44)   Acc@5  67.97 ( 70.38)
Epoch: [4][401/625]     Time  0.621 ( 0.626)    Data  0.163 ( 0.165)    Loss 2.4921e+00 (2.3235e+00)    Acc@1  46.88 ( 44.43)   Acc@5  65.62 ( 70.30)
Epoch: [4][501/625]     Time  0.623 ( 0.626)    Data  0.163 ( 0.165)    Loss 2.6866e+00 (2.3216e+00)    Acc@1  40.62 ( 44.62)   Acc@5  65.62 ( 70.39)
Epoch: [4][601/625]     Time  0.627 ( 0.625)    Data  0.165 ( 0.164)    Loss 2.8156e+00 (2.3225e+00)    Acc@1  35.94 ( 44.54)   Acc@5  61.72 ( 70.38)
train Loss: 2.3237 Acc: 0.4452
Epoch: [4][  1/625]     Time  0.383 ( 0.625)    Data  0.234 ( 0.165)    Loss 3.0362e+00 (2.3248e+00)    Acc@1  34.38 ( 44.51)   Acc@5  56.25 ( 70.36)
Epoch: [4][101/625]     Time  0.309 ( 0.582)    Data  0.164 ( 0.165)    Loss 2.7403e+00 (2.3686e+00)    Acc@1  38.28 ( 43.80)   Acc@5  65.62 ( 69.60)
val Loss: 2.6586 Acc: 0.3921

Epoch 5/19
----------
Epoch: [5][  1/625]     Time  0.749 ( 0.749)    Data  0.289 ( 0.289)    Loss 1.9003e+00 (1.9003e+00)    Acc@1  53.91 ( 53.91)   Acc@5  78.12 ( 78.12)
Epoch: [5][101/625]     Time  0.628 ( 0.626)    Data  0.163 ( 0.166)    Loss 2.2913e+00 (2.1716e+00)    Acc@1  45.31 ( 48.14)   Acc@5  72.66 ( 72.99)
Epoch: [5][201/625]     Time  1.245 ( 0.821)    Data  0.790 ( 0.359)    Loss 2.3597e+00 (2.1758e+00)    Acc@1  39.84 ( 47.57)   Acc@5  67.97 ( 72.81)
Epoch: [5][301/625]     Time  1.246 ( 0.964)    Data  0.790 ( 0.503)    Loss 2.0001e+00 (2.1872e+00)    Acc@1  50.00 ( 47.34)   Acc@5  75.78 ( 72.52)
Epoch: [5][401/625]     Time  1.246 ( 1.037)    Data  0.789 ( 0.576)    Loss 2.1568e+00 (2.1966e+00)    Acc@1  43.75 ( 47.16)   Acc@5  72.66 ( 72.41)
Epoch: [5][501/625]     Time  1.255 ( 1.080)    Data  0.789 ( 0.619)    Loss 2.3136e+00 (2.1960e+00)    Acc@1  46.09 ( 47.29)   Acc@5  67.19 ( 72.37)
Epoch: [5][601/625]     Time  1.257 ( 1.109)    Data  0.795 ( 0.648)    Loss 2.1192e+00 (2.2062e+00)    Acc@1  48.44 ( 47.11)   Acc@5  71.88 ( 72.18)
train Loss: 2.2071 Acc: 0.4713
Epoch: [5][  1/625]     Time  0.991 ( 1.114)    Data  0.847 ( 0.653)    Loss 2.5300e+00 (2.2076e+00)    Acc@1  41.41 ( 47.12)   Acc@5  67.97 ( 72.15)
Epoch: [5][101/625]     Time  0.977 ( 1.093)    Data  0.829 ( 0.676)    Loss 2.5672e+00 (2.2611e+00)    Acc@1  43.75 ( 46.15)   Acc@5  66.41 ( 71.31)
val Loss: 2.5861 Acc: 0.4019

Epoch 6/19
----------
Epoch: [6][  1/625]     Time  1.097 ( 1.097)    Data  0.631 ( 0.631)    Loss 2.2347e+00 (2.2347e+00)    Acc@1  49.22 ( 49.22)   Acc@5  68.75 ( 68.75)
Epoch: [6][101/625]     Time  0.817 ( 0.807)    Data  0.353 ( 0.344)    Loss 1.8254e+00 (2.0433e+00)    Acc@1  56.25 ( 51.14)   Acc@5  78.91 ( 74.81)
Epoch: [6][201/625]     Time  0.787 ( 0.804)    Data  0.323 ( 0.341)    Loss 2.0770e+00 (2.0456e+00)    Acc@1  47.66 ( 50.85)   Acc@5  73.44 ( 74.88)
Epoch: [6][301/625]     Time  0.816 ( 0.804)    Data  0.356 ( 0.341)    Loss 1.9834e+00 (2.0649e+00)    Acc@1  52.34 ( 50.35)   Acc@5  74.22 ( 74.46)
Epoch: [6][401/625]     Time  0.802 ( 0.804)    Data  0.342 ( 0.342)    Loss 2.1162e+00 (2.0761e+00)    Acc@1  50.78 ( 50.01)   Acc@5  75.00 ( 74.37)
Epoch: [6][501/625]     Time  0.817 ( 0.803)    Data  0.357 ( 0.341)    Loss 1.9407e+00 (2.0868e+00)    Acc@1  57.03 ( 49.76)   Acc@5  79.69 ( 74.15)
Epoch: [6][601/625]     Time  0.837 ( 0.803)    Data  0.373 ( 0.340)    Loss 2.2710e+00 (2.0951e+00)    Acc@1  42.97 ( 49.56)   Acc@5  69.53 ( 74.05)
train Loss: 2.0987 Acc: 0.4949
Epoch: [6][  1/625]     Time  0.415 ( 0.802)    Data  0.270 ( 0.340)    Loss 2.5363e+00 (2.0994e+00)    Acc@1  42.19 ( 49.48)   Acc@5  66.41 ( 74.01)
Epoch: [6][101/625]     Time  0.343 ( 0.739)    Data  0.195 ( 0.320)    Loss 2.6330e+00 (2.1584e+00)    Acc@1  38.28 ( 48.44)   Acc@5  63.28 ( 73.07)
val Loss: 2.5216 Acc: 0.4197

Epoch 7/19
----------
Epoch: [7][  1/625]     Time  0.775 ( 0.775)    Data  0.310 ( 0.310)    Loss 1.8424e+00 (1.8424e+00)    Acc@1  60.94 ( 60.94)   Acc@5  77.34 ( 77.34)
Epoch: [7][101/625]     Time  0.626 ( 0.657)    Data  0.163 ( 0.196)    Loss 2.1722e+00 (1.9255e+00)    Acc@1  46.88 ( 52.58)   Acc@5  71.09 ( 77.00)
Epoch: [7][201/625]     Time  0.622 ( 0.641)    Data  0.164 ( 0.180)    Loss 2.1704e+00 (1.9379e+00)    Acc@1  48.44 ( 52.47)   Acc@5  70.31 ( 76.82)
Epoch: [7][301/625]     Time  0.631 ( 0.635)    Data  0.167 ( 0.175)    Loss 2.0941e+00 (1.9591e+00)    Acc@1  48.44 ( 52.18)   Acc@5  77.34 ( 76.51)
Epoch: [7][401/625]     Time  0.630 ( 0.633)    Data  0.166 ( 0.173)    Loss 2.1937e+00 (1.9768e+00)    Acc@1  42.97 ( 51.74)   Acc@5  70.31 ( 76.15)
Epoch: [7][501/625]     Time  0.621 ( 0.632)    Data  0.167 ( 0.172)    Loss 1.7893e+00 (1.9856e+00)    Acc@1  54.69 ( 51.58)   Acc@5  78.12 ( 76.01)
Epoch: [7][601/625]     Time  0.625 ( 0.632)    Data  0.166 ( 0.171)    Loss 2.0227e+00 (1.9962e+00)    Acc@1  49.22 ( 51.37)   Acc@5  77.34 ( 75.83)
train Loss: 1.9983 Acc: 0.5135
Epoch: [7][  1/625]     Time  0.378 ( 0.631)    Data  0.230 ( 0.171)    Loss 2.8846e+00 (1.9998e+00)    Acc@1  39.06 ( 51.33)   Acc@5  60.94 ( 75.80)
Epoch: [7][101/625]     Time  0.314 ( 0.587)    Data  0.167 ( 0.171)    Loss 2.4629e+00 (2.0721e+00)    Acc@1  49.22 ( 50.07)   Acc@5  67.19 ( 74.58)
val Loss: 2.5228 Acc: 0.4220

Epoch 8/19
----------
Epoch: [8][  1/625]     Time  0.899 ( 0.899)    Data  0.439 ( 0.439)    Loss 1.9508e+00 (1.9508e+00)    Acc@1  52.34 ( 52.34)   Acc@5  73.44 ( 73.44)
Epoch: [8][101/625]     Time  0.618 ( 0.629)    Data  0.165 ( 0.168)    Loss 1.8349e+00 (1.8570e+00)    Acc@1  53.91 ( 54.75)   Acc@5  77.34 ( 77.89)
Epoch: [8][201/625]     Time  0.618 ( 0.627)    Data  0.165 ( 0.167)    Loss 1.7359e+00 (1.8868e+00)    Acc@1  59.38 ( 53.93)   Acc@5  82.81 ( 77.64)
Epoch: [8][301/625]     Time  0.628 ( 0.627)    Data  0.165 ( 0.166)    Loss 1.6653e+00 (1.8927e+00)    Acc@1  60.94 ( 53.81)   Acc@5  78.91 ( 77.61)
Epoch: [8][401/625]     Time  0.632 ( 0.627)    Data  0.167 ( 0.166)    Loss 1.5808e+00 (1.8979e+00)    Acc@1  57.81 ( 53.74)   Acc@5  81.25 ( 77.56)
Epoch: [8][501/625]     Time  0.633 ( 0.627)    Data  0.174 ( 0.166)    Loss 1.9904e+00 (1.9017e+00)    Acc@1  46.88 ( 53.57)   Acc@5  73.44 ( 77.45)
Epoch: [8][601/625]     Time  0.618 ( 0.627)    Data  0.165 ( 0.166)    Loss 1.8560e+00 (1.9065e+00)    Acc@1  50.00 ( 53.49)   Acc@5  78.12 ( 77.30)
train Loss: 1.9073 Acc: 0.5349
Epoch: [8][  1/625]     Time  0.368 ( 0.626)    Data  0.222 ( 0.166)    Loss 2.3320e+00 (1.9080e+00)    Acc@1  44.53 ( 53.48)   Acc@5  71.09 ( 77.27)
Epoch: [8][101/625]     Time  0.309 ( 0.583)    Data  0.164 ( 0.166)    Loss 2.6358e+00 (1.9953e+00)    Acc@1  39.06 ( 51.84)   Acc@5  66.41 ( 75.94)
val Loss: 2.5357 Acc: 0.4210

Epoch 9/19
----------
Epoch: [9][  1/625]     Time  0.744 ( 0.744)    Data  0.287 ( 0.287)    Loss 1.6542e+00 (1.6542e+00)    Acc@1  63.28 ( 63.28)   Acc@5  79.69 ( 79.69)
Epoch: [9][101/625]     Time  0.625 ( 0.626)    Data  0.163 ( 0.166)    Loss 1.2935e+00 (1.7503e+00)    Acc@1  65.62 ( 57.05)   Acc@5  87.50 ( 80.11)
Epoch: [9][201/625]     Time  0.626 ( 0.625)    Data  0.163 ( 0.164)    Loss 1.8013e+00 (1.7662e+00)    Acc@1  59.38 ( 56.41)   Acc@5  76.56 ( 79.74)
Epoch: [9][301/625]     Time  0.628 ( 0.624)    Data  0.162 ( 0.164)    Loss 1.7439e+00 (1.7898e+00)    Acc@1  55.47 ( 55.82)   Acc@5  76.56 ( 79.35)
Epoch: [9][401/625]     Time  0.620 ( 0.624)    Data  0.162 ( 0.163)    Loss 1.8521e+00 (1.7997e+00)    Acc@1  53.12 ( 55.66)   Acc@5  81.25 ( 79.14)
Epoch: [9][501/625]     Time  0.628 ( 0.624)    Data  0.162 ( 0.163)    Loss 1.9963e+00 (1.8114e+00)    Acc@1  54.69 ( 55.44)   Acc@5  80.47 ( 78.91)
Epoch: [9][601/625]     Time  0.628 ( 0.624)    Data  0.163 ( 0.163)    Loss 1.4918e+00 (1.8245e+00)    Acc@1  65.62 ( 55.14)   Acc@5  83.59 ( 78.66)
train Loss: 1.8253 Acc: 0.5514
Epoch: [9][  1/625]     Time  0.364 ( 0.623)    Data  0.218 ( 0.163)    Loss 2.7477e+00 (1.8267e+00)    Acc@1  37.50 ( 55.11)   Acc@5  67.97 ( 78.63)
Epoch: [9][101/625]     Time  0.323 ( 0.580)    Data  0.176 ( 0.163)    Loss 2.6852e+00 (1.9121e+00)    Acc@1  42.19 ( 53.63)   Acc@5  64.84 ( 77.36)
val Loss: 2.4430 Acc: 0.4459

Epoch 10/19
----------
Epoch: [10][  1/625]    Time  0.906 ( 0.906)    Data  0.442 ( 0.442)    Loss 1.8724e+00 (1.8724e+00)    Acc@1  54.69 ( 54.69)   Acc@5  77.34 ( 77.34)
Epoch: [10][101/625]    Time  0.625 ( 0.626)    Data  0.162 ( 0.166)    Loss 1.9542e+00 (1.7010e+00)    Acc@1  58.59 ( 58.18)   Acc@5  72.66 ( 80.24)
Epoch: [10][201/625]    Time  0.617 ( 0.626)    Data  0.164 ( 0.165)    Loss 2.0536e+00 (1.7144e+00)    Acc@1  50.78 ( 57.40)   Acc@5  75.78 ( 80.10)
Epoch: [10][301/625]    Time  0.621 ( 0.625)    Data  0.161 ( 0.164)    Loss 1.8167e+00 (1.7225e+00)    Acc@1  53.12 ( 57.28)   Acc@5  80.47 ( 79.96)
Epoch: [10][401/625]    Time  0.615 ( 0.625)    Data  0.162 ( 0.164)    Loss 1.5284e+00 (1.7305e+00)    Acc@1  57.03 ( 57.11)   Acc@5  85.16 ( 79.89)
Epoch: [10][501/625]    Time  0.622 ( 0.624)    Data  0.163 ( 0.164)    Loss 1.5623e+00 (1.7417e+00)    Acc@1  59.38 ( 56.87)   Acc@5  83.59 ( 79.73)
Epoch: [10][601/625]    Time  0.618 ( 0.624)    Data  0.163 ( 0.163)    Loss 1.6821e+00 (1.7489e+00)    Acc@1  64.06 ( 56.70)   Acc@5  79.69 ( 79.67)
train Loss: 1.7485 Acc: 0.5674
Epoch: [10][  1/625]    Time  0.365 ( 0.624)    Data  0.219 ( 0.164)    Loss 2.3944e+00 (1.7495e+00)    Acc@1  42.97 ( 56.72)   Acc@5  69.53 ( 79.67)
Epoch: [10][101/625]    Time  0.307 ( 0.580)    Data  0.162 ( 0.163)    Loss 2.4450e+00 (1.8596e+00)    Acc@1  38.28 ( 54.80)   Acc@5  65.62 ( 78.04)
val Loss: 2.5572 Acc: 0.4279

Epoch 11/19
----------
Epoch: [11][  1/625]    Time  0.742 ( 0.742)    Data  0.278 ( 0.278)    Loss 1.8762e+00 (1.8762e+00)    Acc@1  57.03 ( 57.03)   Acc@5  77.34 ( 77.34)
Epoch: [11][101/625]    Time  0.626 ( 0.624)    Data  0.162 ( 0.164)    Loss 1.7832e+00 (1.6202e+00)    Acc@1  57.03 ( 60.05)   Acc@5  76.56 ( 81.54)
Epoch: [11][201/625]    Time  0.618 ( 0.624)    Data  0.164 ( 0.163)    Loss 1.6479e+00 (1.6449e+00)    Acc@1  53.91 ( 59.26)   Acc@5  82.81 ( 81.34)
Epoch: [11][301/625]    Time  0.628 ( 0.623)    Data  0.164 ( 0.163)    Loss 1.4503e+00 (1.6673e+00)    Acc@1  64.84 ( 58.79)   Acc@5  84.38 ( 81.04)
Epoch: [11][401/625]    Time  0.622 ( 0.623)    Data  0.162 ( 0.163)    Loss 1.7501e+00 (1.6754e+00)    Acc@1  58.59 ( 58.65)   Acc@5  80.47 ( 80.98)
Epoch: [11][501/625]    Time  0.626 ( 0.623)    Data  0.162 ( 0.163)    Loss 1.9226e+00 (1.6852e+00)    Acc@1  52.34 ( 58.36)   Acc@5  75.78 ( 80.78)
Epoch: [11][601/625]    Time  0.621 ( 0.623)    Data  0.163 ( 0.163)    Loss 1.4817e+00 (1.6908e+00)    Acc@1  60.94 ( 58.23)   Acc@5  85.16 ( 80.61)
train Loss: 1.6922 Acc: 0.5818
Epoch: [11][  1/625]    Time  0.360 ( 0.623)    Data  0.214 ( 0.163)    Loss 2.5101e+00 (1.6935e+00)    Acc@1  42.97 ( 58.16)   Acc@5  67.97 ( 80.56)
Epoch: [11][101/625]    Time  0.311 ( 0.580)    Data  0.163 ( 0.163)    Loss 2.7345e+00 (1.7926e+00)    Acc@1  39.06 ( 56.33)   Acc@5  63.28 ( 79.13)
val Loss: 2.3844 Acc: 0.4522

Epoch 12/19
----------
Epoch: [12][  1/625]    Time  0.922 ( 0.922)    Data  0.453 ( 0.453)    Loss 1.3972e+00 (1.3972e+00)    Acc@1  62.50 ( 62.50)   Acc@5  85.94 ( 85.94)
Epoch: [12][101/625]    Time  0.620 ( 0.626)    Data  0.163 ( 0.165)    Loss 1.7538e+00 (1.5539e+00)    Acc@1  59.38 ( 61.32)   Acc@5  78.91 ( 82.19)
Epoch: [12][201/625]    Time  0.628 ( 0.624)    Data  0.163 ( 0.164)    Loss 1.5571e+00 (1.5686e+00)    Acc@1  63.28 ( 61.02)   Acc@5  80.47 ( 82.03)
Epoch: [12][301/625]    Time  0.620 ( 0.624)    Data  0.162 ( 0.163)    Loss 1.7423e+00 (1.5840e+00)    Acc@1  53.12 ( 60.69)   Acc@5  81.25 ( 81.88)
Epoch: [12][401/625]    Time  0.617 ( 0.624)    Data  0.164 ( 0.163)    Loss 1.4469e+00 (1.5989e+00)    Acc@1  60.16 ( 60.23)   Acc@5  84.38 ( 81.72)
Epoch: [12][501/625]    Time  0.621 ( 0.624)    Data  0.162 ( 0.163)    Loss 1.4900e+00 (1.6158e+00)    Acc@1  58.59 ( 59.82)   Acc@5  87.50 ( 81.51)
Epoch: [12][601/625]    Time  0.622 ( 0.624)    Data  0.163 ( 0.163)    Loss 1.6453e+00 (1.6200e+00)    Acc@1  62.50 ( 59.75)   Acc@5  82.03 ( 81.51)
train Loss: 1.6213 Acc: 0.5974
Epoch: [12][  1/625]    Time  0.366 ( 0.623)    Data  0.220 ( 0.163)    Loss 2.4061e+00 (1.6225e+00)    Acc@1  44.53 ( 59.71)   Acc@5  69.53 ( 81.47)
Epoch: [12][101/625]    Time  0.306 ( 0.580)    Data  0.162 ( 0.163)    Loss 2.3245e+00 (1.7484e+00)    Acc@1  50.00 ( 57.52)   Acc@5  67.97 ( 79.62)
val Loss: 2.5515 Acc: 0.4368

Epoch 13/19
----------
Epoch: [13][  1/625]    Time  0.743 ( 0.743)    Data  0.278 ( 0.278)    Loss 1.4330e+00 (1.4330e+00)    Acc@1  60.94 ( 60.94)   Acc@5  85.16 ( 85.16)
Epoch: [13][101/625]    Time  0.629 ( 0.625)    Data  0.163 ( 0.164)    Loss 1.6889e+00 (1.5237e+00)    Acc@1  57.03 ( 61.98)   Acc@5  78.12 ( 82.73)
Epoch: [13][201/625]    Time  0.615 ( 0.624)    Data  0.162 ( 0.163)    Loss 1.2055e+00 (1.5343e+00)    Acc@1  69.53 ( 61.85)   Acc@5  83.59 ( 82.59)
Epoch: [13][301/625]    Time  0.627 ( 0.623)    Data  0.162 ( 0.163)    Loss 1.5537e+00 (1.5413e+00)    Acc@1  60.94 ( 61.77)   Acc@5  80.47 ( 82.53)
Epoch: [13][401/625]    Time  0.624 ( 0.623)    Data  0.161 ( 0.163)    Loss 1.5151e+00 (1.5499e+00)    Acc@1  57.03 ( 61.45)   Acc@5  82.81 ( 82.48)
Epoch: [13][501/625]    Time  0.497 ( 0.626)    Data  0.256 ( 0.167)    Loss 1.6867e+00 (1.5532e+00)    Acc@1  59.38 ( 61.26)   Acc@5  80.47 ( 82.53)
Epoch: [13][601/625]    Time  0.431 ( 0.594)    Data  0.191 ( 0.172)    Loss 1.9728e+00 (1.5659e+00)    Acc@1  53.12 ( 60.97)   Acc@5  80.47 ( 82.38)
train Loss: 1.5681 Acc: 0.6089
Epoch: [13][  1/625]    Time  0.311 ( 0.588)    Data  0.233 ( 0.173)    Loss 2.4114e+00 (1.5695e+00)    Acc@1  41.41 ( 60.86)   Acc@5  72.66 ( 82.35)
Epoch: [13][101/625]    Time  0.281 ( 0.545)    Data  0.194 ( 0.176)    Loss 2.1569e+00 (1.7037e+00)    Acc@1  48.44 ( 58.55)   Acc@5  75.00 ( 80.44)
val Loss: 2.5256 Acc: 0.4449

Epoch 14/19
----------
Epoch: [14][  1/625]    Time  0.904 ( 0.904)    Data  0.447 ( 0.447)    Loss 1.3466e+00 (1.3466e+00)    Acc@1  65.62 ( 65.62)   Acc@5  87.50 ( 87.50)
Epoch: [14][101/625]    Time  0.671 ( 0.660)    Data  0.208 ( 0.200)    Loss 1.4864e+00 (1.4366e+00)    Acc@1  57.81 ( 64.22)   Acc@5  85.94 ( 84.25)
Epoch: [14][201/625]    Time  0.654 ( 0.657)    Data  0.192 ( 0.197)    Loss 1.8012e+00 (1.4572e+00)    Acc@1  53.12 ( 63.55)   Acc@5  81.25 ( 83.88)
Epoch: [14][301/625]    Time  0.655 ( 0.659)    Data  0.197 ( 0.198)    Loss 1.9169e+00 (1.4749e+00)    Acc@1  60.16 ( 63.02)   Acc@5  76.56 ( 83.62)
Epoch: [14][401/625]    Time  0.655 ( 0.659)    Data  0.197 ( 0.198)    Loss 1.5145e+00 (1.4784e+00)    Acc@1  57.81 ( 62.99)   Acc@5  86.72 ( 83.53)
Epoch: [14][501/625]    Time  0.655 ( 0.659)    Data  0.199 ( 0.197)    Loss 1.3701e+00 (1.4882e+00)    Acc@1  64.84 ( 62.77)   Acc@5  86.72 ( 83.38)
Epoch: [14][601/625]    Time  0.654 ( 0.658)    Data  0.192 ( 0.197)    Loss 1.4543e+00 (1.5015e+00)    Acc@1  67.19 ( 62.49)   Acc@5  82.81 ( 83.15)
train Loss: 1.5021 Acc: 0.6245
Epoch: [14][  1/625]    Time  0.411 ( 0.658)    Data  0.264 ( 0.197)    Loss 2.4830e+00 (1.5037e+00)    Acc@1  50.00 ( 62.43)   Acc@5  69.53 ( 83.11)
Epoch: [14][101/625]    Time  0.342 ( 0.615)    Data  0.193 ( 0.197)    Loss 2.5423e+00 (1.6396e+00)    Acc@1  44.53 ( 60.12)   Acc@5  73.44 ( 81.21)
val Loss: 2.4724 Acc: 0.4570

Epoch 15/19
----------
Epoch: [15][  1/625]    Time  0.794 ( 0.794)    Data  0.327 ( 0.327)    Loss 1.1137e+00 (1.1137e+00)    Acc@1  70.31 ( 70.31)   Acc@5  92.19 ( 92.19)
Epoch: [15][101/625]    Time  0.647 ( 0.661)    Data  0.192 ( 0.198)    Loss 1.4869e+00 (1.4049e+00)    Acc@1  64.84 ( 64.86)   Acc@5  82.03 ( 84.69)
Epoch: [15][201/625]    Time  0.658 ( 0.659)    Data  0.192 ( 0.197)    Loss 1.5071e+00 (1.4116e+00)    Acc@1  65.62 ( 64.56)   Acc@5  82.81 ( 84.55)
Epoch: [15][301/625]    Time  0.619 ( 0.649)    Data  0.164 ( 0.187)    Loss 1.6197e+00 (1.4293e+00)    Acc@1  57.03 ( 64.14)   Acc@5  81.25 ( 84.28)
Epoch: [15][401/625]    Time  0.618 ( 0.646)    Data  0.163 ( 0.184)    Loss 1.4647e+00 (1.4452e+00)    Acc@1  58.59 ( 63.65)   Acc@5  85.94 ( 84.12)
Epoch: [15][501/625]    Time  0.621 ( 0.642)    Data  0.165 ( 0.181)    Loss 1.5863e+00 (1.4495e+00)    Acc@1  62.50 ( 63.55)   Acc@5  82.81 ( 84.06)
Epoch: [15][601/625]    Time  0.623 ( 0.640)    Data  0.163 ( 0.179)    Loss 1.3837e+00 (1.4600e+00)    Acc@1  66.41 ( 63.26)   Acc@5  85.16 ( 83.91)
train Loss: 1.4630 Acc: 0.6319
Epoch: [15][  1/625]    Time  0.365 ( 0.639)    Data  0.218 ( 0.178)    Loss 2.7366e+00 (1.4650e+00)    Acc@1  43.75 ( 63.16)   Acc@5  68.75 ( 83.85)
Epoch: [15][101/625]    Time  0.307 ( 0.594)    Data  0.162 ( 0.176)    Loss 2.7534e+00 (1.6047e+00)    Acc@1  37.50 ( 60.72)   Acc@5  67.19 ( 82.00)
val Loss: 2.4890 Acc: 0.4548

Epoch 16/19
----------
Epoch: [16][  1/625]    Time  0.889 ( 0.889)    Data  0.426 ( 0.426)    Loss 1.5879e+00 (1.5879e+00)    Acc@1  55.47 ( 55.47)   Acc@5  85.16 ( 85.16)
Epoch: [16][101/625]    Time  0.618 ( 0.629)    Data  0.165 ( 0.169)    Loss 1.4433e+00 (1.3584e+00)    Acc@1  67.19 ( 65.71)   Acc@5  83.59 ( 85.26)
Epoch: [16][201/625]    Time  0.628 ( 0.629)    Data  0.164 ( 0.168)    Loss 1.2672e+00 (1.3694e+00)    Acc@1  67.97 ( 65.62)   Acc@5  87.50 ( 85.09)
Epoch: [16][301/625]    Time  0.628 ( 0.629)    Data  0.165 ( 0.168)    Loss 1.3294e+00 (1.3824e+00)    Acc@1  66.41 ( 65.30)   Acc@5  88.28 ( 84.88)
Epoch: [16][401/625]    Time  0.629 ( 0.628)    Data  0.165 ( 0.167)    Loss 1.4964e+00 (1.4006e+00)    Acc@1  65.62 ( 64.77)   Acc@5  84.38 ( 84.58)
Epoch: [16][501/625]    Time  0.630 ( 0.629)    Data  0.167 ( 0.168)    Loss 1.1359e+00 (1.4076e+00)    Acc@1  75.78 ( 64.63)   Acc@5  86.72 ( 84.50)
Epoch: [16][601/625]    Time  0.628 ( 0.629)    Data  0.165 ( 0.168)    Loss 1.7358e+00 (1.4183e+00)    Acc@1  54.69 ( 64.30)   Acc@5  82.81 ( 84.39)
train Loss: 1.4218 Acc: 0.6422
Epoch: [16][  1/625]    Time  0.372 ( 0.628)    Data  0.224 ( 0.168)    Loss 2.2693e+00 (1.4232e+00)    Acc@1  50.00 ( 64.19)   Acc@5  70.31 ( 84.30)
Epoch: [16][101/625]    Time  0.315 ( 0.585)    Data  0.169 ( 0.168)    Loss 2.4952e+00 (1.5772e+00)    Acc@1  46.88 ( 61.56)   Acc@5  64.06 ( 82.24)
val Loss: 2.5113 Acc: 0.4551

Epoch 17/19
----------
Epoch: [17][  1/625]    Time  0.731 ( 0.731)    Data  0.276 ( 0.276)    Loss 1.2871e+00 (1.2871e+00)    Acc@1  66.41 ( 66.41)   Acc@5  84.38 ( 84.38)
Epoch: [17][101/625]    Time  0.620 ( 0.629)    Data  0.166 ( 0.168)    Loss 1.3561e+00 (1.3182e+00)    Acc@1  64.06 ( 66.91)   Acc@5  89.06 ( 85.76)
Epoch: [17][201/625]    Time  0.619 ( 0.627)    Data  0.166 ( 0.167)    Loss 1.4658e+00 (1.3225e+00)    Acc@1  64.06 ( 66.86)   Acc@5  82.81 ( 85.86)
Epoch: [17][301/625]    Time  0.628 ( 0.628)    Data  0.165 ( 0.167)    Loss 1.2551e+00 (1.3303e+00)    Acc@1  67.97 ( 66.58)   Acc@5  86.72 ( 85.77)
Epoch: [17][401/625]    Time  0.627 ( 0.627)    Data  0.164 ( 0.167)    Loss 1.4532e+00 (1.3395e+00)    Acc@1  61.72 ( 66.36)   Acc@5  84.38 ( 85.62)
Epoch: [17][501/625]    Time  0.624 ( 0.627)    Data  0.165 ( 0.166)    Loss 1.3389e+00 (1.3507e+00)    Acc@1  68.75 ( 66.12)   Acc@5  84.38 ( 85.49)
Epoch: [17][601/625]    Time  0.637 ( 0.627)    Data  0.171 ( 0.167)    Loss 1.5117e+00 (1.3638e+00)    Acc@1  60.94 ( 65.70)   Acc@5  82.03 ( 85.29)
train Loss: 1.3667 Acc: 0.6563
Epoch: [17][  1/625]    Time  0.367 ( 0.627)    Data  0.221 ( 0.167)    Loss 2.1132e+00 (1.3679e+00)    Acc@1  53.12 ( 65.61)   Acc@5  77.34 ( 85.23)
Epoch: [17][101/625]    Time  0.309 ( 0.584)    Data  0.164 ( 0.167)    Loss 2.0068e+00 (1.5248e+00)    Acc@1  58.59 ( 62.88)   Acc@5  76.56 ( 83.09)
val Loss: 2.4952 Acc: 0.4606

Epoch 18/19
----------
Epoch: [18][  1/625]    Time  0.902 ( 0.902)    Data  0.444 ( 0.444)    Loss 1.2936e+00 (1.2936e+00)    Acc@1  67.97 ( 67.97)   Acc@5  88.28 ( 88.28)
Epoch: [18][101/625]    Time  0.615 ( 0.627)    Data  0.162 ( 0.166)    Loss 1.0696e+00 (1.2600e+00)    Acc@1  72.66 ( 68.35)   Acc@5  90.62 ( 86.45)
Epoch: [18][201/625]    Time  0.624 ( 0.626)    Data  0.162 ( 0.165)    Loss 1.8771e+00 (1.2857e+00)    Acc@1  49.22 ( 67.70)   Acc@5  80.47 ( 86.14)
Epoch: [18][301/625]    Time  0.616 ( 0.625)    Data  0.163 ( 0.165)    Loss 1.3868e+00 (1.2969e+00)    Acc@1  65.62 ( 67.56)   Acc@5  84.38 ( 85.93)
Epoch: [18][401/625]    Time  0.620 ( 0.625)    Data  0.162 ( 0.165)    Loss 1.4262e+00 (1.3105e+00)    Acc@1  65.62 ( 67.16)   Acc@5  85.16 ( 85.90)
Epoch: [18][501/625]    Time  0.617 ( 0.626)    Data  0.164 ( 0.165)    Loss 1.4940e+00 (1.3168e+00)    Acc@1  65.62 ( 67.00)   Acc@5  82.81 ( 85.79)
Epoch: [18][601/625]    Time  0.618 ( 0.625)    Data  0.164 ( 0.165)    Loss 1.1648e+00 (1.3312e+00)    Acc@1  67.97 ( 66.68)   Acc@5  89.06 ( 85.57)
train Loss: 1.3322 Acc: 0.6668
Epoch: [18][  1/625]    Time  0.366 ( 0.625)    Data  0.220 ( 0.165)    Loss 2.6106e+00 (1.3343e+00)    Acc@1  46.88 ( 66.65)   Acc@5  72.66 ( 85.55)
Epoch: [18][101/625]    Time  0.308 ( 0.581)    Data  0.163 ( 0.164)    Loss 2.1732e+00 (1.5063e+00)    Acc@1  51.56 ( 63.64)   Acc@5  74.22 ( 83.32)
val Loss: 2.5835 Acc: 0.4498

Epoch 19/19
----------
Epoch: [19][  1/625]    Time  0.736 ( 0.736)    Data  0.277 ( 0.277)    Loss 1.3433e+00 (1.3433e+00)    Acc@1  63.28 ( 63.28)   Acc@5  88.28 ( 88.28)
Epoch: [19][101/625]    Time  0.621 ( 0.627)    Data  0.164 ( 0.167)    Loss 1.1330e+00 (1.2392e+00)    Acc@1  71.88 ( 68.87)   Acc@5  90.62 ( 86.86)
Epoch: [19][201/625]    Time  0.625 ( 0.626)    Data  0.164 ( 0.165)    Loss 1.1424e+00 (1.2574e+00)    Acc@1  67.97 ( 68.19)   Acc@5  89.06 ( 86.60)
Epoch: [19][301/625]    Time  0.625 ( 0.625)    Data  0.161 ( 0.165)    Loss 1.5216e+00 (1.2698e+00)    Acc@1  62.50 ( 67.92)   Acc@5  83.59 ( 86.38)
Epoch: [19][401/625]    Time  0.625 ( 0.625)    Data  0.161 ( 0.165)    Loss 1.2324e+00 (1.2723e+00)    Acc@1  69.53 ( 67.82)   Acc@5  85.94 ( 86.48)
Epoch: [19][501/625]    Time  0.627 ( 0.625)    Data  0.162 ( 0.164)    Loss 1.3790e+00 (1.2804e+00)    Acc@1  64.84 ( 67.63)   Acc@5  82.81 ( 86.36)
Epoch: [19][601/625]    Time  0.629 ( 0.625)    Data  0.164 ( 0.164)    Loss 1.4337e+00 (1.2933e+00)    Acc@1  64.84 ( 67.34)   Acc@5  83.59 ( 86.17)
train Loss: 1.2956 Acc: 0.6729
Epoch: [19][  1/625]    Time  0.365 ( 0.624)    Data  0.219 ( 0.164)    Loss 2.5172e+00 (1.2975e+00)    Acc@1  46.09 ( 67.26)   Acc@5  71.88 ( 86.12)
Epoch: [19][101/625]    Time  0.307 ( 0.581)    Data  0.161 ( 0.164)    Loss 2.6266e+00 (1.4701e+00)    Acc@1  45.31 ( 64.36)   Acc@5  67.97 ( 83.85)
val Loss: 2.5539 Acc: 0.4595

Training complete in 155m 30s
Best val Acc: 0.460550
Test Loss: 0.497545

Test Accuracy of n01443537: 71% (77/108)
Test Accuracy of n01629819: 73% (65/88)
Test Accuracy of n01641577: 40% (40/100)
Test Accuracy of n01644900: 32% (35/109)
Test Accuracy of n01698640: 49% (55/111)
Test Accuracy of n01742172: 40% (37/91)
Test Accuracy of n01768244: 51% (43/84)
Test Accuracy of n01770393: 30% (26/84)
Test Accuracy of n01774384: 61% (58/95)
Test Accuracy of n01774750: 46% (42/91)
Test Accuracy of n01784675: 41% (38/92)
Test Accuracy of n01855672: 40% (41/102)
Test Accuracy of n01882714: 47% (45/95)
Test Accuracy of n01910747: 52% (56/106)
Test Accuracy of n01917289: 65% (62/95)
Test Accuracy of n01944390: 41% (40/96)
Test Accuracy of n01945685: 32% (31/96)
Test Accuracy of n01950731: 45% (47/104)
Test Accuracy of n01983481: 50% (59/117)
Test Accuracy of n01984695: 34% (33/95)
Test Accuracy of n02002724: 60% (65/107)
Test Accuracy of n02056570: 69% (58/84)
Test Accuracy of n02058221: 64% (64/99)
Test Accuracy of n02074367: 81% (90/110)
Test Accuracy of n02085620: 37% (40/108)
Test Accuracy of n02094433: 38% (42/110)
Test Accuracy of n02099601: 51% (48/94)
Test Accuracy of n02099712: 26% (28/106)
Test Accuracy of n02106662: 62% (71/114)
Test Accuracy of n02113799: 26% (26/97)
Test Accuracy of n02123045: 29% (32/107)
Test Accuracy of n02123394: 41% (42/102)
Test Accuracy of n02124075: 60% (58/96)
Test Accuracy of n02125311: 55% (50/90)
Test Accuracy of n02129165: 57% (59/102)
Test Accuracy of n02132136: 52% (51/97)
Test Accuracy of n02165456: 49% (49/100)
Test Accuracy of n02190166: 45% (47/104)
Test Accuracy of n02206856: 67% (57/84)
Test Accuracy of n02226429: 33% (29/86)
Test Accuracy of n02231487: 33% (34/101)
Test Accuracy of n02233338: 39% (41/103)
Test Accuracy of n02236044: 48% (46/94)
Test Accuracy of n02268443: 59% (62/105)
Test Accuracy of n02279972: 81% (90/110)
Test Accuracy of n02281406: 84% (81/96)
Test Accuracy of n02321529: 37% (38/101)
Test Accuracy of n02364673: 51% (49/96)
Test Accuracy of n02395406: 24% (26/106)
Test Accuracy of n02403003: 33% (37/112)
Test Accuracy of n02410509: 54% (50/91)
Test Accuracy of n02415577: 58% (65/111)
Test Accuracy of n02423022: 66% (73/109)
Test Accuracy of n02437312: 52% (49/93)
Test Accuracy of n02480495: 51% (53/102)
Test Accuracy of n02481823: 51% (53/102)
Test Accuracy of n02486410: 30% (28/92)
Test Accuracy of n02504458: 66% (65/98)
Test Accuracy of n02509815: 68% (66/97)
Test Accuracy of n02666196: 54% (43/79)
Test Accuracy of n02669723: 52% (51/97)
Test Accuracy of n02699494: 53% (56/105)
Test Accuracy of n02730930: 28% (28/99)
Test Accuracy of n02769748: 25% (26/101)
Test Accuracy of n02788148: 21% (22/103)
Test Accuracy of n02791270: 30% (27/90)
Test Accuracy of n02793495: 64% (64/99)
Test Accuracy of n02795169: 29% (32/107)
Test Accuracy of n02802426: 41% (37/89)
Test Accuracy of n02808440: 44% (42/94)
Test Accuracy of n02814533: 55% (50/90)
Test Accuracy of n02814860: 57% (57/100)
Test Accuracy of n02815834: 51% (52/101)
Test Accuracy of n02823428: 22% (25/112)
Test Accuracy of n02837789: 45% (41/91)
Test Accuracy of n02841315: 23% (21/90)
Test Accuracy of n02843684: 56% (60/107)
Test Accuracy of n02883205: 17% (18/101)
Test Accuracy of n02892201: 69% (73/105)
Test Accuracy of n02906734: 37% (36/96)
Test Accuracy of n02909870: 23% (26/110)
Test Accuracy of n02917067: 53% (54/101)
Test Accuracy of n02927161: 41% (46/111)
Test Accuracy of n02948072: 36% (33/90)
Test Accuracy of n02950826: 32% (26/81)
Test Accuracy of n02963159: 44% (48/109)
Test Accuracy of n02977058: 38% (35/90)
Test Accuracy of n02988304: 42% (45/107)
Test Accuracy of n02999410: 23% (25/105)
Test Accuracy of n03014705: 20% (22/106)
Test Accuracy of n03026506: 58% (54/92)
Test Accuracy of n03042490: 55% (57/103)
Test Accuracy of n03085013: 42% (49/115)
Test Accuracy of n03089624: 41% (49/119)
Test Accuracy of n03100240: 47% (47/100)
Test Accuracy of n03126707: 42% (38/89)
Test Accuracy of n03160309: 50% (45/90)
Test Accuracy of n03179701: 32% (28/85)
Test Accuracy of n03201208: 53% (48/90)
Test Accuracy of n03250847: 22% (25/109)
Test Accuracy of n03255030: 40% (40/98)
Test Accuracy of n03355925: 40% (43/106)
Test Accuracy of n03388043: 41% (41/98)
Test Accuracy of n03393912: 82% (85/103)
Test Accuracy of n03400231: 37% (39/104)
Test Accuracy of n03404251: 23% (23/98)
Test Accuracy of n03424325: 31% (32/103)
Test Accuracy of n03444034: 35% (35/100)
Test Accuracy of n03447447: 60% (66/109)
Test Accuracy of n03544143: 50% (50/99)
Test Accuracy of n03584254: 45% (43/94)
Test Accuracy of n03599486: 55% (58/104)
Test Accuracy of n03617480: 52% (56/107)
Test Accuracy of n03637318: 33% (37/109)
Test Accuracy of n03649909: 30% (31/101)
Test Accuracy of n03662601: 64% (67/104)
Test Accuracy of n03670208: 48% (48/99)
Test Accuracy of n03706229: 55% (54/97)
Test Accuracy of n03733131: 64% (61/94)
Test Accuracy of n03763968: 51% (44/86)
Test Accuracy of n03770439: 36% (37/102)
Test Accuracy of n03796401: 45% (48/106)
Test Accuracy of n03804744: 24% (24/97)
Test Accuracy of n03814639: 25% (26/102)
Test Accuracy of n03837869: 69% (74/106)
Test Accuracy of n03838899: 32% (30/91)
Test Accuracy of n03854065: 67% (70/104)
Test Accuracy of n03891332: 40% (37/91)
Test Accuracy of n03902125: 48% (51/106)
Test Accuracy of n03930313: 52% (58/110)
Test Accuracy of n03937543: 31% (29/93)
Test Accuracy of n03970156:  6% ( 6/90)
Test Accuracy of n03976657: 19% (20/101)
Test Accuracy of n03977966: 43% (47/108)
Test Accuracy of n03980874: 25% (25/98)
Test Accuracy of n03983396: 33% (34/102)
Test Accuracy of n03992509: 32% (32/98)
Test Accuracy of n04008634: 27% (29/105)
Test Accuracy of n04023962: 32% (31/95)
Test Accuracy of n04067472: 19% (21/106)
Test Accuracy of n04070727: 39% (40/101)
Test Accuracy of n04074963: 47% (41/86)
Test Accuracy of n04099969: 50% (51/102)
Test Accuracy of n04118538: 59% (52/87)
Test Accuracy of n04133789: 49% (52/105)
Test Accuracy of n04146614: 75% (71/94)
Test Accuracy of n04149813: 69% (73/105)
Test Accuracy of n04179913: 38% (43/113)
Test Accuracy of n04251144: 50% (45/89)
Test Accuracy of n04254777: 38% (40/103)
Test Accuracy of n04259630: 44% (51/114)
Test Accuracy of n04265275: 40% (41/101)
Test Accuracy of n04275548: 47% (47/98)
Test Accuracy of n04285008: 40% (45/112)
Test Accuracy of n04311004: 65% (68/104)
Test Accuracy of n04328186: 53% (49/91)
Test Accuracy of n04356056: 30% (29/96)
Test Accuracy of n04366367: 49% (52/105)
Test Accuracy of n04371430: 25% (24/93)
Test Accuracy of n04376876: 23% (26/112)
Test Accuracy of n04398044: 51% (55/107)
Test Accuracy of n04399382: 41% (46/110)
Test Accuracy of n04417672: 48% (46/95)
Test Accuracy of n04456115: 43% (43/100)
Test Accuracy of n04465501: 44% (47/105)
Test Accuracy of n04486054: 68% (72/105)
Test Accuracy of n04487081: 79% (79/100)
Test Accuracy of n04501370: 59% (54/91)
Test Accuracy of n04507155: 21% (21/99)
Test Accuracy of n04532106: 36% (37/101)
Test Accuracy of n04532670: 66% (66/99)
Test Accuracy of n04540053: 70% (70/99)
Test Accuracy of n04560804: 14% (16/108)
Test Accuracy of n04562935: 65% (68/104)
Test Accuracy of n04596742: 32% (31/95)
Test Accuracy of n04597913: 15% (17/110)
Test Accuracy of n06596364: 54% (56/103)
Test Accuracy of n07579787: 35% (44/123)
Test Accuracy of n07583066: 43% (48/110)
Test Accuracy of n07614500: 44% (41/92)
Test Accuracy of n07615774: 40% (40/99)
Test Accuracy of n07695742: 54% (51/93)
Test Accuracy of n07711569: 45% (42/92)
Test Accuracy of n07715103: 55% (46/83)
Test Accuracy of n07720875: 51% (53/102)
Test Accuracy of n07734744: 59% (55/92)
Test Accuracy of n07747607: 60% (64/105)
Test Accuracy of n07749582: 63% (57/90)
Test Accuracy of n07753592: 49% (54/110)
Test Accuracy of n07768694: 59% (58/98)
Test Accuracy of n07871810: 40% (40/100)
Test Accuracy of n07873807: 68% (64/93)
Test Accuracy of n07875152: 37% (37/100)
Test Accuracy of n07920052: 58% (63/108)
Test Accuracy of n09193705: 45% (47/103)
Test Accuracy of n09246464: 37% (34/91)
Test Accuracy of n09256479: 55% (58/104)
Test Accuracy of n09332890: 32% (32/100)
Test Accuracy of n09428293: 42% (45/107)
Test Accuracy of n12267677: 16% (18/106)

Test Accuracy (Overall): 45% (9140/20000)